{"2024-07-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.09635v2","updated":"2024-07-18T17:59:35Z","published":"2024-03-14T17:59:14Z","title":"Transformers Get Stable: An End-to-End Signal Propagation Theory for\n  Language Models","summary":"  In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 1000\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across encoder-only, decoder-only and\nencoder-decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for Image Classification.\n","authors":["Akhil Kedia","Mohd Abbas Zaidi","Sushil Khyalia","Jungho Jung","Harshith Goka","Haejun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.09635v2.pdf","comment":"Accepted at ICML 2024. Source code is available at\n  https://github.com/akhilkedia/TranformersGetStable. Akhil Kedia, Mohd Abbas\n  Zaidi, Sushil Khyalia equal contribution"},{"id":"http://arxiv.org/abs/2407.13765v1","updated":"2024-07-18T17:59:27Z","published":"2024-07-18T17:59:27Z","title":"Latent Causal Probing: A Formal Perspective on Probing with Causal\n  Models of Data","summary":"  As language models (LMs) deliver increasing performance on a range of NLP\ntasks, probing classifiers have become an indispensable technique in the effort\nto better understand their inner workings. A typical setup involves (1)\ndefining an auxiliary task consisting of a dataset of text annotated with\nlabels, then (2) supervising small classifiers to predict the labels from the\nrepresentations of a pretrained LM as it processed the dataset. A high probing\naccuracy is interpreted as evidence that the LM has learned to perform the\nauxiliary task as an unsupervised byproduct of its original pretraining\nobjective. Despite the widespread usage of probes, however, the robust design\nand analysis of probing experiments remains a challenge. We develop a formal\nperspective on probing using structural causal models (SCM). Specifically,\ngiven an SCM which explains the distribution of tokens observed during\ntraining, we frame the central hypothesis as whether the LM has learned to\nrepresent the latent variables of the SCM. Empirically, we extend a recent\nstudy of LMs in the context of a synthetic grid-world navigation task, where\nhaving an exact model of the underlying causal structure allows us to draw\nstrong inferences from the result of probing experiments. Our techniques\nprovide robust empirical evidence for the ability of LMs to learn the latent\ncausal concepts underlying text.\n","authors":["Charles Jin"],"pdf_url":"https://arxiv.org/pdf/2407.13765v1.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2407.13757v1","updated":"2024-07-18T17:55:55Z","published":"2024-07-18T17:55:55Z","title":"Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation\n  of Large Language Models","summary":"  Retrieval-Augmented Generation (RAG) is applied to solve hallucination\nproblems and real-time constraints of large language models, but it also\ninduces vulnerabilities against retrieval corruption attacks. Existing research\nmainly explores the unreliability of RAG in white-box and closed-domain QA\ntasks. In this paper, we aim to reveal the vulnerabilities of\nRetrieval-Enhanced Generative (RAG) models when faced with black-box attacks\nfor opinion manipulation. We explore the impact of such attacks on user\ncognition and decision-making, providing new insight to enhance the reliability\nand security of RAG models. We manipulate the ranking results of the retrieval\nmodel in RAG with instruction and use these results as data to train a\nsurrogate model. By employing adversarial retrieval attack methods to the\nsurrogate model, black-box transfer attacks on RAG are further realized.\nExperiments conducted on opinion datasets across multiple topics show that the\nproposed attack strategy can significantly alter the opinion polarity of the\ncontent generated by RAG. This demonstrates the model's vulnerability and, more\nimportantly, reveals the potential negative impact on user cognition and\ndecision-making, making it easier to mislead users into accepting incorrect or\nbiased information.\n","authors":["Zhuo Chen","Jiawei Liu","Haotan Liu","Qikai Cheng","Fan Zhang","Wei Lu","Xiaozhong Liu"],"pdf_url":"https://arxiv.org/pdf/2407.13757v1.pdf","comment":"10 pages, 3 figures, under review"},{"id":"http://arxiv.org/abs/2306.00978v5","updated":"2024-07-18T17:51:33Z","published":"2023-06-01T17:59:10Z","title":"AWQ: Activation-aware Weight Quantization for LLM Compression and\n  Acceleration","summary":"  Large language models (LLMs) have transformed numerous AI applications.\nOn-device LLM is becoming increasingly important: running LLMs locally on edge\ndevices can reduce the cloud computing cost and protect users' privacy.\nHowever, the astronomical model size and the limited hardware resource pose\nsignificant deployment challenges. We propose Activation-aware Weight\nQuantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only\nquantization. AWQ finds that not all weights in an LLM are equally important.\nProtecting only 1% salient weights can greatly reduce quantization error. To\nidentify salient weight channels, we should refer to the activation\ndistribution, not weights. To avoid the hardware-inefficient mix-precision\nquantization, we mathematically derive that scaling up the salient channels can\nreduce the quantization error. AWQ employs an equivalent transformation to\nscale the salient weight channels to protect them. The scale is determined by\ncollecting the activation statistics offline. AWQ does not rely on any\nbackpropagation or reconstruction, so it generalizes to different domains and\nmodalities without overfitting the calibration set. AWQ outperforms existing\nwork on various language modeling and domain-specific benchmarks (coding and\nmath). Thanks to better generalization, it achieves excellent quantization\nperformance for instruction-tuned LMs and, for the first time, multi-modal LMs.\nAlongside AWQ, we implement TinyChat, an efficient and flexible inference\nframework tailored for 4-bit on-device LLM/VLMs. With kernel fusion and\nplatform-aware weight packing, TinyChat offers more than 3x speedup over the\nHuggingface FP16 implementation on both desktop and mobile GPUs. It also\ndemocratizes the deployment of the 70B Llama-2 model on mobile GPUs.\n","authors":["Ji Lin","Jiaming Tang","Haotian Tang","Shang Yang","Wei-Ming Chen","Wei-Chen Wang","Guangxuan Xiao","Xingyu Dang","Chuang Gan","Song Han"],"pdf_url":"https://arxiv.org/pdf/2306.00978v5.pdf","comment":"MLSys 2024 Best Paper Award. Code available at:\n  https://github.com/mit-han-lab/llm-awq"},{"id":"http://arxiv.org/abs/2407.13744v1","updated":"2024-07-18T17:49:56Z","published":"2024-07-18T17:49:56Z","title":"LLMs as Function Approximators: Terminology, Taxonomy, and Questions for\n  Evaluation","summary":"  Natural Language Processing has moved rather quickly from modelling specific\ntasks to taking more general pre-trained models and fine-tuning them for\nspecific tasks, to a point where we now have what appear to be inherently\ngeneralist models. This paper argues that the resultant loss of clarity on what\nthese models model leads to metaphors like \"artificial general intelligences\"\nthat are not helpful for evaluating their strengths and weaknesses. The\nproposal is to see their generality, and their potential value, in their\nability to approximate specialist function, based on a natural language\nspecification. This framing brings to the fore questions of the quality of the\napproximation, but beyond that, also questions of discoverability, stability,\nand protectability of these functions. As the paper will show, this framing\nhence brings together in one conceptual framework various aspects of\nevaluation, both from a practical and a theoretical perspective, as well as\nquestions often relegated to a secondary status (such as \"prompt injection\" and\n\"jailbreaking\").\n","authors":["David Schlangen"],"pdf_url":"https://arxiv.org/pdf/2407.13744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10920v2","updated":"2024-07-18T17:49:16Z","published":"2024-07-15T17:21:41Z","title":"Benchmarking Vision Language Models for Cultural Understanding","summary":"  Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.\n","authors":["Shravan Nayak","Kanishk Jain","Rabiul Awal","Siva Reddy","Sjoerd van Steenkiste","Lisa Anne Hendricks","Karolina Stańczak","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.10920v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13739v1","updated":"2024-07-18T17:46:02Z","published":"2024-07-18T17:46:02Z","title":"Scaling Granite Code Models to 128K Context","summary":"  This paper introduces long-context Granite code models that support effective\ncontext windows of up to 128K tokens. Our solution for scaling context length\nof Granite 3B/8B code models from 2K/4K to 128K consists of a light-weight\ncontinual pretraining by gradually increasing its RoPE base frequency with\nrepository-level file packing and length-upsampled long-context data.\nAdditionally, we also release instruction-tuned models with long-context\nsupport which are derived by further finetuning the long context base models on\na mix of permissively licensed short and long-context instruction-response\npairs. While comparing to the original short-context Granite code models, our\nlong-context models achieve significant improvements on long-context tasks\nwithout any noticeable performance degradation on regular code completion\nbenchmarks (e.g., HumanEval). We release all our long-context Granite code\nmodels under an Apache 2.0 license for both research and commercial use.\n","authors":["Matt Stallone","Vaibhav Saxena","Leonid Karlinsky","Bridget McGinn","Tim Bula","Mayank Mishra","Adriana Meza Soria","Gaoyuan Zhang","Aditya Prasad","Yikang Shen","Saptha Surendran","Shanmukha Guttula","Hima Patel","Parameswaran Selvam","Xuan-Hong Dang","Yan Koyfman","Atin Sood","Rogerio Feris","Nirmit Desai","David D. Cox","Ruchir Puri","Rameswar Panda"],"pdf_url":"https://arxiv.org/pdf/2407.13739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13729v1","updated":"2024-07-18T17:30:48Z","published":"2024-07-18T17:30:48Z","title":"Baba Is AI: Break the Rules to Beat the Benchmark","summary":"  Humans solve problems by following existing rules and procedures, and also by\nleaps of creativity to redefine those rules and objectives. To probe these\nabilities, we developed a new benchmark based on the game Baba Is You where an\nagent manipulates both objects in the environment and rules, represented by\nmovable tiles with words written on them, to reach a specified goal and win the\ngame. We test three state-of-the-art multi-modal large language models (OpenAI\nGPT-4o, Google Gemini-1.5-Pro and Gemini-1.5-Flash) and find that they fail\ndramatically when generalization requires that the rules of the game must be\nmanipulated and combined.\n","authors":["Nathan Cloos","Meagan Jens","Michelangelo Naim","Yen-Ling Kuo","Ignacio Cases","Andrei Barbu","Christopher J. Cueva"],"pdf_url":"https://arxiv.org/pdf/2407.13729v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2305.14239v3","updated":"2024-07-18T17:23:59Z","published":"2023-05-23T16:56:04Z","title":"On Learning to Summarize with Large Language Models as References","summary":"  Recent studies have found that summaries generated by large language models\n(LLMs) are favored by human annotators over the original reference summaries in\ncommonly used summarization datasets. Therefore, we study an LLM-as-reference\nlearning setting for smaller text summarization models to investigate whether\ntheir performance can be substantially improved. To this end, we use LLMs as\nboth oracle summary generators for standard supervised fine-tuning and oracle\nsummary evaluators for efficient contrastive learning that leverages the LLMs'\nsupervision signals. We conduct comprehensive experiments with source news\narticles and find that (1) summarization models trained under the\nLLM-as-reference setting achieve significant performance improvement in both\nLLM and human evaluations; (2) contrastive learning outperforms standard\nsupervised fine-tuning under both low and high resource settings. Our\nexperimental results also enable a meta-analysis of LLMs' summary evaluation\ncapacities under a challenging setting, showing that LLMs are not well-aligned\nwith human evaluators. Particularly, our expert human evaluation reveals\nremaining nuanced performance gaps between LLMs and our fine-tuned models,\nwhich LLMs fail to capture. Thus, we call for further studies into both the\npotential and challenges of using LLMs in summarization model development.\n","authors":["Yixin Liu","Kejian Shi","Katherine S He","Longtian Ye","Alexander R. Fabbri","Pengfei Liu","Dragomir Radev","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2305.14239v3.pdf","comment":"NAACL 2024, GitHub Repo: https://github.com/yixinL7/SumLLM"},{"id":"http://arxiv.org/abs/2407.13709v1","updated":"2024-07-18T17:08:10Z","published":"2024-07-18T17:08:10Z","title":"Understanding Reference Policies in Direct Preference Optimization","summary":"  Direct Preference Optimization (DPO) has become a widely used training method\nfor the instruction fine-tuning of large language models (LLMs). In this work,\nwe explore an under-investigated aspect of DPO - its dependency on the\nreference model or policy. Such reference policies, typically instantiated as\nthe model to be further fine-tuned, are important since they can impose an\nupper limit on DPO's effectiveness. Therefore, we address three related\nresearch questions in this work. First, we explore the optimal strength of the\nKL-divergence constraint in DPO, which penalizes deviations from the reference\npolicy, and find that DPO is sensitive to this strength. Next, we examine the\nnecessity of reference policies for instruction fine-tuning by providing both\ntheoretical and empirical comparisons between DPO and related learning\nobjectives, demonstrating DPO's superiority. Additionally, we investigate\nwhether DPO benefits from stronger reference policies, finding that a stronger\nreference policy can lead to improved performance, but only when it is similar\nto the model being fine-tuned. Our findings highlight the confounding role of\nreference policies in DPO and offer insights for best practices, while also\nidentifying open research questions for future studies.\n","authors":["Yixin Liu","Pengfei Liu","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2407.13709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13702v1","updated":"2024-07-18T17:01:38Z","published":"2024-07-18T17:01:38Z","title":"ANHALTEN: Cross-Lingual Transfer for German Token-Level Reference-Free\n  Hallucination Detection","summary":"  Research on token-level reference-free hallucination detection has\npredominantly focused on English, primarily due to the scarcity of robust\ndatasets in other languages. This has hindered systematic investigations into\nthe effectiveness of cross-lingual transfer for this important NLP application.\nTo address this gap, we introduce ANHALTEN, a new evaluation dataset that\nextends the English hallucination detection dataset to German. To the best of\nour knowledge, this is the first work that explores cross-lingual transfer for\ntoken-level reference-free hallucination detection. ANHALTEN contains gold\nannotations in German that are parallel (i.e., directly comparable to the\noriginal English instances). We benchmark several prominent cross-lingual\ntransfer approaches, demonstrating that larger context length leads to better\nhallucination detection in German, even without succeeding context.\nImportantly, we show that the sample-efficient few-shot transfer is the most\neffective approach in most setups. This highlights the practical benefits of\nminimal annotation effort in the target language for reference-free\nhallucination detection. Aiming to catalyze future research on cross-lingual\ntoken-level reference-free hallucination detection, we make ANHALTEN publicly\navailable: https://github.com/janekh24/anhalten\n","authors":["Janek Herrlein","Chia-Chien Hung","Goran Glavaš"],"pdf_url":"https://arxiv.org/pdf/2407.13702v1.pdf","comment":"ACL 2024 Student Research Workshop"},{"id":"http://arxiv.org/abs/2407.13696v1","updated":"2024-07-18T17:00:23Z","published":"2024-07-18T17:00:23Z","title":"Benchmark Agreement Testing Done Right: A Guide for LLM Benchmark\n  Evaluation","summary":"  Recent advancements in Language Models (LMs) have catalyzed the creation of\nmultiple benchmarks, designed to assess these models' general capabilities. A\ncrucial task, however, is assessing the validity of the benchmarks themselves.\nThis is most commonly done via Benchmark Agreement Testing (BAT), where new\nbenchmarks are validated against established ones using some agreement metric\n(e.g., rank correlation). Despite the crucial role of BAT for benchmark\nbuilders and consumers, there are no standardized procedures for such agreement\ntesting. This deficiency can lead to invalid conclusions, fostering mistrust in\nbenchmarks and upending the ability to properly choose the appropriate\nbenchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how\nsome overlooked methodological choices can significantly influence BAT results,\npotentially undermining the validity of conclusions. To address these\ninconsistencies, we propose a set of best practices for BAT and demonstrate how\nutilizing these methodologies greatly improves BAT robustness and validity. To\nfoster adoption and facilitate future research,, we introduce BenchBench, a\npython package for BAT, and release the BenchBench-leaderboard, a\nmeta-benchmark designed to evaluate benchmarks using their peers. Our findings\nunderscore the necessity for standardized BAT, ensuring the robustness and\nvalidity of benchmark evaluations in the evolving landscape of language model\nresearch.\n  BenchBench Package: https://github.com/IBM/BenchBench\n  Leaderboard: https://huggingface.co/spaces/per/BenchBench\n","authors":["Yotam Perlitz","Ariel Gera","Ofir Arviv","Asaf Yehudai","Elron Bandel","Eyal Shnarch","Michal Shmueli-Scheuer","Leshem Choshen"],"pdf_url":"https://arxiv.org/pdf/2407.13696v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2407.13692v1","updated":"2024-07-18T16:58:18Z","published":"2024-07-18T16:58:18Z","title":"Prover-Verifier Games improve legibility of LLM outputs","summary":"  One way to increase confidence in the outputs of Large Language Models (LLMs)\nis to support them with reasoning that is clear and easy to check -- a property\nwe call legibility. We study legibility in the context of solving grade-school\nmath problems and show that optimizing chain-of-thought solutions only for\nanswer correctness can make them less legible. To mitigate the loss in\nlegibility, we propose a training algorithm inspired by Prover-Verifier Game\nfrom Anil et al. (2021). Our algorithm iteratively trains small verifiers to\npredict solution correctness, \"helpful\" provers to produce correct solutions\nthat the verifier accepts, and \"sneaky\" provers to produce incorrect solutions\nthat fool the verifier. We find that the helpful prover's accuracy and the\nverifier's robustness to adversarial attacks increase over the course of\ntraining. Furthermore, we show that legibility training transfers to\ntime-constrained humans tasked with verifying solution correctness. Over course\nof LLM training human accuracy increases when checking the helpful prover's\nsolutions, and decreases when checking the sneaky prover's solutions. Hence,\ntraining for checkability by small verifiers is a plausible technique for\nincreasing output legibility. Our results suggest legibility training against\nsmall verifiers as a practical avenue for increasing legibility of large LLMs\nto humans, and thus could help with alignment of superhuman models.\n","authors":["Jan Hendrik Kirchner","Yining Chen","Harri Edwards","Jan Leike","Nat McAleese","Yuri Burda"],"pdf_url":"https://arxiv.org/pdf/2407.13692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13657v1","updated":"2024-07-18T16:32:48Z","published":"2024-07-18T16:32:48Z","title":"FuLG: 150B Romanian Corpus for Language Model Pretraining","summary":"  Research in the field of language models is rapidly evolving, with many open\nmodels being released to the public. Openly available pretraining corpora\nusually focus on only a handful of languages, with many others either missing\ncompletely or extremely underrepresented. In this report, we introduce FuLG, a\nhundred-fifty-billion-token Romanian corpus extracted from CommonCrawl. We\npresent our methodology for filtering FuLG and compare it via ablation studies\nagainst existing Romanian corpora.\n","authors":["Vlad-Andrei Bădoiu","Mihai-Valentin Dumitru","Alexandru M. Gherghescu","Alexandru Agache","Costin Raiciu"],"pdf_url":"https://arxiv.org/pdf/2407.13657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03482v2","updated":"2024-07-18T16:31:29Z","published":"2024-06-05T17:42:05Z","title":"QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead","summary":"  Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.\n","authors":["Amir Zandieh","Majid Daliri","Insu Han"],"pdf_url":"https://arxiv.org/pdf/2406.03482v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2407.13647v1","updated":"2024-07-18T16:25:17Z","published":"2024-07-18T16:25:17Z","title":"Weak-to-Strong Reasoning","summary":"  When large language models (LLMs) exceed human-level capabilities, it becomes\nincreasingly challenging to provide full-scale and accurate supervisions for\nthese models. Weak-to-strong learning, which leverages a less capable model to\nunlock the latent abilities of a stronger model, proves valuable in this\ncontext. Yet, the efficacy of this approach for complex reasoning tasks is\nstill untested. Furthermore, tackling reasoning tasks under the weak-to-strong\nsetting currently lacks efficient methods to avoid blindly imitating the weak\nsupervisor including its errors. In this paper, we introduce a progressive\nlearning framework that enables the strong model to autonomously refine its\ntraining data, without requiring input from either a more advanced model or\nhuman-annotated data. This framework begins with supervised fine-tuning on a\nselective small but high-quality dataset, followed by preference optimization\non contrastive samples identified by the strong model itself. Extensive\nexperiments on the GSM8K and MATH datasets demonstrate that our method\nsignificantly enhances the reasoning capabilities of Llama2-70b using three\nseparate weak models. This method is further validated in a forward-looking\nexperimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b\non the highly challenging OlympicArena dataset. This work paves the way for a\nmore scalable and sophisticated strategy to enhance AI reasoning powers. All\nrelevant code and resources are available in\n\\url{https://github.com/GAIR-NLP/weak-to-strong-reasoning}.\n","authors":["Yuqing Yang","Yan Ma","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.13647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13638v1","updated":"2024-07-18T16:12:47Z","published":"2024-07-18T16:12:47Z","title":"A Comparative Study on Automatic Coding of Medical Letters with\n  Explainability","summary":"  This study aims to explore the implementation of Natural Language Processing\n(NLP) and machine learning (ML) techniques to automate the coding of medical\nletters with visualised explainability and light-weighted local computer\nsettings. Currently in clinical settings, coding is a manual process that\ninvolves assigning codes to each condition, procedure, and medication in a\npatient's paperwork (e.g., 56265001 heart disease using SNOMED CT code). There\nare preliminary research on automatic coding in this field using\nstate-of-the-art ML models; however, due to the complexity and size of the\nmodels, the real-world deployment is not achieved. To further facilitate the\npossibility of automatic coding practice, we explore some solutions in a local\ncomputer setting; in addition, we explore the function of explainability for\ntransparency of AI models. We used the publicly available MIMIC-III database\nand the HAN/HLAN network models for ICD code prediction purposes. We also\nexperimented with the mapping between ICD and SNOMED CT knowledge bases. In our\nexperiments, the models provided useful information for 97.98\\% of codes. The\nresult of this investigation can shed some light on implementing automatic\nclinical coding in practice, such as in hospital settings, on the local\ncomputers used by clinicians , project page\n\\url{https://github.com/Glenj01/Medical-Coding}.\n","authors":["Jamie Glen","Lifeng Han","Paul Rayson","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2407.13638v1.pdf","comment":"working paper"},{"id":"http://arxiv.org/abs/2404.17912v2","updated":"2024-07-18T16:03:18Z","published":"2024-04-27T13:46:23Z","title":"SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision\n  Language Models","summary":"  Radiology Report Generation (R2Gen) demonstrates how Multi-modal Large\nLanguage Models (MLLMs) can automate the creation of accurate and coherent\nradiological reports. Existing methods often hallucinate details in text-based\nreports that don't accurately reflect the image content. To mitigate this, we\nintroduce a novel strategy, SERPENT-VLM (SElf Refining Radiology RePort\nGENeraTion using Vision Language Models), which improves the R2Gen task by\nintegrating a self-refining mechanism into the MLLM framework. We employ a\nunique self-supervised loss that leverages similarity between pooled image\nrepresentations and the contextual representations of the generated\nradiological text, alongside the standard Causal Language Modeling objective,\nto refine image-text representations. This allows the model to scrutinize and\nalign the generated text through dynamic interaction between a given image and\nthe generated text, therefore reducing hallucination and continuously enhancing\nnuanced report generation. SERPENT-VLM outperforms existing baselines such as\nLLaVA-Med, BiomedGPT, etc., achieving SoTA performance on the IU X-ray and\nRadiology Objects in COntext (ROCO) datasets, and also proves to be robust\nagainst noisy images. A qualitative case study emphasizes the significant\nadvancements towards more sophisticated MLLM frameworks for R2Gen, opening\npaths for further research into self-supervised refinement in the medical\nimaging domain.\n","authors":["Manav Nitin Kapadnis","Sohan Patnaik","Abhilash Nandy","Sourjyadip Ray","Pawan Goyal","Debdoot Sheet"],"pdf_url":"https://arxiv.org/pdf/2404.17912v2.pdf","comment":"8 pages, 3 figures, 4 tables, Accepted as oral at Clinical NLP\n  workshop at NAACL 2024"},{"id":"http://arxiv.org/abs/2407.13623v1","updated":"2024-07-18T15:58:54Z","published":"2024-07-18T15:58:54Z","title":"Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies","summary":"  Research on scaling large language models (LLMs) has primarily focused on\nmodel parameters and training data size, overlooking the role of vocabulary\nsize. % Intuitively, larger vocabularies enable more efficient tokenization by\nrepresenting sentences with fewer tokens, but they also increase the risk of\nunder-fitting representations for rare tokens. We investigate how vocabulary\nsize impacts LLM scaling laws by training models ranging from 33M to 3B\nparameters on up to 500B characters with various vocabulary configurations. We\npropose three complementary approaches for predicting the compute-optimal\nvocabulary size: IsoFLOPs analysis, derivative estimation, and parametric fit\nof the loss function. Our approaches converge on the same result that the\noptimal vocabulary size depends on the available compute budget and that larger\nmodels deserve larger vocabularies. However, most LLMs use too small vocabulary\nsizes. For example, we predict that the optimal vocabulary size of Llama2-70B\nshould have been at least 216K, 7 times larger than its vocabulary of 32K. We\nvalidate our predictions empirically by training models with 3B parameters\nacross different FLOPs budgets. Adopting our predicted optimal vocabulary size\nconsistently improves downstream performance over commonly used vocabulary\nsizes. By increasing the vocabulary size from the conventional 32K to 43K, we\nimprove performance on ARC-Challenge from 29.1 to 32.0 with the same 2.3e21\nFLOPs. Our work emphasizes the necessity of jointly considering model\nparameters and vocabulary size for efficient scaling.\n","authors":["Chaofan Tao","Qian Liu","Longxu Dou","Niklas Muennighoff","Zhongwei Wan","Ping Luo","Min Lin","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2407.13623v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2407.13608v1","updated":"2024-07-18T15:47:42Z","published":"2024-07-18T15:47:42Z","title":"dzNLP at NADI 2024 Shared Task: Multi-Classifier Ensemble with Weighted\n  Voting and TF-IDF Features","summary":"  This paper presents the contribution of our dzNLP team to the NADI 2024\nshared task, specifically in Subtask 1 - Multi-label Country-level Dialect\nIdentification (MLDID) (Closed Track). We explored various configurations to\naddress the challenge: in Experiment 1, we utilized a union of n-gram analyzers\n(word, character, character with word boundaries) with different n-gram values;\nin Experiment 2, we combined a weighted union of Term Frequency-Inverse\nDocument Frequency (TF-IDF) features with various weights; and in Experiment 3,\nwe implemented a weighted major voting scheme using three classifiers: Linear\nSupport Vector Classifier (LSVC), Random Forest (RF), and K-Nearest Neighbors\n(KNN).\n  Our approach, despite its simplicity and reliance on traditional machine\nlearning techniques, demonstrated competitive performance in terms of F1-score\nand precision. Notably, we achieved the highest precision score of 63.22% among\nthe participating teams. However, our overall F1 score was approximately 21%,\nsignificantly impacted by a low recall rate of 12.87%. This indicates that\nwhile our models were highly precise, they struggled to recall a broad range of\ndialect labels, highlighting a critical area for improvement in handling\ndiverse dialectal variations.\n","authors":["Mohamed Lichouri","Khaled Lounnas","Boualem Nadjib Zahaf","Mehdi Ayoub Rabiai"],"pdf_url":"https://arxiv.org/pdf/2407.13608v1.pdf","comment":"Accepted for publication in the conference proceedings of ArabicNLP\n  2024"},{"id":"http://arxiv.org/abs/2407.13603v1","updated":"2024-07-18T15:43:27Z","published":"2024-07-18T15:43:27Z","title":"dzStance at StanceEval2024: Arabic Stance Detection based on Sentence\n  Transformers","summary":"  This study compares Term Frequency-Inverse Document Frequency (TF-IDF)\nfeatures with Sentence Transformers for detecting writers' stances--favorable,\nopposing, or neutral--towards three significant topics: COVID-19 vaccine,\ndigital transformation, and women empowerment. Through empirical evaluation, we\ndemonstrate that Sentence Transformers outperform TF-IDF features across\nvarious experimental setups. Our team, dzStance, participated in a stance\ndetection competition, achieving the 13th position (74.91%) among 15 teams in\nWomen Empowerment, 10th (73.43%) in COVID Vaccine, and 12th (66.97%) in Digital\nTransformation. Overall, our team's performance ranked 13th (71.77%) among all\nparticipants. Notably, our approach achieved promising F1-scores, highlighting\nits effectiveness in identifying writers' stances on diverse topics. These\nresults underscore the potential of Sentence Transformers to enhance stance\ndetection models for addressing critical societal issues.\n","authors":["Mohamed Lichouri","Khaled Lounnas","Khelil Rafik Ouaras","Mohamed Abi","Anis Guechtouli"],"pdf_url":"https://arxiv.org/pdf/2407.13603v1.pdf","comment":"Accepted for publication in the conference proceedings of ArabicNLP\n  2024"},{"id":"http://arxiv.org/abs/2407.13597v1","updated":"2024-07-18T15:36:02Z","published":"2024-07-18T15:36:02Z","title":"PLANTS: A Novel Problem and Dataset for Summarization of Planning-Like\n  (PL) Tasks","summary":"  Text summarization is a well-studied problem that deals with deriving\ninsights from unstructured text consumed by humans, and it has found extensive\nbusiness applications. However, many real-life tasks involve generating a\nseries of actions to achieve specific goals, such as workflows, recipes,\ndialogs, and travel plans. We refer to them as planning-like (PL) tasks noting\nthat the main commonality they share is control flow information. which may be\npartially specified. Their structure presents an opportunity to create more\npractical summaries to help users make quick decisions. We investigate this\nobservation by introducing a novel plan summarization problem, presenting a\ndataset, and providing a baseline method for generating PL summaries. Using\nquantitative metrics and qualitative user studies to establish baselines, we\nevaluate the plan summaries from our method and large language models. We\nbelieve the novel problem and dataset can reinvigorate research in\nsummarization, which some consider as a solved problem.\n","authors":["Vishal Pallagani","Biplav Srivastava","Nitin Gupta"],"pdf_url":"https://arxiv.org/pdf/2407.13597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13579v1","updated":"2024-07-18T15:20:31Z","published":"2024-07-18T15:20:31Z","title":"Towards Zero-Shot Multimodal Machine Translation","summary":"  Current multimodal machine translation (MMT) systems rely on fully supervised\ndata (i.e models are trained on sentences with their translations and\naccompanying images). However, this type of data is costly to collect, limiting\nthe extension of MMT to other language pairs for which such data does not\nexist. In this work, we propose a method to bypass the need for fully\nsupervised data to train MMT systems, using multimodal English data only. Our\nmethod, called ZeroMMT, consists in adapting a strong text-only machine\ntranslation (MT) model by training it on a mixture of two objectives: visually\nconditioned masked language modelling and the Kullback-Leibler divergence\nbetween the original and new MMT outputs. We evaluate on standard MMT\nbenchmarks and the recently released CoMMuTE, a contrastive benchmark aiming to\nevaluate how well models use images to disambiguate English sentences. We\nobtain disambiguation performance close to state-of-the-art MMT models trained\nadditionally on fully supervised examples. To prove that our method generalizes\nto languages with no fully supervised training data available, we extend the\nCoMMuTE evaluation dataset to three new languages: Arabic, Russian and Chinese.\nWe further show that we can control the trade-off between disambiguation\ncapabilities and translation fidelity at inference time using classifier-free\nguidance and without any additional data. Our code, data and trained models are\npublicly accessible.\n","authors":["Matthieu Futeral","Cordelia Schmid","Benoît Sagot","Rachel Bawden"],"pdf_url":"https://arxiv.org/pdf/2407.13579v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2407.13578v1","updated":"2024-07-18T15:20:18Z","published":"2024-07-18T15:20:18Z","title":"Large Language Models as Reliable Knowledge Bases?","summary":"  The NLP community has recently shown a growing interest in leveraging Large\nLanguage Models (LLMs) for knowledge-intensive tasks, viewing LLMs as potential\nknowledge bases (KBs). However, the reliability and extent to which LLMs can\nfunction as KBs remain underexplored. While previous studies suggest LLMs can\nencode knowledge within their parameters, the amount of parametric knowledge\nalone is not sufficient to evaluate their effectiveness as KBs. This study\ndefines criteria that a reliable LLM-as-KB should meet, focusing on factuality\nand consistency, and covering both seen and unseen knowledge. We develop\nseveral metrics based on these criteria and use them to evaluate 26 popular\nLLMs, while providing a comprehensive analysis of the effects of model size,\ninstruction tuning, and in-context learning (ICL). Our results paint a worrying\npicture. Even a high-performant model like GPT-3.5-turbo is not factual or\nconsistent, and strategies like ICL and fine-tuning are unsuccessful at making\nLLMs better KBs.\n","authors":["Danna Zheng","Mirella Lapata","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2407.13578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13571v1","updated":"2024-07-18T15:14:35Z","published":"2024-07-18T15:14:35Z","title":"New Capability to Look Up an ASL Sign from a Video Example","summary":"  Looking up an unknown sign in an ASL dictionary can be difficult. Most ASL\ndictionaries are organized based on English glosses, despite the fact that (1)\nthere is no convention for assigning English-based glosses to ASL signs; and\n(2) there is no 1-1 correspondence between ASL signs and English words.\nFurthermore, what if the user does not know either the meaning of the target\nsign or its possible English translation(s)? Some ASL dictionaries enable\nsearching through specification of articulatory properties, such as handshapes,\nlocations, movement properties, etc. However, this is a cumbersome process and\ndoes not always result in successful lookup. Here we describe a new system,\npublicly shared on the Web, to enable lookup of a video of an ASL sign (e.g., a\nwebcam recording or a clip from a continuous signing video). The user submits a\nvideo for analysis and is presented with the five most likely sign matches, in\ndecreasing order of likelihood, so that the user can confirm the selection and\nthen be taken to our ASLLRP Sign Bank entry for that sign. Furthermore, this\nvideo lookup is also integrated into our newest version of SignStream(R)\nsoftware to facilitate linguistic annotation of ASL video data, enabling the\nuser to directly look up a sign in the video being annotated, and, upon\nconfirmation of the match, to directly enter into the annotation the gloss and\nfeatures of that sign, greatly increasing the efficiency and consistency of\nlinguistic annotations of ASL video data.\n","authors":["Carol Neidle","Augustine Opoku","Carey Ballard","Yang Zhou","Xiaoxiao He","Gregory Dimitriadis","Dimitris Metaxas"],"pdf_url":"https://arxiv.org/pdf/2407.13571v1.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.13565v1","updated":"2024-07-18T14:37:20Z","published":"2024-07-18T14:37:20Z","title":"dzFinNlp at AraFinNLP: Improving Intent Detection in Financial\n  Conversational Agents","summary":"  In this paper, we present our dzFinNlp team's contribution for intent\ndetection in financial conversational agents, as part of the AraFinNLP shared\ntask. We experimented with various models and feature configurations, including\ntraditional machine learning methods like LinearSVC with TF-IDF, as well as\ndeep learning models like Long Short-Term Memory (LSTM). Additionally, we\nexplored the use of transformer-based models for this task. Our experiments\nshow promising results, with our best model achieving a micro F1-score of\n93.02% and 67.21% on the ArBanking77 dataset, in the development and test sets,\nrespectively.\n","authors":["Mohamed Lichouri","Khaled Lounnas","Mohamed Zakaria Amziane"],"pdf_url":"https://arxiv.org/pdf/2407.13565v1.pdf","comment":"Accepted for publication in the conference proceedings of ArabicNLP\n  2024"},{"id":"http://arxiv.org/abs/2407.13561v1","updated":"2024-07-18T14:31:53Z","published":"2024-07-18T14:31:53Z","title":"Research on Tibetan Tourism Viewpoints information generation system\n  based on LLM","summary":"  Tibet, ensconced within China's territorial expanse, is distinguished by its\nlabyrinthine and heterogeneous topography, a testament to its profound\nhistorical heritage, and the cradle of a unique religious ethos. The very\nessence of these attributes, however, has impeded the advancement of Tibet's\ntourism service infrastructure, rendering existing smart tourism services\ninadequate for the region's visitors. This study delves into the ramifications\nof informational disparities at tourist sites on Tibetan tourism and addresses\nthe challenge of establishing the Large Language Model (LLM) evaluation\ncriteria. It introduces an innovative approach, the DualGen Bridge AI system,\nemploying supervised fine-tuning techniques to bolster model functionality and\nenhance optimization processes. Furthermore, it pioneers a multi-structured\ngenerative results assessment framework. Empirical validation confirms the\nefficacy of this framework. The study also explores the application of the\nsupervised fine-tuning method within the proprietary DualGen Bridge AI, aimed\nat refining the generation of tourist site information. The study's findings\noffer valuable insights for optimizing system performance and provide support\nand inspiration for the application of LLM technology in Tibet's tourism\nservices and beyond, potentially revolutionizing the smart tourism industry\nwith advanced, tailored information generation capabilities.\n","authors":["Jinhu Qi","Shuai Yan","Wentao Zhang","Yibo Zhang","Zirui Liu","Ke Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13559v1","updated":"2024-07-18T14:31:09Z","published":"2024-07-18T14:31:09Z","title":"Qalam : A Multimodal LLM for Arabic Optical Character and Handwriting\n  Recognition","summary":"  Arabic Optical Character Recognition (OCR) and Handwriting Recognition (HWR)\npose unique challenges due to the cursive and context-sensitive nature of the\nArabic script. This study introduces Qalam, a novel foundation model designed\nfor Arabic OCR and HWR, built on a SwinV2 encoder and RoBERTa decoder\narchitecture. Our model significantly outperforms existing methods, achieving a\nWord Error Rate (WER) of just 0.80% in HWR tasks and 1.18% in OCR tasks. We\ntrain Qalam on a diverse dataset, including over 4.5 million images from Arabic\nmanuscripts and a synthetic dataset comprising 60k image-text pairs. Notably,\nQalam demonstrates exceptional handling of Arabic diacritics, a critical\nfeature in Arabic scripts. Furthermore, it shows a remarkable ability to\nprocess high-resolution inputs, addressing a common limitation in current OCR\nsystems. These advancements underscore Qalam's potential as a leading solution\nfor Arabic script recognition, offering a significant leap in accuracy and\nefficiency.\n","authors":["Gagan Bhatia","El Moatez Billah Nagoudi","Fakhraddin Alwajih","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2407.13559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00448v2","updated":"2024-07-18T14:23:29Z","published":"2023-12-31T10:53:58Z","title":"Beyond Chinchilla-Optimal: Accounting for Inference in Language Model\n  Scaling Laws","summary":"  Large language model (LLM) scaling laws are empirical formulas that estimate\nchanges in model quality as a result of increasing parameter count and training\ndata. However, these formulas, including the popular Deepmind Chinchilla\nscaling laws, neglect to include the cost of inference. We modify the\nChinchilla scaling laws to calculate the optimal LLM parameter count and\npre-training data size to train and deploy a model of a given quality and\ninference demand. We conduct our analysis both in terms of a compute budget and\nreal-world costs and find that LLM researchers expecting reasonably large\ninference demand (~1B requests) should train models smaller and longer than\nChinchilla-optimal. Furthermore, we train 47 models of varying sizes and\nparameter counts to validate our formula and find that model quality continues\nto improve as we scale tokens per parameter to extreme ranges (up to 10,000).\nFinally, we ablate the procedure used to fit the Chinchilla scaling law\ncoefficients and find that developing scaling laws only from data collected at\ntypical token/parameter ratios overestimates the impact of additional tokens at\nthese extreme ranges.\n","authors":["Nikhil Sardana","Jacob Portes","Sasha Doubov","Jonathan Frankle"],"pdf_url":"https://arxiv.org/pdf/2401.00448v2.pdf","comment":"16 pages, 7 figures, To appear in the 41st International Conference\n  on Machine Learning, 2024"},{"id":"http://arxiv.org/abs/2402.13950v3","updated":"2024-07-18T13:49:56Z","published":"2024-02-21T17:23:59Z","title":"Making Reasoning Matter: Measuring and Improving Faithfulness of\n  Chain-of-Thought Reasoning","summary":"  Large language models (LLMs) have been shown to perform better when asked to\nreason step-by-step before answering a question. However, it is unclear to what\ndegree the model's final answer is faithful to the stated reasoning steps. In\nthis paper, we perform a causal mediation analysis on twelve LLMs to examine\nhow intermediate reasoning steps generated by the LLM influence the final\noutcome and find that LLMs do not reliably use their intermediate reasoning\nsteps when generating an answer. To address this issue, we introduce FRODO, a\nframework to tailor small-sized LMs to generate correct reasoning steps and\nrobustly reason over these steps. FRODO consists of an inference module that\nlearns to generate correct reasoning steps using an implicit causal reward\nfunction and a reasoning module that learns to faithfully reason over these\nintermediate inferences using a counterfactual and causal preference objective.\nOur experiments show that FRODO significantly outperforms four competitive\nbaselines. Furthermore, FRODO improves the robustness and generalization\nability of the reasoning LM, yielding higher performance on out-of-distribution\ntest sets. Finally, we find that FRODO's rationales are more faithful to its\nfinal answer predictions than standard supervised fine-tuning.\n","authors":["Debjit Paul","Robert West","Antoine Bosselut","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2402.13950v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13511v1","updated":"2024-07-18T13:43:01Z","published":"2024-07-18T13:43:01Z","title":"Can Open-Source LLMs Compete with Commercial Models? Exploring the\n  Few-Shot Performance of Current GPT Models in Biomedical Tasks","summary":"  Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPT\nand Anthropic's Claude 3 Opus, have dominated natural language processing (NLP)\nbenchmarks across different domains. New competing Open-Source alternatives\nlike Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap while\noften offering higher throughput and being less costly to use. Open-Source LLMs\ncan also be self-hosted, which makes them interesting for enterprise and\nclinical use cases where sensitive data should not be processed by third\nparties. We participated in the 12th BioASQ challenge, which is a retrieval\naugmented generation (RAG) setting, and explored the performance of current GPT\nmodels Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning\n(zero-shot, few-shot) and QLoRa fine-tuning. We also explored how additional\nrelevant knowledge from Wikipedia added to the context-window of the LLM might\nimprove their performance. Mixtral 8x7b was competitive in the 10-shot setting,\nboth with and without fine-tuning, but failed to produce usable results in the\nzero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead to\nmeasurable performance gains. Our results indicate that the performance gap\nbetween commercial and open-source models in RAG setups exists mainly in the\nzero-shot setting and can be closed by simply collecting few-shot examples for\ndomain-specific use cases. The code needed to rerun these experiments is\navailable through GitHub.\n","authors":["Samy Ateia","Udo Kruschwitz"],"pdf_url":"https://arxiv.org/pdf/2407.13511v1.pdf","comment":"Version as accepted at the BioASQ Lab at CLEF 2024"},{"id":"http://arxiv.org/abs/2407.13509v1","updated":"2024-07-18T13:42:38Z","published":"2024-07-18T13:42:38Z","title":"Spontaneous Style Text-to-Speech Synthesis with Controllable Spontaneous\n  Behaviors Based on Language Models","summary":"  Spontaneous style speech synthesis, which aims to generate human-like speech,\noften encounters challenges due to the scarcity of high-quality data and\nlimitations in model capabilities. Recent language model-based TTS systems can\nbe trained on large, diverse, and low-quality speech datasets, resulting in\nhighly natural synthesized speech. However, they are limited by the difficulty\nof simulating various spontaneous behaviors and capturing prosody variations in\nspontaneous speech. In this paper, we propose a novel spontaneous speech\nsynthesis system based on language models. We systematically categorize and\nuniformly model diverse spontaneous behaviors. Moreover, fine-grained prosody\nmodeling is introduced to enhance the model's ability to capture subtle prosody\nvariations in spontaneous speech.Experimental results show that our proposed\nmethod significantly outperforms the baseline methods in terms of prosody\nnaturalness and spontaneous behavior naturalness.\n","authors":["Weiqin Li","Peiji Yang","Yicheng Zhong","Yixuan Zhou","Zhisheng Wang","Zhiyong Wu","Xixin Wu","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2407.13509v1.pdf","comment":"Accepted by INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2407.00463v4","updated":"2024-07-18T13:26:57Z","published":"2024-06-29T15:20:11Z","title":"Open-Source Conversational AI with SpeechBrain 1.0","summary":"  SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,\nfocused particularly on speech processing tasks such as speech recognition,\nspeech enhancement, speaker recognition, text-to-speech, and much more. It\npromotes transparency and replicability by releasing both the pre-trained\nmodels and the complete \"recipes\" of code and algorithms required for training\nthem. This paper presents SpeechBrain 1.0, a significant milestone in the\nevolution of the toolkit, which now has over 200 recipes for speech, audio, and\nlanguage processing tasks, and more than 100 models available on Hugging Face.\nSpeechBrain 1.0 introduces new technologies to support diverse learning\nmodalities, Large Language Model (LLM) integration, and advanced decoding\nstrategies, along with novel models, tasks, and modalities. It also includes a\nnew benchmark repository, offering researchers a unified platform for\nevaluating models across diverse tasks.\n","authors":["Mirco Ravanelli","Titouan Parcollet","Adel Moumen","Sylvain de Langen","Cem Subakan","Peter Plantinga","Yingzhi Wang","Pooneh Mousavi","Luca Della Libera","Artem Ploujnikov","Francesco Paissan","Davide Borra","Salah Zaiem","Zeyu Zhao","Shucong Zhang","Georgios Karakasidis","Sung-Lin Yeh","Pierre Champion","Aku Rouhe","Rudolf Braun","Florian Mai","Juan Zuluaga-Gomez","Seyed Mahed Mousavi","Andreas Nautsch","Xuechen Liu","Sangeet Sagar","Jarod Duret","Salima Mdhaffar","Gaelle Laperriere","Mickael Rouvier","Renato De Mori","Yannick Esteve"],"pdf_url":"https://arxiv.org/pdf/2407.00463v4.pdf","comment":"Submitted to JMLR (Machine Learning Open Source Software)"},{"id":"http://arxiv.org/abs/2407.13492v1","updated":"2024-07-18T13:20:53Z","published":"2024-07-18T13:20:53Z","title":"Enhancing Biomedical Knowledge Discovery for Diseases: An End-To-End\n  Open-Source Framework","summary":"  The ever-growing volume of biomedical publications creates a critical need\nfor efficient knowledge discovery. In this context, we introduce an open-source\nend-to-end framework designed to construct knowledge around specific diseases\ndirectly from raw text. To facilitate research in disease-related knowledge\ndiscovery, we create two annotated datasets focused on Rett syndrome and\nAlzheimer's disease, enabling the identification of semantic relations between\nbiomedical entities. Extensive benchmarking explores various ways to represent\nrelations and entity representations, offering insights into optimal modeling\nstrategies for semantic relation detection and highlighting language models'\ncompetence in knowledge discovery. We also conduct probing experiments using\ndifferent layer representations and attention scores to explore transformers'\nability to capture semantic relations.\n","authors":["Christos Theodoropoulos","Andrei Catalin Coman","James Henderson","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2407.13492v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2407.13490v1","updated":"2024-07-18T13:15:55Z","published":"2024-07-18T13:15:55Z","title":"Combining Constraint Programming Reasoning with Large Language Model\n  Predictions","summary":"  Constraint Programming (CP) and Machine Learning (ML) face challenges in text\ngeneration due to CP's struggle with implementing \"meaning'' and ML's\ndifficulty with structural constraints. This paper proposes a solution by\ncombining both approaches and embedding a Large Language Model (LLM) in CP. The\nLLM handles word generation and meaning, while CP manages structural\nconstraints. This approach builds on GenCP, an improved version of On-the-fly\nConstraint Programming Search (OTFS) using LLM-generated domains. Compared to\nBeam Search (BS), a standard NLP method, this combined approach (GenCP with\nLLM) is faster and produces better results, ensuring all constraints are\nsatisfied. This fusion of CP and ML presents new possibilities for enhancing\ntext generation under constraints.\n","authors":["Florian Régin","Elisabetta De Maria","Alexandre Bonlarron"],"pdf_url":"https://arxiv.org/pdf/2407.13490v1.pdf","comment":"To appear at The 30th International Conference on Principles and\n  Practice of Constraint Programming (CP 2024)"},{"id":"http://arxiv.org/abs/2407.09209v2","updated":"2024-07-18T13:09:20Z","published":"2024-07-12T12:16:14Z","title":"Pronunciation Assessment with Multi-modal Large Language Models","summary":"  Large language models (LLMs), renowned for their powerful conversational\nabilities, are widely recognized as exceptional tools in the field of\neducation, particularly in the context of automated intelligent instruction\nsystems for language learning. In this paper, we propose a scoring system based\non LLMs, motivated by their positive impact on text-related scoring tasks.\nSpecifically, the speech encoder first maps the learner's speech into\ncontextual features. The adapter layer then transforms these features to align\nwith the text embedding in latent space. The assessment task-specific prefix\nand prompt text are embedded and concatenated with the features generated by\nthe modality adapter layer, enabling the LLMs to predict accuracy and fluency\nscores. Our experiments demonstrate that the proposed scoring systems achieve\ncompetitive results compared to the baselines on the Speechocean762 datasets.\nMoreover, we also conducted an ablation study to better understand the\ncontributions of the prompt text and training strategy in the proposed scoring\nsystem.\n","authors":["Kaiqi Fu","Linkai Peng","Nan Yang","Shuran Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.09209v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13481v1","updated":"2024-07-18T13:00:30Z","published":"2024-07-18T13:00:30Z","title":"Attention Overflow: Language Model Input Blur during Long-Context\n  Missing Items Recommendation","summary":"  Large language models (LLMs) can suggest missing elements from items listed\nin a prompt, which can be used for list completion or recommendations based on\nusers' history. However, their performance degrades when presented with too\nmany items, as they start to suggest items already included in the input list.\nThis occurs at around 100 items for mid-2024 flagship LLMs. We evaluate this\nphenomenon on both synthetic problems (e.g., finding missing numbers in a given\nrange of shuffled integers) and realistic movie recommendation scenarios. We\nrefer to this issue as \\textit{attention overflow}, as preventing repetition\nrequires attending to all items simultaneously. Although iterative loops can\nmitigate this problem, their costs increase with the repetition rate, affecting\nthe language models' ability to derive novelty from lengthy inputs.\n","authors":["Damien Sileo"],"pdf_url":"https://arxiv.org/pdf/2407.13481v1.pdf","comment":"Dataset URL:\n  https://huggingface.co/datasets/sileod/missing-item-prediction"},{"id":"http://arxiv.org/abs/2407.13469v1","updated":"2024-07-18T12:42:45Z","published":"2024-07-18T12:42:45Z","title":"Fixed and Adaptive Simultaneous Machine Translation Strategies Using\n  Adapters","summary":"  Simultaneous machine translation aims at solving the task of real-time\ntranslation by starting to translate before consuming the full input, which\nposes challenges in terms of balancing quality and latency of the translation.\nThe wait-$k$ policy offers a solution by starting to translate after consuming\n$k$ words, where the choice of the number $k$ directly affects the latency and\nquality. In applications where we seek to keep the choice over latency and\nquality at inference, the wait-$k$ policy obliges us to train more than one\nmodel. In this paper, we address the challenge of building one model that can\nfulfil multiple latency levels and we achieve this by introducing lightweight\nadapter modules into the decoder. The adapters are trained to be specialized\nfor different wait-$k$ values and compared to other techniques they offer more\nflexibility to allow for reaping the benefits of parameter sharing and\nminimizing interference. Additionally, we show that by combining with an\nadaptive strategy, we can further improve the results. Experiments on two\nlanguage directions show that our method outperforms or competes with other\nstrong baselines on most latency values.\n","authors":["Abderrahmane Issam","Yusuf Can Semerci","Jan Scholtes","Gerasimos Spanakis"],"pdf_url":"https://arxiv.org/pdf/2407.13469v1.pdf","comment":"Accepted at IWSLT 2024"},{"id":"http://arxiv.org/abs/2407.13463v1","updated":"2024-07-18T12:36:26Z","published":"2024-07-18T12:36:26Z","title":"End-To-End Clinical Trial Matching with Large Language Models","summary":"  Matching cancer patients to clinical trials is essential for advancing\ntreatment and patient care. However, the inconsistent format of medical free\ntext documents and complex trial eligibility criteria make this process\nextremely challenging and time-consuming for physicians. We investigated\nwhether the entire trial matching process - from identifying relevant trials\namong 105,600 oncology-related clinical trials on clinicaltrials.gov to\ngenerating criterion-level eligibility matches - could be automated using Large\nLanguage Models (LLMs). Using GPT-4o and a set of 51 synthetic Electronic\nHealth Records (EHRs), we demonstrate that our approach identifies relevant\ncandidate trials in 93.3% of cases and achieves a preliminary accuracy of 88.0%\nwhen matching patient-level information at the criterion level against a\nbaseline defined by human experts. Utilizing LLM feedback reveals that 39.3%\ncriteria that were initially considered incorrect are either ambiguous or\ninaccurately annotated, leading to a total model accuracy of 92.7% after\nrefining our human baseline. In summary, we present an end-to-end pipeline for\nclinical trial matching using LLMs, demonstrating high precision in screening\nand matching trials to individual patients, even outperforming the performance\nof qualified medical doctors. Our fully end-to-end pipeline can operate\nautonomously or with human supervision and is not restricted to oncology,\noffering a scalable solution for enhancing patient-trial matching in real-world\nsettings.\n","authors":["Dyke Ferber","Lars Hilgers","Isabella C. Wiest","Marie-Elisabeth Leßmann","Jan Clusmann","Peter Neidlinger","Jiefu Zhu","Georg Wölflein","Jacqueline Lammert","Maximilian Tschochohei","Heiko Böhme","Dirk Jäger","Mihaela Aldea","Daniel Truhn","Christiane Höper","Jakob Nikolas Kather"],"pdf_url":"https://arxiv.org/pdf/2407.13463v1.pdf","comment":"149 pages, including Supplements. 3 Main Figures"},{"id":"http://arxiv.org/abs/2404.07202v2","updated":"2024-07-18T12:30:48Z","published":"2024-04-10T17:59:20Z","title":"UMBRAE: Unified Multimodal Brain Decoding","summary":"  We address prevailing challenges of the brain-powered research, departing\nfrom the observation that the literature hardly recover accurate spatial\ninformation and require subject-specific models. To address these challenges,\nwe propose UMBRAE, a unified multimodal decoding of brain signals. First, to\nextract instance-level conceptual and spatial details from neural signals, we\nintroduce an efficient universal brain encoder for multimodal-brain alignment\nand recover object descriptions at multiple levels of granularity from\nsubsequent multimodal large language model (MLLM). Second, we introduce a\ncross-subject training strategy mapping subject-specific features to a common\nfeature space. This allows a model to be trained on multiple subjects without\nextra resources, even yielding superior results compared to subject-specific\nmodels. Further, we demonstrate this supports weakly-supervised adaptation to\nnew subjects, with only a fraction of the total training data. Experiments\ndemonstrate that UMBRAE not only achieves superior results in the newly\nintroduced tasks but also outperforms methods in well established tasks. To\nassess our method, we construct and share with the community a comprehensive\nbrain understanding benchmark BrainHub. Our code and benchmark are available at\nhttps://weihaox.github.io/UMBRAE.\n","authors":["Weihao Xia","Raoul de Charette","Cengiz Öztireli","Jing-Hao Xue"],"pdf_url":"https://arxiv.org/pdf/2404.07202v2.pdf","comment":"ECCV 2024. Project: https://weihaox.github.io/UMBRAE"},{"id":"http://arxiv.org/abs/2402.19088v3","updated":"2024-07-18T12:28:27Z","published":"2024-02-29T12:13:50Z","title":"Survey in Characterization of Semantic Change","summary":"  Live languages continuously evolve to integrate the cultural change of human\nsocieties. This evolution manifests through neologisms (new words) or\n\\textbf{semantic changes} of words (new meaning to existing words).\nUnderstanding the meaning of words is vital for interpreting texts coming from\ndifferent cultures (regionalism or slang), domains (e.g., technical terms), or\nperiods. In computer science, these words are relevant to computational\nlinguistics algorithms such as translation, information retrieval, question\nanswering, etc. Semantic changes can potentially impact the quality of the\noutcomes of these algorithms. Therefore, it is important to understand and\ncharacterize these changes formally. The study of this impact is a recent\nproblem that has attracted the attention of the computational linguistics\ncommunity. Several approaches propose methods to detect semantic changes with\ngood precision, but more effort is needed to characterize how the meaning of\nwords changes and to reason about how to reduce the impact of semantic change.\nThis survey provides an understandable overview of existing approaches to the\n\\textit{characterization of semantic changes} and also formally defines three\nclasses of characterizations: if the meaning of a word becomes more general or\nnarrow (change in dimension) if the word is used in a more pejorative or\npositive/ameliorated sense (change in orientation), and if there is a trend to\nuse the word in a, for instance, metaphoric or metonymic context (change in\nrelation). We summarized the main aspects of the selected publications in a\ntable and discussed the needs and trends in the research activities on semantic\nchange characterization.\n","authors":["Jader Martins Camboim de Sá","Marcos Da Silveira","Cédric Pruski"],"pdf_url":"https://arxiv.org/pdf/2402.19088v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13442v1","updated":"2024-07-18T12:11:12Z","published":"2024-07-18T12:11:12Z","title":"BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in\n  Vision-language Models","summary":"  Vision language models (VLMs) perceive the world through a combination of a\nvisual encoder and a large language model (LLM). The visual encoder,\npre-trained on large-scale vision-text datasets, provides zero-shot\ngeneralization to visual data, and the LLM endows its high reasoning ability to\nVLMs. It leads VLMs to achieve high performance on wide benchmarks without\nfine-tuning, exhibiting zero or few-shot capability. However, recent studies\nshow that VLMs are vulnerable to hallucination. This undesirable behavior\ndegrades reliability and credibility, thereby making users unable to fully\ntrust the output from VLMs. To enhance trustworthiness and better tackle the\nhallucination of VLMs, we curate a new evaluation dataset, called the\nBEfore-AFter hallucination dataset (BEAF), and introduce new metrics: True\nUnderstanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID).\nUnlike prior works that focus only on constructing questions and answers, the\nkey idea of our benchmark is to manipulate visual scene information by image\nediting models and to design the metrics based on scene changes. This allows us\nto clearly assess whether VLMs correctly understand a given scene by observing\nthe ability to perceive changes. We also visualize image-wise object\nrelationship by virtue of our two-axis view: vision and text. Upon evaluating\nVLMs with our dataset, we observed that our metrics reveal different aspects of\nVLM hallucination that have not been reported before. Project page:\n\\url{https://beafbench.github.io/}\n","authors":["Moon Ye-Bin","Nam Hyeon-Woo","Wonseok Choi","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2407.13442v1.pdf","comment":"Accepted at ECCV 2024. [Project Pages] https://beafbench.github.io/"},{"id":"http://arxiv.org/abs/2407.13435v1","updated":"2024-07-18T12:03:14Z","published":"2024-07-18T12:03:14Z","title":"Enhancing Out-of-Vocabulary Performance of Indian TTS Systems for\n  Practical Applications through Low-Effort Data Strategies","summary":"  Publicly available TTS datasets for low-resource languages like Hindi and\nTamil typically contain 10-20 hours of data, leading to poor vocabulary\ncoverage. This limitation becomes evident in downstream applications where\ndomain-specific vocabulary coupled with frequent code-mixing with English,\nresults in many OOV words. To highlight this problem, we create a benchmark\ncontaining OOV words from several real-world applications. Indeed,\nstate-of-the-art Hindi and Tamil TTS systems perform poorly on this OOV\nbenchmark, as indicated by intelligibility tests. To improve the model's OOV\nperformance, we propose a low-effort and economically viable strategy to obtain\nmore training data. Specifically, we propose using volunteers as opposed to\nhigh quality voice artists to record words containing character bigrams unseen\nin the training data. We show that using such inexpensive data, the model's\nperformance improves on OOV words, while not affecting voice quality and\nin-domain performance.\n","authors":["Srija Anand","Praveen Srinivasa Varadhan","Ashwin Sankar","Giri Raju","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2407.13435v1.pdf","comment":"Accepted at INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2407.13419v1","updated":"2024-07-18T11:42:13Z","published":"2024-07-18T11:42:13Z","title":"From Words to Worlds: Compositionality for Cognitive Architectures","summary":"  Large language models (LLMs) are very performant connectionist systems, but\ndo they exhibit more compositionality? More importantly, is that part of why\nthey perform so well? We present empirical analyses across four LLM families\n(12 models) and three task categories, including a novel task introduced below.\nOur findings reveal a nuanced relationship in learning of compositional\nstrategies by LLMs -- while scaling enhances compositional abilities,\ninstruction tuning often has a reverse effect. Such disparity brings forth some\nopen issues regarding the development and improvement of large language models\nin alignment with human cognitive capacities.\n","authors":["Ruchira Dhar","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2407.13419v1.pdf","comment":"Accepted to ICML 2024 Workshop on LLMs & Cognition"},{"id":"http://arxiv.org/abs/2406.10273v4","updated":"2024-07-18T11:21:10Z","published":"2024-06-11T19:20:27Z","title":"Beyond Words: On Large Language Models Actionability in Mission-Critical\n  Risk Analysis","summary":"  Context. Risk analysis assesses potential risks in specific scenarios. Risk\nanalysis principles are context-less; the same methodology can be applied to a\nrisk connected to health and information technology security. Risk analysis\nrequires a vast knowledge of national and international regulations and\nstandards and is time and effort-intensive. A large language model can quickly\nsummarize information in less time than a human and can be fine-tuned to\nspecific tasks.\n  Aim. Our empirical study aims to investigate the effectiveness of\nRetrieval-Augmented Generation and fine-tuned LLM in risk analysis. To our\nknowledge, no prior study has explored its capabilities in risk analysis.\n  Method. We manually curated 193 unique scenarios leading to 1283\nrepresentative samples from over 50 mission-critical analyses archived by the\nindustrial context team in the last five years. We compared the base GPT-3.5\nand GPT-4 models versus their Retrieval-Augmented Generation and fine-tuned\ncounterparts. We employ two human experts as competitors of the models and\nthree other human experts to review the models and the former human experts'\nanalysis. The reviewers analyzed 5,000 scenario analyses.\n  Results and Conclusions. Human experts demonstrated higher accuracy, but LLMs\nare quicker and more actionable. Moreover, our findings show that RAG-assisted\nLLMs have the lowest hallucination rates, effectively uncovering hidden risks\nand complementing human expertise. Thus, the choice of model depends on\nspecific needs, with FTMs for accuracy, RAG for hidden risks discovery, and\nbase models for comprehensiveness and actionability. Therefore, experts can\nleverage LLMs as an effective complementing companion in risk analysis within a\ncondensed timeframe. They can also save costs by averting unnecessary expenses\nassociated with implementing unwarranted countermeasures.\n","authors":["Matteo Esposito","Francesco Palagiano","Valentina Lenarduzzi","Davide Taibi"],"pdf_url":"https://arxiv.org/pdf/2406.10273v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13399v1","updated":"2024-07-18T11:08:40Z","published":"2024-07-18T11:08:40Z","title":"Correcting the Mythos of KL-Regularization: Direct Alignment without\n  Overparameterization via Chi-squared Preference Optimization","summary":"  Language model alignment methods, such as reinforcement learning from human\nfeedback (RLHF), have led to impressive advances in language model\ncapabilities, but existing techniques are limited by a widely observed\nphenomenon known as overoptimization, where the quality of the language model\nplateaus or degrades over the course of the alignment process. Overoptimization\nis often attributed to overfitting to an inaccurate reward model, and while it\ncan be mitigated through online data collection, this is infeasible in many\nsettings. This raises a fundamental question: Do existing offline alignment\nalgorithms make the most of the data they have, or can their sample-efficiency\nbe improved further?\n  We address this question with a new algorithm for offline alignment,\n$\\chi^2$-Preference Optimization ($\\chi$PO). $\\chi$PO is a one-line change to\nDirect Preference Optimization (DPO; Rafailov et al., 2023), which only\ninvolves modifying the logarithmic link function in the DPO objective. Despite\nthis minimal change, $\\chi$PO implicitly implements the principle of pessimism\nin the face of uncertainty via regularization with the $\\chi^2$-divergence --\nwhich quantifies uncertainty more effectively than KL-regularization -- and\nprovably alleviates overoptimization, achieving sample-complexity guarantees\nbased on single-policy concentrability -- the gold standard in offline\nreinforcement learning. $\\chi$PO's simplicity and strong guarantees make it the\nfirst practical and general-purpose offline alignment algorithm that is\nprovably robust to overoptimization.\n","authors":["Audrey Huang","Wenhao Zhan","Tengyang Xie","Jason D. Lee","Wen Sun","Akshay Krishnamurthy","Dylan J. Foster"],"pdf_url":"https://arxiv.org/pdf/2407.13399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06600v2","updated":"2024-07-18T11:04:24Z","published":"2024-06-06T13:44:57Z","title":"HORAE: A Domain-Agnostic Modeling Language for Automating Multimodal\n  Service Regulation","summary":"  Artificial intelligence is rapidly encroaching on the field of service\nregulation. This work presents the design principles behind HORAE, a unified\nspecification language to model multimodal regulation rules across a diverse\nset of domains. We show how HORAE facilitates an intelligent service regulation\npipeline by further exploiting a fine-tuned large language model named HORAE\nthat automates the HORAE modeling process, thereby yielding an end-to-end\nframework for fully automated intelligent service regulation.\n","authors":["Yutao Sun","Mingshuai Chen","Tiancheng Zhao","Kangjia Zhao","He Li","Jintao Chen","Liqiang Lu","Xinkui Zhao","Shuiguang Deng","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2406.06600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13377v1","updated":"2024-07-18T10:34:33Z","published":"2024-07-18T10:34:33Z","title":"Linear-Complexity Self-Supervised Learning for Speech Processing","summary":"  Self-supervised learning (SSL) models usually require weeks of pre-training\nwith dozens of high-end GPUs. These models typically have a multi-headed\nself-attention (MHSA) context encoder. However, MHSA takes quadratic time and\nspace in the input length, contributing to the high pre-training cost.\nLinear-complexity alternatives to MHSA have been proposed. For instance, in\nsupervised training, the SummaryMixing model is the first to outperform MHSA\nacross multiple speech processing tasks. However, these cheaper alternatives\nhave not been explored for SSL yet. This paper studies a linear-complexity\ncontext encoder for SSL for the first time. With better or equivalent\nperformance for the downstream tasks of the MP3S benchmark, SummaryMixing\nreduces the pre-training time and peak VRAM of wav2vec 2.0 model by 18% and by\n23%, respectively, leading to the pre-training of a 155M wav2vec 2.0 model\nfinished within one week with 4 Tesla A100 GPUs. Code is available at\nhttps://github.com/SamsungLabs/SummaryMixing.\n","authors":["Shucong Zhang","Titouan Parcollet","Rogier van Dalen","Sourav Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2407.13377v1.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2407.13358v1","updated":"2024-07-18T10:01:09Z","published":"2024-07-18T10:01:09Z","title":"Capturing Style in Author and Document Representation","summary":"  A wide range of Deep Natural Language Processing (NLP) models integrates\ncontinuous and low dimensional representations of words and documents.\nSurprisingly, very few models study representation learning for authors. These\nrepresentations can be used for many NLP tasks, such as author identification\nand classification, or in recommendation systems. A strong limitation of\nexisting works is that they do not explicitly capture writing style, making\nthem hardly applicable to literary data. We therefore propose a new\narchitecture based on Variational Information Bottleneck (VIB) that learns\nembeddings for both authors and documents with a stylistic constraint. Our\nmodel fine-tunes a pre-trained document encoder. We stimulate the detection of\nwriting style by adding predefined stylistic features making the representation\naxis interpretable with respect to writing style indicators. We evaluate our\nmethod on three datasets: a literary corpus extracted from the Gutenberg\nProject, the Blog Authorship Corpus and IMDb62, for which we show that it\nmatches or outperforms strong/recent baselines in authorship attribution while\ncapturing much more accurately the authors stylistic aspects.\n","authors":["Enzo Terreau","Antoine Gourru","Julien Velcin"],"pdf_url":"https://arxiv.org/pdf/2407.13358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17249v2","updated":"2024-07-18T09:48:02Z","published":"2024-05-27T15:04:50Z","title":"Assessing LLMs Suitability for Knowledge Graph Completion","summary":"  Recent work has shown the capability of Large Language Models (LLMs) to solve\ntasks related to Knowledge Graphs, such as Knowledge Graph Completion, even in\nZero- or Few-Shot paradigms. However, they are known to hallucinate answers, or\noutput results in a non-deterministic manner, thus leading to wrongly reasoned\nresponses, even if they satisfy the user's demands. To highlight opportunities\nand challenges in knowledge graphs-related tasks, we experiment with three\ndistinguished LLMs, namely Mixtral-8x7b-Instruct-v0.1, GPT-3.5-Turbo-0125 and\nGPT-4o, on Knowledge Graph Completion for static knowledge graphs, using\nprompts constructed following the TELeR taxonomy, in Zero- and One-Shot\ncontexts, on a Task-Oriented Dialogue system use case. When evaluated using\nboth strict and flexible metrics measurement manners, our results show that\nLLMs could be fit for such a task if prompts encapsulate sufficient information\nand relevant examples.\n","authors":["Vasile Ionut Remus Iga","Gheorghe Cosmin Silaghi"],"pdf_url":"https://arxiv.org/pdf/2405.17249v2.pdf","comment":"Accepted at 18th International Conference on Neural-Symbolic Learning\n  and Reasoning, NESY 2024. Evaluating Mixtral-8x7b-Instruct-v0.1,\n  GPT-3.5-Turbo-0125 and GPT-4o for Knowledge Graph Completion task with\n  prompts formatted according to the TELeR taxonomy"},{"id":"http://arxiv.org/abs/2407.13343v1","updated":"2024-07-18T09:41:20Z","published":"2024-07-18T09:41:20Z","title":"Learning-From-Mistakes Prompting for Indigenous Language Translation","summary":"  Using large language models, this paper presents techniques to improve\nextremely low-resourced indigenous language translations. Our approaches are\ngrounded in the use of (1) the presence of a datastore consisting of a limited\nnumber of parallel translation examples, (2) the inherent capabilities of LLMs\nlike GPT-3.5, and (3) a word-level translation dictionary. We harness the\npotential of LLMs and in-context learning techniques in such a setting for\nusing LLMs as universal translators for extremely low-resourced languages. Our\nmethodology hinges on utilizing LLMs as language compilers for selected\nlanguage pairs, hypothesizing that they could internalize syntactic structures\nto facilitate accurate translation. We introduce three techniques: KNNPrompting\nwith Retrieved Prompting Context, Chain-of-Thought Prompting and\nLearningfrom-Mistakes Prompting, with the last method addressing past errors.\nThe evaluation results suggest that, even with limited corpora, LLMs can\neffectively translate extremely low-resource languages when paired with proper\nprompting.\n","authors":["You-Cheng Liao","Chen-Jui Yu","Chi-Yi Lin","He-Feng Yun","Yen-Hsiang Wang","Hsiao-Min Li","Yao-Chung Fan"],"pdf_url":"https://arxiv.org/pdf/2407.13343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13329v1","updated":"2024-07-18T09:29:33Z","published":"2024-07-18T09:29:33Z","title":"Why do you cite? An investigation on citation intents and\n  decision-making classification processes","summary":"  Identifying the reason for which an author cites another work is essential to\nunderstand the nature of scientific contributions and to assess their impact.\nCitations are one of the pillars of scholarly communication and most metrics\nemployed to analyze these conceptual links are based on quantitative\nobservations. Behind the act of referencing another scholarly work there is a\nwhole world of meanings that needs to be proficiently and effectively revealed.\nThis study emphasizes the importance of trustfully classifying citation intents\nto provide more comprehensive and insightful analyses in research assessment.\nWe address this task by presenting a study utilizing advanced Ensemble\nStrategies for Citation Intent Classification (CIC) incorporating Language\nModels (LMs) and employing Explainable AI (XAI) techniques to enhance the\ninterpretability and trustworthiness of models' predictions. Our approach\ninvolves two ensemble classifiers that utilize fine-tuned SciBERT and XLNet LMs\nas baselines. We further demonstrate the critical role of section titles as a\nfeature in improving models' performances. The study also introduces a web\napplication developed with Flask and currently available at\nhttp://137.204.64.4:81/cic/classifier, aimed at classifying citation intents.\nOne of our models sets as a new state-of-the-art (SOTA) with an 89.46% Macro-F1\nscore on the SciCite benchmark. The integration of XAI techniques provides\ninsights into the decision-making processes, highlighting the contributions of\nindividual words for level-0 classifications, and of individual models for the\nmetaclassification. The findings suggest that the inclusion of section titles\nsignificantly enhances classification performances in the CIC task. Our\ncontributions provide useful insights for developing more robust datasets and\nmethodologies, thus fostering a deeper understanding of scholarly\ncommunication.\n","authors":["Lorenzo Paolini","Sahar Vahdati","Angelo Di Iorio","Robert Wardenga","Ivan Heibi","Silvio Peroni"],"pdf_url":"https://arxiv.org/pdf/2407.13329v1.pdf","comment":"42 pages, 14 figures, 1 table, submitted to Scientometrics Journal"},{"id":"http://arxiv.org/abs/2407.13301v1","updated":"2024-07-18T09:06:27Z","published":"2024-07-18T09:06:27Z","title":"CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis","summary":"  The field of medical diagnosis has undergone a significant transformation\nwith the advent of large language models (LLMs), yet the challenges of\ninterpretability within these models remain largely unaddressed. This study\nintroduces Chain-of-Diagnosis (CoD) to enhance the interpretability of\nLLM-based medical diagnostics. CoD transforms the diagnostic process into a\ndiagnostic chain that mirrors a physician's thought process, providing a\ntransparent reasoning pathway. Additionally, CoD outputs the disease confidence\ndistribution to ensure transparency in decision-making. This interpretability\nmakes model diagnostics controllable and aids in identifying critical symptoms\nfor inquiry through the entropy reduction of confidences. With CoD, we\ndeveloped DiagnosisGPT, capable of diagnosing 9604 diseases. Experimental\nresults demonstrate that DiagnosisGPT outperforms other LLMs on diagnostic\nbenchmarks. Moreover, DiagnosisGPT provides interpretability while ensuring\ncontrollability in diagnostic rigor.\n","authors":["Junying Chen","Chi Gui","Anningzhe Gao","Ke Ji","Xidong Wang","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13300v1","updated":"2024-07-18T09:05:49Z","published":"2024-07-18T09:05:49Z","title":"Robust ASR Error Correction with Conservative Data Filtering","summary":"  Error correction (EC) based on large language models is an emerging\ntechnology to enhance the performance of automatic speech recognition (ASR)\nsystems. Generally, training data for EC are collected by automatically pairing\na large set of ASR hypotheses (as sources) and their gold references (as\ntargets). However, the quality of such pairs is not guaranteed, and we observed\nvarious types of noise which can make the EC models brittle, e.g. inducing\novercorrection in out-of-domain (OOD) settings. In this work, we propose two\nfundamental criteria that EC training data should satisfy: namely, EC targets\nshould (1) improve linguistic acceptability over sources and (2) be inferable\nfrom the available context (e.g. source phonemes). Through these criteria, we\nidentify low-quality EC pairs and train the models not to make any correction\nin such cases, the process we refer to as conservative data filtering. In our\nexperiments, we focus on Japanese ASR using a strong Conformer-CTC as the\nbaseline and finetune Japanese LLMs for EC. Through our evaluation on a suite\nof 21 internal benchmarks, we demonstrate that our approach can significantly\nreduce overcorrection and improve both the accuracy and quality of ASR results\nin the challenging OOD settings.\n","authors":["Takuma Udagawa","Masayuki Suzuki","Masayasu Muraoka","Gakuto Kurata"],"pdf_url":"https://arxiv.org/pdf/2407.13300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16640v2","updated":"2024-07-18T09:01:52Z","published":"2024-05-26T17:31:21Z","title":"A Survey of Multimodal Large Language Model from A Data-centric\n  Perspective","summary":"  Multimodal large language models (MLLMs) enhance the capabilities of standard\nlarge language models by integrating and processing data from multiple\nmodalities, including text, vision, audio, video, and 3D environments. Data\nplays a pivotal role in the development and refinement of these models. In this\nsurvey, we comprehensively review the literature on MLLMs from a data-centric\nperspective. Specifically, we explore methods for preparing multimodal data\nduring the pretraining and adaptation phases of MLLMs. Additionally, we analyze\nthe evaluation methods for the datasets and review the benchmarks for\nevaluating MLLMs. Our survey also outlines potential future research\ndirections. This work aims to provide researchers with a detailed understanding\nof the data-driven aspects of MLLMs, fostering further exploration and\ninnovation in this field.\n","authors":["Tianyi Bai","Hao Liang","Binwang Wan","Yanran Xu","Xi Li","Shiyu Li","Ling Yang","Bozhou Li","Yifan Wang","Bin Cui","Ping Huang","Jiulong Shan","Conghui He","Binhang Yuan","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.16640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13542v3","updated":"2024-07-18T09:00:23Z","published":"2024-06-19T13:29:53Z","title":"Self-play with Execution Feedback: Improving Instruction-following\n  Capabilities of Large Language Models","summary":"  One core capability of large language models (LLMs) is to follow natural\nlanguage instructions. However, the issue of automatically constructing\nhigh-quality training data to enhance the complex instruction-following\nabilities of LLMs without manual annotation remains unresolved. In this paper,\nwe introduce AutoIF, the first scalable and reliable method for automatically\ngenerating instruction-following training data. AutoIF transforms the\nvalidation of instruction-following data quality into code verification,\nrequiring LLMs to generate instructions, the corresponding code to check the\ncorrectness of the instruction responses, and unit test samples to verify the\ncode's correctness. Then, execution feedback-based rejection sampling can\ngenerate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from\nHuman Feedback (RLHF) training. AutoIF achieves significant improvements across\nthree training algorithms, SFT, Offline DPO, and Online DPO, when applied to\nthe top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and\nstrong-to-weak distillation settings. Our code is publicly available at\nhttps://github.com/QwenLM/AutoIF.\n","authors":["Guanting Dong","Keming Lu","Chengpeng Li","Tingyu Xia","Bowen Yu","Chang Zhou","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.13542v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2407.13297v1","updated":"2024-07-18T08:56:02Z","published":"2024-07-18T08:56:02Z","title":"SpeciaLex: A Benchmark for In-Context Specialized Lexicon Learning","summary":"  Specialized lexicons are collections of words with associated constraints\nsuch as special definitions, specific roles, and intended target audiences.\nThese constraints are necessary for content generation and documentation tasks\n(e.g., writing technical manuals or children's books), where the goal is to\nreduce the ambiguity of text content and increase its overall readability for a\nspecific group of audience. Understanding how large language models can capture\nthese constraints can help researchers build better, more impactful tools for\nwider use beyond the NLP community. Towards this end, we introduce SpeciaLex, a\nbenchmark for evaluating a language model's ability to follow specialized\nlexicon-based constraints across 18 diverse subtasks with 1,285 test instances\ncovering core tasks of Checking, Identification, Rewriting, and Open\nGeneration. We present an empirical evaluation of 15 open and closed-source\nLLMs and discuss insights on how factors such as model scale, openness, setup,\nand recency affect performance upon evaluating with the benchmark.\n","authors":["Joseph Marvin Imperial","Harish Tayyar Madabushi"],"pdf_url":"https://arxiv.org/pdf/2407.13297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13292v1","updated":"2024-07-18T08:46:47Z","published":"2024-07-18T08:46:47Z","title":"Low-Resourced Speech Recognition for Iu Mien Language via\n  Weakly-Supervised Phoneme-based Multilingual Pre-training","summary":"  The mainstream automatic speech recognition (ASR) technology usually requires\nhundreds to thousands of hours of annotated speech data. Three approaches to\nlow-resourced ASR are phoneme or subword based supervised pre-training, and\nself-supervised pre-training over multilingual data. The Iu Mien language is\nthe main ethnic language of the Yao ethnic group in China and is low-resourced\nin the sense that the annotated speech is very limited. With less than 10 hours\nof transcribed Iu Mien language, this paper investigates and compares the three\napproaches for Iu Mien speech recognition. Our experiments are based on the\nrecently released, three backbone models pretrained over the 10 languages from\nthe CommonVoice dataset (CV-Lang10), which correspond to the three approaches\nfor low-resourced ASR. It is found that phoneme supervision can achieve better\nresults compared to subword supervision and self-supervision, thereby providing\nhigher data-efficiency. Particularly, the Whistle models, i.e., obtained by the\nweakly-supervised phoneme-based multilingual pre-training, obtain the most\ncompetitive results.\n","authors":["Lukuan Dong","Donghong Qin","Fengbo Bai","Fanhua Song","Yan Liu","Chen Xu","Zhijian Ou"],"pdf_url":"https://arxiv.org/pdf/2407.13292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06027v4","updated":"2024-07-18T08:46:34Z","published":"2024-07-08T15:25:33Z","title":"PAS: Data-Efficient Plug-and-Play Prompt Augmentation System","summary":"  In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering.\n","authors":["Miao Zheng","Hao Liang","Fan Yang","Haoze Sun","Tianpeng Li","Lingchu Xiong","Yan Zhang","Youzhen Wu","Kun Li","Yanjun Shen","Mingan Lin","Tao Zhang","Guosheng Dong","Yujing Qiao","Kun Fang","Weipeng Chen","Bin Cui","Wentao Zhang","Zenan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.06027v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16374v3","updated":"2024-07-18T08:41:50Z","published":"2023-12-27T01:44:47Z","title":"LLM Factoscope: Uncovering LLMs' Factual Discernment through Inner\n  States Analysis","summary":"  Large Language Models (LLMs) have revolutionized various domains with\nextensive knowledge and creative capabilities. However, a critical issue with\nLLMs is their tendency to produce outputs that diverge from factual reality.\nThis phenomenon is particularly concerning in sensitive applications such as\nmedical consultation and legal advice, where accuracy is paramount. In this\npaper, we introduce the LLM factoscope, a novel Siamese network-based model\nthat leverages the inner states of LLMs for factual detection. Our\ninvestigation reveals distinguishable patterns in LLMs' inner states when\ngenerating factual versus non-factual content. We demonstrate the LLM\nfactoscope's effectiveness across various architectures, achieving over 96%\naccuracy in factual detection. Our work opens a new avenue for utilizing LLMs'\ninner states for factual detection and encourages further exploration into\nLLMs' inner workings for enhanced reliability and transparency.\n","authors":["Jinwen He","Yujia Gong","Kai Chen","Zijin Lin","Chengan Wei","Yue Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.16374v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18676v2","updated":"2024-07-18T08:28:09Z","published":"2024-06-26T18:26:53Z","title":"Understand What LLM Needs: Dual Preference Alignment for\n  Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) has demonstrated effectiveness in\nmitigating the hallucination problem of large language models (LLMs). However,\nthe difficulty of aligning the retriever with the diverse LLMs' knowledge\npreferences inevitably poses an inevitable challenge in developing a reliable\nRAG system. To address this issue, we propose DPA-RAG, a universal framework\ndesigned to align diverse knowledge preferences within RAG systems.\nSpecifically, we initially introduce a preference knowledge construction\npipline and incorporate five novel query augmentation strategies to alleviate\npreference data scarcity. Based on preference data, DPA-RAG accomplishes both\nexternal and internal preference alignment: 1) It jointly integrate pair-wise,\npoint-wise, and contrastive preference alignment abilities into the reranker,\nachieving external preference alignment among RAG components. 2) It further\nintroduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT),\nenabling LLMs to implicitly capture knowledge aligned with their reasoning\npreferences, achieving LLMs' internal alignment. Experimental results across\nfour knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all\nbaselines and seamlessly integrates both black-box and open-sourced LLM\nreaders. Further qualitative analysis and discussions also provide empirical\nguidance for achieving reliable RAG systems. Our code is publicly available at\nhttps://github.com/dongguanting/DPA-RAG.\n","authors":["Guanting Dong","Yutao Zhu","Chenghao Zhang","Zechen Wang","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2406.18676v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2407.13248v1","updated":"2024-07-18T08:02:49Z","published":"2024-07-18T08:02:49Z","title":"Are Large Language Models Capable of Generating Human-Level Narratives?","summary":"  This paper investigates the capability of LLMs in storytelling, focusing on\nnarrative development and plot progression. We introduce a novel computational\nframework to analyze narratives through three discourse-level aspects: i) story\narcs, ii) turning points, and iii) affective dimensions, including arousal and\nvalence. By leveraging expert and automatic annotations, we uncover significant\ndiscrepancies between the LLM- and human- written stories. While human-written\nstories are suspenseful, arousing, and diverse in narrative structures, LLM\nstories are homogeneously positive and lack tension. Next, we measure narrative\nreasoning skills as a precursor to generative capacities, concluding that most\nLLMs fall short of human abilities in discourse understanding. Finally, we show\nthat explicit integration of aforementioned discourse features can enhance\nstorytelling, as is demonstrated by over 40% improvement in neural storytelling\nin terms of diversity, suspense, and arousal.\n","authors":["Yufei Tian","Tenghao Huang","Miri Liu","Derek Jiang","Alexander Spangher","Muhao Chen","Jonathan May","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2407.13248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00126v2","updated":"2024-07-18T07:59:36Z","published":"2024-01-31T19:11:58Z","title":"Common Sense Reasoning for Deepfake Detection","summary":"  State-of-the-art deepfake detection approaches rely on image-based features\nextracted via neural networks. While these approaches trained in a supervised\nmanner extract likely fake features, they may fall short in representing\nunnatural `non-physical' semantic facial attributes -- blurry hairlines, double\neyebrows, rigid eye pupils, or unnatural skin shading. However, such facial\nattributes are easily perceived by humans and used to discern the authenticity\nof an image based on human common sense. Furthermore, image-based feature\nextraction methods that provide visual explanations via saliency maps can be\nhard to interpret for humans. To address these challenges, we frame deepfake\ndetection as a Deepfake Detection VQA (DD-VQA) task and model human intuition\nby providing textual explanations that describe common sense reasons for\nlabeling an image as real or fake. We introduce a new annotated dataset and\npropose a Vision and Language Transformer-based framework for the DD-VQA task.\nWe also incorporate text and image-aware feature alignment formulation to\nenhance multi-modal representation learning. As a result, we improve upon\nexisting deepfake detection models by integrating our learned vision\nrepresentations, which reason over common sense knowledge from the DD-VQA task.\nWe provide extensive empirical results demonstrating that our method enhances\ndetection performance, generalization ability, and language-based\ninterpretability in the deepfake detection task.\n","authors":["Yue Zhang","Ben Colman","Xiao Guo","Ali Shahriyari","Gaurav Bharaj"],"pdf_url":"https://arxiv.org/pdf/2402.00126v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13244v1","updated":"2024-07-18T07:57:31Z","published":"2024-07-18T07:57:31Z","title":"PM-LLM-Benchmark: Evaluating Large Language Models on Process Mining\n  Tasks","summary":"  Large Language Models (LLMs) have the potential to semi-automate some process\nmining (PM) analyses. While commercial models are already adequate for many\nanalytics tasks, the competitive level of open-source LLMs in PM tasks is\nunknown. In this paper, we propose PM-LLM-Benchmark, the first comprehensive\nbenchmark for PM focusing on domain knowledge (process-mining-specific and\nprocess-specific) and on different implementation strategies. We focus also on\nthe challenges in creating such a benchmark, related to the public availability\nof the data and on evaluation biases by the LLMs. Overall, we observe that most\nof the considered LLMs can perform some process mining tasks at a satisfactory\nlevel, but tiny models that would run on edge devices are still inadequate. We\nalso conclude that while the proposed benchmark is useful for identifying LLMs\nthat are adequate for process mining tasks, further research is needed to\novercome the evaluation biases and perform a more thorough ranking of the\ncompetitive LLMs.\n","authors":["Alessandro Berti","Humam Kourani","Wil M. P. van der Aalst"],"pdf_url":"https://arxiv.org/pdf/2407.13244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03814v2","updated":"2024-07-18T07:31:58Z","published":"2024-03-06T16:01:44Z","title":"Evaluating the Elementary Multilingual Capabilities of Large Language\n  Models with MultiQ","summary":"  Large language models (LLMs) need to serve everyone, including a global\nmajority of non-English speakers. However, most LLMs today, and open LLMs in\nparticular, are often intended for use in just English (e.g. Llama2, Mistral)\nor a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent\nresearch shows that, despite limits in their intended use, people prompt LLMs\nin many different languages. Therefore, in this paper, we investigate the basic\nmultilingual capabilities of state-of-the-art open LLMs beyond their intended\nuse. For this purpose, we introduce MultiQ, a new silver standard benchmark for\nbasic open-ended question answering with 27.4k test questions across a\ntypologically diverse set of 137 languages. With MultiQ, we evaluate language\nfidelity, i.e. whether models respond in the prompted language, and question\nanswering accuracy. All LLMs we test respond faithfully and/or accurately for\nat least some languages beyond their intended use. Most models are more\naccurate when they respond faithfully. However, differences across models are\nlarge, and there is a long tail of languages where models are neither accurate\nnor faithful. We explore differences in tokenization as a potential explanation\nfor our findings, identifying possible correlations that warrant further\ninvestigation.\n","authors":["Carolin Holtermann","Paul Röttger","Timm Dill","Anne Lauscher"],"pdf_url":"https://arxiv.org/pdf/2403.03814v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13228v1","updated":"2024-07-18T07:26:09Z","published":"2024-07-18T07:26:09Z","title":"Evaluating Large Language Models for Anxiety and Depression\n  Classification using Counseling and Psychotherapy Transcripts","summary":"  We aim to evaluate the efficacy of traditional machine learning and large\nlanguage models (LLMs) in classifying anxiety and depression from long\nconversational transcripts. We fine-tune both established transformer models\n(BERT, RoBERTa, Longformer) and more recent large models (Mistral-7B), trained\na Support Vector Machine with feature engineering, and assessed GPT models\nthrough prompting. We observe that state-of-the-art models fail to enhance\nclassification outcomes compared to traditional machine learning methods.\n","authors":["Junwei Sun","Siqi Ma","Yiran Fan","Peter Washington"],"pdf_url":"https://arxiv.org/pdf/2407.13228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19094v2","updated":"2024-07-18T06:53:22Z","published":"2024-03-28T02:12:49Z","title":"Learning From Correctness Without Prompting Makes LLM Efficient Reasoner","summary":"  Large language models (LLMs) have demonstrated outstanding performance across\nvarious tasks, yet they still exhibit limitations such as hallucination,\nunfaithful reasoning, and toxic content. One potential approach to mitigate\nthese issues is learning from human or external feedback (e.g. tools). In this\npaper, we introduce an intrinsic self-correct reasoning framework for LLMs that\neliminates the need for human feedback, external tools, and handcraft prompts.\nThe proposed framework, based on a multi-step reasoning paradigm\n\\textbf{Le}arning from \\textbf{Co}rrectness (\\textsc{LeCo}), improves reasoning\nperformance without needing to learn from errors. This paradigm prioritizes\nlearning from correct reasoning steps, and a unique method to measure\nconfidence for each reasoning step based on generation logits. Experimental\nresults across various multi-step reasoning tasks demonstrate the effectiveness\nof the framework in improving reasoning performance with reduced token\nconsumption.\n","authors":["Yuxuan Yao","Han Wu","Zhijiang Guo","Biyan Zhou","Jiahui Gao","Sichun Luo","Hanxu Hou","Xiaojin Fu","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2403.19094v2.pdf","comment":"Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2407.13205v1","updated":"2024-07-18T06:43:12Z","published":"2024-07-18T06:43:12Z","title":"Transformer-based Single-Cell Language Model: A Survey","summary":"  The transformers have achieved significant accomplishments in the natural\nlanguage processing as its outstanding parallel processing capabilities and\nhighly flexible attention mechanism. In addition, increasing studies based on\ntransformers have been proposed to model single-cell data. In this review, we\nattempt to systematically summarize the single-cell language models and\napplications based on transformers. First, we provide a detailed introduction\nabout the structure and principles of transformers. Then, we review the\nsingle-cell language models and large language models for single-cell data\nanalysis. Moreover, we explore the datasets and applications of single-cell\nlanguage models in downstream tasks such as batch correction, cell clustering,\ncell type annotation, gene regulatory network inference and perturbation\nresponse. Further, we discuss the challenges of single-cell language models and\nprovide promising research directions. We hope this review will serve as an\nup-to-date reference for researchers interested in the direction of single-cell\nlanguage models.\n","authors":["Wei Lan","Guohang He","Mingyang Liu","Qingfeng Chen","Junyue Cao","Wei Peng"],"pdf_url":"https://arxiv.org/pdf/2407.13205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05161v2","updated":"2024-07-18T06:35:17Z","published":"2023-11-09T06:19:51Z","title":"Enhancing Computation Efficiency in Large Language Models through Weight\n  and Activation Quantization","summary":"  Large Language Models (LLMs) are proficient in natural language processing\ntasks, but their deployment is often restricted by extensive parameter sizes\nand computational demands. This paper focuses on post-training quantization\n(PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8)\nquantization, to enhance computational efficiency -- a topic less explored\ncompared to weight-only quantization. We present two innovative techniques:\nactivation-quantization-aware scaling (AQAS) and sequence-length-aware\ncalibration (SLAC) to enhance PTQ by considering the combined effects on\nweights and activations and aligning calibration sequence lengths to target\ntasks. Moreover, we introduce dINT, a hybrid data format combining integer and\ndenormal representations, to address the underflow issue in W4A8 quantization,\nwhere small values are rounded to zero. Through rigorous evaluations of LLMs,\nincluding OPT and LLaMA, we demonstrate that our techniques significantly boost\ntask accuracies to levels comparable with full-precision models. By developing\narithmetic units compatible with dINT, we further confirm that our methods\nyield a 2$\\times$ hardware efficiency improvement compared to 8-bit integer MAC\nunit.\n","authors":["Janghwan Lee","Minsoo Kim","Seungcheol Baek","Seok Joong Hwang","Wonyong Sung","Jungwook Choi"],"pdf_url":"https://arxiv.org/pdf/2311.05161v2.pdf","comment":"EMNLP 2023 Main Conference. Corrected an error in the first author\n  name"},{"id":"http://arxiv.org/abs/2407.03051v2","updated":"2024-07-18T06:21:23Z","published":"2024-07-03T12:19:06Z","title":"Improving Conversational Abilities of Quantized Large Language Models\n  via Direct Preference Alignment","summary":"  The rapid advancement of large language models (LLMs) has facilitated their\ntransformation into conversational chatbots that can grasp contextual nuances\nand generate pertinent sentences, closely mirroring human values through\nadvanced techniques such as instruction tuning and reinforcement learning from\nhuman feedback (RLHF). However, the computational efficiency required for LLMs,\nachieved through techniques like post-training quantization (PTQ), presents\nchallenges such as token-flipping that can impair chatbot performance. In\nresponse, we propose a novel preference alignment approach, quantization-aware\ndirect preference optimization (QDPO), that aligns quantized LLMs with their\nfull-precision counterparts, improving conversational abilities. Evaluated on\ntwo instruction-tuned LLMs in various languages, QDPO demonstrated superior\nperformance in improving conversational abilities compared to established PTQ\nand knowledge-distillation fine-tuning techniques, marking a significant step\nforward in the development of efficient and effective conversational LLMs.\n","authors":["Janghwan Lee","Seongmin Park","Sukjin Hong","Minsoo Kim","Du-Seong Chang","Jungwook Choi"],"pdf_url":"https://arxiv.org/pdf/2407.03051v2.pdf","comment":"ACL 2024 Main"},{"id":"http://arxiv.org/abs/2405.07764v2","updated":"2024-07-18T06:11:41Z","published":"2024-05-13T14:07:15Z","title":"LGDE: Local Graph-based Dictionary Expansion","summary":"  We present Local Graph-based Dictionary Expansion (LGDE), a method for\ndata-driven discovery of the semantic neighbourhood of words using tools from\nmanifold learning and network science. At the heart of LGDE lies the creation\nof a word similarity graph from the geometry of word embeddings followed by\nlocal community detection based on graph diffusion. The diffusion in the local\ngraph manifold allows the exploration of the complex nonlinear geometry of word\nembeddings to capture word similarities based on paths of semantic association,\nover and above direct pairwise similarities. Exploiting such semantic\nneighbourhoods enables the expansion of dictionaries of pre-selected keywords,\nan important step for tasks in information retrieval, such as database queries\nand online data collection. We validate LGDE on a corpus of English-language\nhate speech-related posts from Reddit and Gab and show that LGDE enriches the\nlist of keywords with significantly better performance than threshold methods\nbased on direct word similarities. We further demonstrate our method through a\nreal-world use case from communication science, where LGDE is evaluated\nquantitatively on the expansion of a conspiracy-related dictionary from online\ndata collected and analysed by domain experts. Our empirical results and expert\nuser assessment indicate that LGDE expands the seed dictionary with more useful\nkeywords due to the manifold-learning-based similarity network.\n","authors":["Dominik J. Schindler","Sneha Jha","Xixuan Zhang","Kilian Buehling","Annett Heft","Mauricio Barahona"],"pdf_url":"https://arxiv.org/pdf/2405.07764v2.pdf","comment":"Python code available at:\n  https://github.com/barahona-research-group/LGDE"},{"id":"http://arxiv.org/abs/2407.13193v1","updated":"2024-07-18T06:06:53Z","published":"2024-07-18T06:06:53Z","title":"Retrieval-Augmented Generation for Natural Language Processing: A Survey","summary":"  Large language models (LLMs) have demonstrated great success in various\nfields, benefiting from their huge amount of parameters that store knowledge.\nHowever, LLMs still suffer from several key issues, such as hallucination\nproblems, knowledge update issues, and lacking domain-specific expertise. The\nappearance of retrieval-augmented generation (RAG), which leverages an external\nknowledge database to augment LLMs, makes up those drawbacks of LLMs. This\npaper reviews all significant techniques of RAG, especially in the retriever\nand the retrieval fusions. Besides, tutorial codes are provided for\nimplementing the representative techniques in RAG. This paper further discusses\nthe RAG training, including RAG with/without datastore update. Then, we\nintroduce the application of RAG in representative natural language processing\ntasks and industrial scenarios. Finally, this paper discusses the future\ndirections and challenges of RAG for promoting its development.\n","authors":["Shangyu Wu","Ying Xiong","Yufei Cui","Haolun Wu","Can Chen","Ye Yuan","Lianming Huang","Xue Liu","Tei-Wei Kuo","Nan Guan","Chun Jason Xue"],"pdf_url":"https://arxiv.org/pdf/2407.13193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13102v2","updated":"2024-07-18T05:45:45Z","published":"2023-11-22T02:04:35Z","title":"Detecting out-of-distribution text using topological features of\n  transformer-based language models","summary":"  To safeguard machine learning systems that operate on textual data against\nout-of-distribution (OOD) inputs that could cause unpredictable behaviour, we\nexplore the use of topological features of self-attention maps from\ntransformer-based language models to detect when input text is out of\ndistribution. Self-attention forms the core of transformer-based language\nmodels, dynamically assigning vectors to words based on context, thus in theory\nour methodology is applicable to any transformer-based language model with\nmultihead self-attention. We evaluate our approach on BERT and compare it to a\ntraditional OOD approach using CLS embeddings. Our results show that our\napproach outperforms CLS embeddings in distinguishing in-distribution samples\nfrom far-out-of-domain samples, but struggles with near or same-domain\ndatasets.\n","authors":["Andres Pollano","Anupam Chaudhuri","Anj Simmons"],"pdf_url":"https://arxiv.org/pdf/2311.13102v2.pdf","comment":"8 pages, 6 figures, 3 tables, to be published in proceedings of the\n  IJCAI-2024 AISafety Workshop"},{"id":"http://arxiv.org/abs/2311.17371v3","updated":"2024-07-18T05:18:14Z","published":"2023-11-29T05:54:41Z","title":"Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs","summary":"  Recent advancements in large language models (LLMs) underscore their\npotential for responding to inquiries in various domains. However, ensuring\nthat generative agents provide accurate and reliable answers remains an ongoing\nchallenge. In this context, multi-agent debate (MAD) has emerged as a promising\nstrategy for enhancing the truthfulness of LLMs. We benchmark a range of\ndebating and prompting strategies to explore the trade-offs between cost, time,\nand accuracy. Importantly, we find that multi-agent debating systems, in their\ncurrent form, do not reliably outperform other proposed prompting strategies,\nsuch as self-consistency and ensembling using multiple reasoning paths.\nHowever, when performing hyperparameter tuning, several MAD systems, such as\nMulti-Persona, perform better. This suggests that MAD protocols might not be\ninherently worse than other approaches, but that they are more sensitive to\ndifferent hyperparameter settings and difficult to optimize. We build on these\nresults to offer insights into improving debating strategies, such as adjusting\nagent agreement levels, which can significantly enhance performance and even\nsurpass all other non-debate protocols we evaluated. We provide an open-source\nrepository to the community with several state-of-the-art protocols together\nwith evaluation scripts to benchmark across popular research datasets.\n","authors":["Andries Smit","Paul Duckworth","Nathan Grinsztajn","Thomas D. Barrett","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2311.17371v3.pdf","comment":"2 pages, 13 figures"},{"id":"http://arxiv.org/abs/2407.13168v1","updated":"2024-07-18T05:15:24Z","published":"2024-07-18T05:15:24Z","title":"SciCode: A Research Coding Benchmark Curated by Scientists","summary":"  Since language models (LMs) now outperform average humans on many challenging\ntasks, it has become increasingly difficult to develop challenging,\nhigh-quality, and realistic evaluations. We address this issue by examining\nLMs' capabilities to generate code for solving real scientific research\nproblems. Incorporating input from scientists and AI researchers in 16 diverse\nnatural science sub-fields, including mathematics, physics, chemistry, biology,\nand materials science, we created a scientist-curated coding benchmark,\nSciCode. The problems in SciCode naturally factorize into multiple subproblems,\neach involving knowledge recall, reasoning, and code synthesis. In total,\nSciCode contains 338 subproblems decomposed from 80 challenging main problems.\nIt offers optional descriptions specifying useful scientific background\ninformation and scientist-annotated gold-standard solutions and test cases for\nevaluation. Claude3.5-Sonnet, the best-performing model among those tested, can\nsolve only 4.6% of the problems in the most realistic setting. We believe that\nSciCode demonstrates both contemporary LMs' progress towards becoming helpful\nscientific assistants and sheds light on the development and evaluation of\nscientific AI in the future.\n","authors":["Minyang Tian","Luyu Gao","Shizhuo Dylan Zhang","Xinan Chen","Cunwei Fan","Xuefei Guo","Roland Haas","Pan Ji","Kittithat Krongchon","Yao Li","Shengyan Liu","Di Luo","Yutao Ma","Hao Tong","Kha Trinh","Chenyu Tian","Zihan Wang","Bohao Wu","Yanyu Xiong","Shengzhu Yin","Minhui Zhu","Kilian Lieret","Yanxin Lu","Genglin Liu","Yufeng Du","Tianhua Tao","Ofir Press","Jamie Callan","Eliu Huerta","Hao Peng"],"pdf_url":"https://arxiv.org/pdf/2407.13168v1.pdf","comment":"25 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2407.13164v1","updated":"2024-07-18T05:08:09Z","published":"2024-07-18T05:08:09Z","title":"Translate-and-Revise: Boosting Large Language Models for Constrained\n  Translation","summary":"  Imposing constraints on machine translation systems presents a challenging\nissue because these systems are not trained to make use of constraints in\ngenerating adequate, fluent translations. In this paper, we leverage the\ncapabilities of large language models (LLMs) for constrained translation, given\nthat LLMs can easily adapt to this task by taking translation instructions and\nconstraints as prompts. However, LLMs cannot always guarantee the adequacy of\ntranslation, and, in some cases, ignore the given constraints. This is in part\nbecause LLMs might be overly confident in their predictions, overriding the\ninfluence of the constraints. To overcome this overiding behaviour, we propose\nto add a revision process that encourages LLMs to correct the outputs by\nprompting them about the constraints that have not yet been met. We evaluate\nour approach on four constrained translation tasks, encompassing both lexical\nand structural constraints in multiple constraint domains. Experiments show\n15\\% improvement in constraint-based translation accuracy over standard LLMs\nand the approach also significantly outperforms neural machine translation\n(NMT) state-of-the-art methods.\n","authors":["Pengcheng Huang","Yongyu Mu","Yuzhang Wu","Bei Li","Chunyang Xiao","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.13164v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2404.05449v3","updated":"2024-07-18T04:44:25Z","published":"2024-04-08T12:31:23Z","title":"RoT: Enhancing Large Language Models with Reflection on Search Trees","summary":"  Large language models (LLMs) have demonstrated impressive capability in\nreasoning and planning when integrated with tree-search-based prompting\nmethods. However, since these methods ignore the previous search experiences,\nthey often make the same mistakes in the search process. To address this issue,\nwe introduce Reflection on search Trees (RoT), an LLM reflection framework\ndesigned to improve the performance of tree-search-based prompting methods. It\nuses a strong LLM to summarize guidelines from previous tree search experiences\nto enhance the ability of a weak LLM. The guidelines are instructions about\nsolving this task through tree search which can prevent the weak LLMs from\nmaking similar mistakes in the past search process. In addition, we proposed a\nnovel state selection method, which identifies the critical information from\nhistorical search processes to help RoT generate more specific and meaningful\nguidelines. In our extensive experiments, we find that RoT significantly\nimproves the performance of LLMs in reasoning or planning tasks with various\ntree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based\nprompting methods such as Chain-of-Thought (CoT) can also benefit from RoT\nguidelines since RoT can provide task-specific knowledge collected from the\nsearch experience.\n","authors":["Wenyang Hui","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2404.05449v3.pdf","comment":"9 pages main"},{"id":"http://arxiv.org/abs/2407.13153v1","updated":"2024-07-18T04:42:01Z","published":"2024-07-18T04:42:01Z","title":"Preset-Voice Matching for Privacy Regulated Speech-to-Speech Translation\n  Systems","summary":"  In recent years, there has been increased demand for speech-to-speech\ntranslation (S2ST) systems in industry settings. Although successfully\ncommercialized, cloning-based S2ST systems expose their distributors to\nliabilities when misused by individuals and can infringe on personality rights\nwhen exploited by media organizations. This work proposes a regulated S2ST\nframework called Preset-Voice Matching (PVM). PVM removes cross-lingual voice\ncloning in S2ST by first matching the input voice to a similar prior consenting\nspeaker voice in the target-language. With this separation, PVM avoids cloning\nthe input speaker, ensuring PVM systems comply with regulations and reduce risk\nof misuse. Our results demonstrate PVM can significantly improve S2ST system\nrun-time in multi-speaker settings and the naturalness of S2ST synthesized\nspeech. To our knowledge, PVM is the first explicitly regulated S2ST framework\nleveraging similarly-matched preset-voices for dynamic S2ST tasks.\n","authors":["Daniel Platnick","Bishoy Abdelnour","Eamon Earl","Rahul Kumar","Zahra Rezaei","Thomas Tsangaris","Faraj Lagum"],"pdf_url":"https://arxiv.org/pdf/2407.13153v1.pdf","comment":"Accepted to the ACL PrivateNLP 2024 Workshop, 7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.07693v3","updated":"2024-07-18T04:24:57Z","published":"2024-06-11T20:14:22Z","title":"A Labelled Dataset for Sentiment Analysis of Videos on YouTube, TikTok,\n  and Other Sources about the 2024 Outbreak of Measles","summary":"  The work of this paper presents a dataset that contains the data of 4011\nvideos about the ongoing outbreak of measles published on 264 websites on the\ninternet between January 1, 2024, and May 31, 2024. The dataset is available at\nhttps://dx.doi.org/10.21227/40s8-xf63. These websites primarily include YouTube\nand TikTok, which account for 48.6% and 15.2% of the videos, respectively. The\nremainder of the websites include Instagram and Facebook as well as the\nwebsites of various global and local news organizations. For each of these\nvideos, the URL of the video, title of the post, description of the post, and\nthe date of publication of the video are presented as separate attributes in\nthe dataset. After developing this dataset, sentiment analysis (using VADER),\nsubjectivity analysis (using TextBlob), and fine-grain sentiment analysis\n(using DistilRoBERTa-base) of the video titles and video descriptions were\nperformed. This included classifying each video title and video description\ninto (i) one of the sentiment classes i.e. positive, negative, or neutral, (ii)\none of the subjectivity classes i.e. highly opinionated, neutral opinionated,\nor least opinionated, and (iii) one of the fine-grain sentiment classes i.e.\nfear, surprise, joy, sadness, anger, disgust, or neutral. These results are\npresented as separate attributes in the dataset for the training and testing of\nmachine learning algorithms for performing sentiment analysis or subjectivity\nanalysis in this field as well as for other applications. Finally, this paper\nalso presents a list of open research questions that may be investigated using\nthis dataset.\n","authors":["Nirmalya Thakur","Vanessa Su","Mingchen Shao","Kesha A. Patel","Hongseok Jeong","Victoria Knieling","Andrew Bian"],"pdf_url":"https://arxiv.org/pdf/2406.07693v3.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2407.12393v2","updated":"2024-07-18T04:18:43Z","published":"2024-07-17T08:13:22Z","title":"PersLLM: A Personified Training Approach for Large Language Models","summary":"  Large language models exhibit aspects of human-level intelligence that\ncatalyze their application as human-like agents in domains such as social\nsimulations, human-machine interactions, and collaborative multi-agent systems.\nHowever, the absence of distinct personalities, such as displaying ingratiating\nbehaviors, inconsistent opinions, and uniform response patterns, diminish LLMs\nutility in practical applications. Addressing this, the development of\npersonality traits in LLMs emerges as a crucial area of research to unlock\ntheir latent potential. Existing methods to personify LLMs generally involve\nstrategies like employing stylized training data for instruction tuning or\nusing prompt engineering to simulate different personalities. These methods\nonly capture superficial linguistic styles instead of the core of personalities\nand are therefore not stable. In this study, we propose PersLLM, integrating\npsychology-grounded principles of personality: social practice, consistency,\nand dynamic development, into a comprehensive training methodology. We\nincorporate personality traits directly into the model parameters, enhancing\nthe model's resistance to induction, promoting consistency, and supporting the\ndynamic evolution of personality. Single-agent evaluation validates our\nmethod's superiority, as it produces responses more aligned with reference\npersonalities compared to other approaches. Case studies for multi-agent\ncommunication highlight its benefits in enhancing opinion consistency within\nindividual agents and fostering collaborative creativity among multiple agents\nin dialogue contexts, potentially benefiting human simulation and multi-agent\ncooperation. Additionally, human-agent interaction evaluations indicate that\nour personified models significantly enhance interactive experiences,\nunderscoring the practical implications of our research.\n","authors":["Zheni Zeng","Jiayi Chen","Huimin Chen","Yukun Yan","Yuxuan Chen","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2407.12393v2.pdf","comment":"10 pages for main text, 5 figures"},{"id":"http://arxiv.org/abs/2407.13142v1","updated":"2024-07-18T04:01:12Z","published":"2024-07-18T04:01:12Z","title":"A light-weight and efficient punctuation and word casing prediction\n  model for on-device streaming ASR","summary":"  Punctuation and word casing prediction are necessary for automatic speech\nrecognition (ASR). With the popularity of on-device end-to-end streaming ASR\nsystems, the on-device punctuation and word casing prediction become a\nnecessity while we found little discussion on this. With the emergence of\nTransformer, Transformer based models have been explored for this scenario.\nHowever, Transformer based models are too large for on-device ASR systems. In\nthis paper, we propose a light-weight and efficient model that jointly predicts\npunctuation and word casing in real time. The model is based on Convolutional\nNeural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM).\nExperimental results on the IWSLT2011 test set show that the proposed model\nobtains 9% relative improvement compared to the best of non-Transformer models\non overall F1-score. Compared to the representative of Transformer based\nmodels, the proposed model achieves comparable results to the representative\nmodel while being only one-fortieth its size and 2.5 times faster in terms of\ninference time. It is suitable for on-device streaming ASR systems. Our code is\npublicly available.\n","authors":["Jian You","Xiangfeng Li"],"pdf_url":"https://arxiv.org/pdf/2407.13142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13115v1","updated":"2024-07-18T02:50:40Z","published":"2024-07-18T02:50:40Z","title":"TrialEnroll: Predicting Clinical Trial Enrollment Success with Deep &\n  Cross Network and Large Language Models","summary":"  Clinical trials need to recruit a sufficient number of volunteer patients to\ndemonstrate the statistical power of the treatment (e.g., a new drug) in curing\na certain disease. Clinical trial recruitment has a significant impact on trial\nsuccess. Forecasting whether the recruitment process would be successful before\nwe run the trial would save many resources and time. This paper develops a\nnovel deep & cross network with large language model (LLM)-augmented text\nfeature that learns semantic information from trial eligibility criteria and\npredicts enrollment success. The proposed method enables interpretability by\nunderstanding which sentence/word in eligibility criteria contributes heavily\nto prediction. We also demonstrate the empirical superiority of the proposed\nmethod (0.7002 PR-AUC) over a bunch of well-established machine learning\nmethods. The code and curated dataset are publicly available at\nhttps://anonymous.4open.science/r/TrialEnroll-7E12.\n","authors":["Ling Yue","Sixue Xing","Jintai Chen","Tianfan Fu"],"pdf_url":"https://arxiv.org/pdf/2407.13115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10671v3","updated":"2024-07-18T02:39:24Z","published":"2024-07-15T12:35:42Z","title":"Qwen2 Technical Report","summary":"  This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.\n","authors":["An Yang","Baosong Yang","Binyuan Hui","Bo Zheng","Bowen Yu","Chang Zhou","Chengpeng Li","Chengyuan Li","Dayiheng Liu","Fei Huang","Guanting Dong","Haoran Wei","Huan Lin","Jialong Tang","Jialin Wang","Jian Yang","Jianhong Tu","Jianwei Zhang","Jianxin Ma","Jianxin Yang","Jin Xu","Jingren Zhou","Jinze Bai","Jinzheng He","Junyang Lin","Kai Dang","Keming Lu","Keqin Chen","Kexin Yang","Mei Li","Mingfeng Xue","Na Ni","Pei Zhang","Peng Wang","Ru Peng","Rui Men","Ruize Gao","Runji Lin","Shijie Wang","Shuai Bai","Sinan Tan","Tianhang Zhu","Tianhao Li","Tianyu Liu","Wenbin Ge","Xiaodong Deng","Xiaohuan Zhou","Xingzhang Ren","Xinyu Zhang","Xipin Wei","Xuancheng Ren","Xuejing Liu","Yang Fan","Yang Yao","Yichang Zhang","Yu Wan","Yunfei Chu","Yuqiong Liu","Zeyu Cui","Zhenru Zhang","Zhifang Guo","Zhihao Fan"],"pdf_url":"https://arxiv.org/pdf/2407.10671v3.pdf","comment":"25 pages, 1 figure"},{"id":"http://arxiv.org/abs/2401.10005v2","updated":"2024-07-18T02:35:30Z","published":"2024-01-18T14:21:56Z","title":"Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and\n  Visual Question Generation","summary":"  The increasing demand for intelligent systems capable of interpreting and\nreasoning about visual content requires the development of large\nVision-and-Language Models (VLMs) that are not only accurate but also have\nexplicit reasoning capabilities. This paper presents a novel approach to\ndevelop a VLM with the ability to conduct explicit reasoning based on visual\ncontent and textual instructions. We introduce a system that can ask a question\nto acquire necessary knowledge, thereby enhancing the robustness and\nexplicability of the reasoning process. To this end, we developed a novel\ndataset generated by a Large Language Model (LLM), designed to promote\nchain-of-thought reasoning combined with a question-asking mechanism. The\ndataset covers a range of tasks, from common ones like caption generation to\nspecialized VQA tasks that require expert knowledge. Furthermore, using the\ndataset we created, we fine-tuned an existing VLM. This training enabled the\nmodels to generate questions and perform iterative reasoning during inference.\nThe results demonstrated a stride toward a more robust, accurate, and\ninterpretable VLM, capable of reasoning explicitly and seeking information\nproactively when confronted with ambiguous visual input.\n","authors":["Kohei Uehara","Nabarun Goswami","Hanqin Wang","Toshiaki Baba","Kohtaro Tanaka","Tomohiro Hashimoto","Kai Wang","Rei Ito","Takagi Naoya","Ryo Umagami","Yingyi Wen","Tanachai Anakewat","Tatsuya Harada"],"pdf_url":"https://arxiv.org/pdf/2401.10005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00936v2","updated":"2024-07-18T02:19:34Z","published":"2024-07-01T03:37:35Z","title":"Large Language Model Enhanced Knowledge Representation Learning: A\n  Survey","summary":"  The integration of Large Language Models (LLM) with Knowledge Representation\nLearning (KRL) signifies a significant advancement in the field of artificial\nintelligence (AI), enhancing the ability to capture and utilize both structure\nand textual information. Despite the increasing research on enhancing KRL with\nLLMs, a thorough survey that analyse processes of these enhanced models is\nconspicuously absent. Our survey addresses this by categorizing these models\nbased on three distinct Transformer architectures, and by analyzing\nexperimental data from various KRL downstream tasks to evaluate the strengths\nand weaknesses of each approach. Finally, we identify and explore potential\nfuture research directions in this emerging yet underexplored domain.\n","authors":["Xin Wang","Zirui Chen","Haofen Wang","Leong Hou U","Zhao Li","Wenbin Guo"],"pdf_url":"https://arxiv.org/pdf/2407.00936v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13101v1","updated":"2024-07-18T02:19:00Z","published":"2024-07-18T02:19:00Z","title":"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with\n  an Iterative Approach","summary":"  Multi-hop question answering is a challenging task with distinct industrial\nrelevance, and Retrieval-Augmented Generation (RAG) methods based on large\nlanguage models (LLMs) have become a popular approach to tackle this task.\nOwing to the potential inability to retrieve all necessary information in a\nsingle iteration, a series of iterative RAG methods has been recently\ndeveloped, showing significant performance improvements. However, existing\nmethods still face two critical challenges: context overload resulting from\nmultiple rounds of retrieval, and over-planning and repetitive planning due to\nthe lack of a recorded retrieval trajectory. In this paper, we propose a novel\niterative RAG method called ReSP, equipped with a dual-function summarizer.\nThis summarizer compresses information from retrieved documents, targeting both\nthe overarching question and the current sub-question concurrently.\nExperimental results on the multi-hop question-answering datasets HotpotQA and\n2WikiMultihopQA demonstrate that our method significantly outperforms the\nstate-of-the-art, and exhibits excellent robustness concerning context length.\n","authors":["Zhouyu Jiang","Mengshu Sun","Lei Liang","Zhiqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.13101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13097v1","updated":"2024-07-18T02:13:50Z","published":"2024-07-18T02:13:50Z","title":"AlcLaM: Arabic Dialectal Language Model","summary":"  Pre-trained Language Models (PLMs) are integral to many modern natural\nlanguage processing (NLP) systems. Although multilingual models cover a wide\nrange of languages, they often grapple with challenges like high inference\ncosts and a lack of diverse non-English training data. Arabic-specific PLMs are\ntrained predominantly on modern standard Arabic, which compromises their\nperformance on regional dialects. To tackle this, we construct an Arabic\ndialectal corpus comprising 3.4M sentences gathered from social media\nplatforms. We utilize this corpus to expand the vocabulary and retrain a\nBERT-based model from scratch. Named AlcLaM, our model was trained using only\n13 GB of text, which represents a fraction of the data used by existing models\nsuch as CAMeL, MARBERT, and ArBERT, compared to 7.8%, 10.2%, and 21.3%,\nrespectively. Remarkably, AlcLaM demonstrates superior performance on a variety\nof Arabic NLP tasks despite the limited training data. AlcLaM is available at\nGitHub https://github.com/amurtadha/Alclam and HuggingFace\nhttps://huggingface.co/rahbi.\n","authors":["Murtadha Ahmed","Saghir Alfasly","Bo Wen","Jamaal Qasem","Mohammed Ahmed","Yunfeng Liu"],"pdf_url":"https://arxiv.org/pdf/2407.13097v1.pdf","comment":"Accepted by ArabicNLP 2024, presented in ACL 2024"},{"id":"http://arxiv.org/abs/2404.01015v2","updated":"2024-07-18T02:00:41Z","published":"2024-04-01T09:35:06Z","title":"PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison","summary":"  Building a reliable and automated evaluation metric is a necessary but\nchallenging problem for open-domain dialogue systems. Recent studies proposed\nevaluation metrics that assess generated responses by considering their\nrelevance to previous dialogue histories. Although effective, these metrics\nevaluate individual responses directly rather than considering their relative\nquality compared to other responses. To handle this, we propose PairEval, a\nnovel dialogue evaluation metric for assessing responses by comparing their\nquality against responses in different conversations. PairEval is built on top\nof open-sourced and moderate-size language models, and we make them specialized\nin pairwise comparison between dialogue responses. Extensive experiments on\nmultiple benchmarks demonstrate that our metric exhibits a higher correlation\nwith human judgments than baseline metrics. We also find that the proposed\ncomparative metric is more robust in detecting common failures from open-domain\ndialogue systems, including repetition and speaker insensitivity.\n","authors":["ChaeHun Park","Minseok Choi","Dohyun Lee","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2404.01015v2.pdf","comment":"COLM2024 (accepted)"},{"id":"http://arxiv.org/abs/2403.15992v2","updated":"2024-07-18T01:56:03Z","published":"2024-03-24T03:10:07Z","title":"BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval","summary":"  The burgeoning integration of 3D medical imaging into healthcare has led to a\nsubstantial increase in the workload of medical professionals. To assist\nclinicians in their diagnostic processes and alleviate their workload, the\ndevelopment of a robust system for retrieving similar case studies presents a\nviable solution. While the concept holds great promise, the field of 3D medical\ntext-image retrieval is currently limited by the absence of robust evaluation\nbenchmarks and curated datasets. To remedy this, our study presents a\ngroundbreaking dataset, {BIMCV-R}, which includes an extensive collection of\n8,069 3D CT volumes, encompassing over 2 million slices, paired with their\nrespective radiological reports. Expanding upon the foundational work of our\ndataset, we craft a retrieval strategy, MedFinder. This approach employs a\ndual-stream network architecture, harnessing the potential of large language\nmodels to advance the field of medical image retrieval beyond existing\ntext-image retrieval solutions. It marks our preliminary step towards\ndeveloping a system capable of facilitating text-to-image, image-to-text, and\nkeyword-based retrieval tasks. Our project is available at\n\\url{https://huggingface.co/datasets/cyd0806/BIMCV-R}.\n","authors":["Yinda Chen","Che Liu","Xiaoyu Liu","Rossella Arcucci","Zhiwei Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.15992v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13089v1","updated":"2024-07-18T01:33:20Z","published":"2024-07-18T01:33:20Z","title":"MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for\n  Fact-Checking","summary":"  Fact-checking real-world claims often requires reviewing multiple multimodal\ndocuments to assess a claim's truthfulness, which is a highly laborious and\ntime-consuming task. In this paper, we present a summarization model designed\nto generate claim-specific summaries useful for fact-checking from multimodal,\nmulti-document datasets. The model takes inputs in the form of documents,\nimages, and a claim, with the objective of assisting in fact-checking tasks. We\nintroduce a dynamic perceiver-based model that can handle inputs from multiple\nmodalities of arbitrary lengths. To train our model, we leverage a novel\nreinforcement learning-based entailment objective to generate summaries that\nprovide evidence distinguishing between different truthfulness labels. To\nassess the efficacy of our approach, we conduct experiments on both an existing\nbenchmark and a new dataset of multi-document claims that we contribute. Our\napproach outperforms the SOTA approach by 4.6% in the claim verification task\non the MOCHEG dataset and demonstrates strong performance on our new\nMulti-News-Fact-Checking dataset.\n","authors":["Ting-Chih Chen","Chia-Wei Tang","Chris Thomas"],"pdf_url":"https://arxiv.org/pdf/2407.13089v1.pdf","comment":"16 pages, 7 figures, The 62nd Annual Meeting of the Association for\n  Computational Linguistics"},{"id":"http://arxiv.org/abs/2407.13069v1","updated":"2024-07-18T00:28:04Z","published":"2024-07-18T00:28:04Z","title":"Dynamic Sentiment Analysis with Local Large Language Models using\n  Majority Voting: A Study on Factors Affecting Restaurant Evaluation","summary":"  User-generated contents (UGCs) on online platforms allow marketing\nresearchers to understand consumer preferences for products and services. With\nthe advance of large language models (LLMs), some studies utilized the models\nfor annotation and sentiment analysis. However, the relationship between the\naccuracy and the hyper-parameters of LLMs is yet to be thoroughly examined. In\naddition, the issues of variability and reproducibility of results from each\ntrial of LLMs have rarely been considered in existing literature. Since actual\nhuman annotation uses majority voting to resolve disagreements among\nannotators, this study introduces a majority voting mechanism to a sentiment\nanalysis model using local LLMs. By a series of three analyses of online\nreviews on restaurant evaluations, we demonstrate that majority voting with\nmultiple attempts using a medium-sized model produces more robust results than\nusing a large model with a single attempt. Furthermore, we conducted further\nanalysis to investigate the effect of each aspect on the overall evaluation.\n","authors":["Junichiro Niimi"],"pdf_url":"https://arxiv.org/pdf/2407.13069v1.pdf","comment":"This manuscript is under peer review"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.13772v1","updated":"2024-07-18T17:59:58Z","published":"2024-07-18T17:59:58Z","title":"GroupMamba: Parameter-Efficient and Accurate Group Visual State Space\n  Model","summary":"  Recent advancements in state-space models (SSMs) have showcased effective\nperformance in modeling long-range dependencies with subquadratic complexity.\nHowever, pure SSM-based models still face challenges related to stability and\nachieving optimal performance on computer vision tasks. Our paper addresses the\nchallenges of scaling SSM-based models for computer vision, particularly the\ninstability and inefficiency of large model sizes. To address this, we\nintroduce a Modulated Group Mamba layer which divides the input channels into\nfour groups and applies our proposed SSM-based efficient Visual Single\nSelective Scanning (VSSS) block independently to each group, with each VSSS\nblock scanning in one of the four spatial directions. The Modulated Group Mamba\nlayer also wraps the four VSSS blocks into a channel modulation operator to\nimprove cross-channel communication. Furthermore, we introduce a\ndistillation-based training objective to stabilize the training of large\nmodels, leading to consistent performance gains. Our comprehensive experiments\ndemonstrate the merits of the proposed contributions, leading to superior\nperformance over existing methods for image classification on ImageNet-1K,\nobject detection, instance segmentation on MS-COCO, and semantic segmentation\non ADE20K. Our tiny variant with 23M parameters achieves state-of-the-art\nperformance with a classification top-1 accuracy of 83.3% on ImageNet-1K, while\nbeing 26% efficient in terms of parameters, compared to the best existing Mamba\ndesign of same model size. Our code and models are available at:\nhttps://github.com/Amshaker/GroupMamba.\n","authors":["Abdelrahman Shaker","Syed Talal Wasim","Salman Khan","Juergen Gall","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2407.13772v1.pdf","comment":"Preprint. Our code and models are available at:\n  https://github.com/Amshaker/GroupMamba"},{"id":"http://arxiv.org/abs/2407.13771v1","updated":"2024-07-18T17:59:57Z","published":"2024-07-18T17:59:57Z","title":"Training-Free Model Merging for Multi-target Domain Adaptation","summary":"  In this paper, we study multi-target domain adaptation of scene understanding\nmodels. While previous methods achieved commendable results through\ninter-domain consistency losses, they often assumed unrealistic simultaneous\naccess to images from all target domains, overlooking constraints such as data\ntransfer bandwidth limitations and data privacy concerns. Given these\nchallenges, we pose the question: How to merge models adapted independently on\ndistinct domains while bypassing the need for direct access to training data?\nOur solution to this problem involves two components, merging model parameters\nand merging model buffers (i.e., normalization layer statistics). For merging\nmodel parameters, empirical analyses of mode connectivity surprisingly reveal\nthat linear merging suffices when employing the same pretrained backbone\nweights for adapting separate models. For merging model buffers, we model the\nreal-world distribution with a Gaussian prior and estimate new statistics from\nthe buffers of separately trained models. Our method is simple yet effective,\nachieving comparable performance with data combination training baselines,\nwhile eliminating the need for accessing training data. Project page:\nhttps://air-discover.github.io/ModelMerging\n","authors":["Wenyi Li","Huan-ang Gao","Mingju Gao","Beiwen Tian","Rong Zhi","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.13771v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2404.01300v3","updated":"2024-07-18T17:59:48Z","published":"2024-04-01T17:59:55Z","title":"NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation\n  Learning for Neural Radiance Fields","summary":"  Neural fields excel in computer vision and robotics due to their ability to\nunderstand the 3D visual world such as inferring semantics, geometry, and\ndynamics. Given the capabilities of neural fields in densely representing a 3D\nscene from 2D images, we ask the question: Can we scale their self-supervised\npretraining, specifically using masked autoencoders, to generate effective 3D\nrepresentations from posed RGB images. Owing to the astounding success of\nextending transformers to novel data modalities, we employ standard 3D Vision\nTransformers to suit the unique formulation of NeRFs. We leverage NeRF's\nvolumetric grid as a dense input to the transformer, contrasting it with other\n3D representations such as pointclouds where the information density can be\nuneven, and the representation is irregular. Due to the difficulty of applying\nmasked autoencoders to an implicit representation, such as NeRF, we opt for\nextracting an explicit representation that canonicalizes scenes across domains\nby employing the camera trajectory for sampling. Our goal is made possible by\nmasking random patches from NeRF's radiance and density grid and employing a\nstandard 3D Swin Transformer to reconstruct the masked patches. In doing so,\nthe model can learn the semantic and spatial structure of complete scenes. We\npretrain this representation at scale on our proposed curated posed-RGB data,\ntotaling over 1.8 million images. Once pretrained, the encoder is used for\neffective 3D transfer learning. Our novel self-supervised pretraining for\nNeRFs, NeRF-MAE, scales remarkably well and improves performance on various\nchallenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,\nNeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF\nscene understanding baselines on Front3D and ScanNet datasets with an absolute\nperformance improvement of over 20% AP50 and 8% AP25 for 3D object detection.\n","authors":["Muhammad Zubair Irshad","Sergey Zakharov","Vitor Guizilini","Adrien Gaidon","Zsolt Kira","Rares Ambrus"],"pdf_url":"https://arxiv.org/pdf/2404.01300v3.pdf","comment":"Accepted to ECCV 2024. Project Page: https://nerf-mae.github.io/"},{"id":"http://arxiv.org/abs/2407.13768v1","updated":"2024-07-18T17:59:44Z","published":"2024-07-18T17:59:44Z","title":"Addressing Imbalance for Class Incremental Learning in Medical Image\n  Classification","summary":"  Deep convolutional neural networks have made significant breakthroughs in\nmedical image classification, under the assumption that training samples from\nall classes are simultaneously available. However, in real-world medical\nscenarios, there's a common need to continuously learn about new diseases,\nleading to the emerging field of class incremental learning (CIL) in the\nmedical domain. Typically, CIL suffers from catastrophic forgetting when\ntrained on new classes. This phenomenon is mainly caused by the imbalance\nbetween old and new classes, and it becomes even more challenging with\nimbalanced medical datasets. In this work, we introduce two simple yet\neffective plug-in methods to mitigate the adverse effects of the imbalance.\nFirst, we propose a CIL-balanced classification loss to mitigate the classifier\nbias toward majority classes via logit adjustment. Second, we propose a\ndistribution margin loss that not only alleviates the inter-class overlap in\nembedding space but also enforces the intra-class compactness. We evaluate the\neffectiveness of our method with extensive experiments on three benchmark\ndatasets (CCH5000, HAM10000, and EyePACS). The results demonstrate that our\napproach outperforms state-of-the-art methods.\n","authors":["Xuze Hao","Wenqian Ni","Xuhao Jiang","Weimin Tan","Bo Yan"],"pdf_url":"https://arxiv.org/pdf/2407.13768v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2403.09635v2","updated":"2024-07-18T17:59:35Z","published":"2024-03-14T17:59:14Z","title":"Transformers Get Stable: An End-to-End Signal Propagation Theory for\n  Language Models","summary":"  In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 1000\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across encoder-only, decoder-only and\nencoder-decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for Image Classification.\n","authors":["Akhil Kedia","Mohd Abbas Zaidi","Sushil Khyalia","Jungho Jung","Harshith Goka","Haejun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.09635v2.pdf","comment":"Accepted at ICML 2024. Source code is available at\n  https://github.com/akhilkedia/TranformersGetStable. Akhil Kedia, Mohd Abbas\n  Zaidi, Sushil Khyalia equal contribution"},{"id":"http://arxiv.org/abs/2407.13766v1","updated":"2024-07-18T17:59:30Z","published":"2024-07-18T17:59:30Z","title":"Visual Haystacks: Answering Harder Questions About Sets of Images","summary":"  Recent advancements in Large Multimodal Models (LMMs) have made significant\nprogress in the field of single-image visual question answering. However, these\nmodels face substantial challenges when tasked with queries that span extensive\ncollections of images, similar to real-world scenarios like searching through\nlarge photo albums, finding specific information across the internet, or\nmonitoring environmental changes through satellite imagery. This paper explores\nthe task of Multi-Image Visual Question Answering (MIQA): given a large set of\nimages and a natural language query, the task is to generate a relevant and\ngrounded response. We propose a new public benchmark, dubbed \"Visual Haystacks\n(VHs),\" specifically designed to evaluate LMMs' capabilities in visual\nretrieval and reasoning over sets of unrelated images, where we perform\ncomprehensive evaluations demonstrating that even robust closed-source models\nstruggle significantly. Towards addressing these shortcomings, we introduce\nMIRAGE (Multi-Image Retrieval Augmented Generation), a novel retrieval/QA\nframework tailored for LMMs that confronts the challenges of MIQA with marked\nefficiency and accuracy improvements over baseline methods. Our evaluation\nshows that MIRAGE surpasses closed-source GPT-4o models by up to 11% on the VHs\nbenchmark and offers up to 3.4x improvements in efficiency over text-focused\nmulti-stage approaches.\n","authors":["Tsung-Han Wu","Giscard Biamby","Jerome Quenum","Ritwik Gupta","Joseph E. Gonzalez","Trevor Darrell","David M. Chan"],"pdf_url":"https://arxiv.org/pdf/2407.13766v1.pdf","comment":"Project page: https://visual-haystacks.github.io"},{"id":"http://arxiv.org/abs/2407.13764v1","updated":"2024-07-18T17:59:08Z","published":"2024-07-18T17:59:08Z","title":"Shape of Motion: 4D Reconstruction from a Single Video","summary":"  Monocular dynamic reconstruction is a challenging and long-standing vision\nproblem due to the highly ill-posed nature of the task. Existing approaches are\nlimited in that they either depend on templates, are effective only in\nquasi-static scenes, or fail to model 3D motion explicitly. In this work, we\nintroduce a method capable of reconstructing generic dynamic scenes, featuring\nexplicit, full-sequence-long 3D motion, from casually captured monocular\nvideos. We tackle the under-constrained nature of the problem with two key\ninsights: First, we exploit the low-dimensional structure of 3D motion by\nrepresenting scene motion with a compact set of SE3 motion bases. Each point's\nmotion is expressed as a linear combination of these bases, facilitating soft\ndecomposition of the scene into multiple rigidly-moving groups. Second, we\nutilize a comprehensive set of data-driven priors, including monocular depth\nmaps and long-range 2D tracks, and devise a method to effectively consolidate\nthese noisy supervisory signals, resulting in a globally consistent\nrepresentation of the dynamic scene. Experiments show that our method achieves\nstate-of-the-art performance for both long-range 3D/2D motion estimation and\nnovel view synthesis on dynamic scenes. Project Page:\nhttps://shape-of-motion.github.io/\n","authors":["Qianqian Wang","Vickie Ye","Hang Gao","Jake Austin","Zhengqi Li","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2407.13764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13761v1","updated":"2024-07-18T17:58:03Z","published":"2024-07-18T17:58:03Z","title":"SegPoint: Segment Any Point Cloud via Large Language Model","summary":"  Despite significant progress in 3D point cloud segmentation, existing methods\nprimarily address specific tasks and depend on explicit instructions to\nidentify targets, lacking the capability to infer and understand implicit user\nintentions in a unified framework. In this work, we propose a model, called\nSegPoint, that leverages the reasoning capabilities of a multi-modal Large\nLanguage Model (LLM) to produce point-wise segmentation masks across a diverse\nrange of tasks: 1) 3D instruction segmentation, 2) 3D referring segmentation,\n3) 3D semantic segmentation, and 4) 3D open-vocabulary semantic segmentation.\nTo advance 3D instruction research, we introduce a new benchmark, Instruct3D,\ndesigned to evaluate segmentation performance from complex and implicit\ninstructional texts, featuring 2,565 point cloud-instruction pairs. Our\nexperimental results demonstrate that SegPoint achieves competitive performance\non established benchmarks such as ScanRefer for referring segmentation and\nScanNet for semantic segmentation, while delivering outstanding outcomes on the\nInstruct3D dataset. To our knowledge, SegPoint is the first model to address\nthese varied segmentation tasks within a single framework, achieving\nsatisfactory performance.\n","authors":["Shuting He","Henghui Ding","Xudong Jiang","Bihan Wen"],"pdf_url":"https://arxiv.org/pdf/2407.13761v1.pdf","comment":"ECCV 2024, Project Page: https://heshuting555.github.io/SegPoint"},{"id":"http://arxiv.org/abs/2407.13759v1","updated":"2024-07-18T17:56:30Z","published":"2024-07-18T17:56:30Z","title":"Streetscapes: Large-scale Consistent Street View Generation Using\n  Autoregressive Video Diffusion","summary":"  We present a method for generating Streetscapes-long sequences of views\nthrough an on-the-fly synthesized city-scale scene. Our generation is\nconditioned by language input (e.g., city name, weather), as well as an\nunderlying map/layout hosting the desired trajectory. Compared to recent models\nfor video generation or 3D view synthesis, our method can scale to much\nlonger-range camera trajectories, spanning several city blocks, while\nmaintaining visual quality and consistency. To achieve this goal, we build on\nrecent work on video diffusion, used within an autoregressive framework that\ncan easily scale to long sequences. In particular, we introduce a new temporal\nimputation method that prevents our autoregressive approach from drifting from\nthe distribution of realistic city imagery. We train our Streetscapes system on\na compelling source of data-posed imagery from Google Street View, along with\ncontextual map data-which allows users to generate city views conditioned on\nany desired city layout, with controllable camera poses. Please see more\nresults at our project page at https://boyangdeng.com/streetscapes.\n","authors":["Boyang Deng","Richard Tucker","Zhengqi Li","Leonidas Guibas","Noah Snavely","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2407.13759v1.pdf","comment":"*Equal Contributions, Project Page:\n  https://boyangdeng.com/streetscapes"},{"id":"http://arxiv.org/abs/2407.13753v1","updated":"2024-07-18T17:55:01Z","published":"2024-07-18T17:55:01Z","title":"Exploring Facial Biomarkers for Depression through Temporal Analysis of\n  Action Units","summary":"  Depression is characterized by persistent sadness and loss of interest,\nsignificantly impairing daily functioning and now a widespread mental disorder.\nTraditional diagnostic methods rely on subjective assessments, necessitating\nobjective approaches for accurate diagnosis. Our study investigates the use of\nfacial action units (AUs) and emotions as biomarkers for depression. We\nanalyzed facial expressions from video data of participants classified with or\nwithout depression. Our methodology involved detailed feature extraction, mean\nintensity comparisons of key AUs, and the application of time series\nclassification models. Furthermore, we employed Principal Component Analysis\n(PCA) and various clustering algorithms to explore the variability in emotional\nexpression patterns. Results indicate significant differences in the\nintensities of AUs associated with sadness and happiness between the groups,\nhighlighting the potential of facial analysis in depression assessment.\n","authors":["Aditya Parikh","Misha Sadeghi","Bjorn Eskofier"],"pdf_url":"https://arxiv.org/pdf/2407.13753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13752v1","updated":"2024-07-18T17:54:49Z","published":"2024-07-18T17:54:49Z","title":"LogoSticker: Inserting Logos into Diffusion Models for Customized\n  Generation","summary":"  Recent advances in text-to-image model customization have underscored the\nimportance of integrating new concepts with a few examples. Yet, these\nprogresses are largely confined to widely recognized subjects, which can be\nlearned with relative ease through models' adequate shared prior knowledge. In\ncontrast, logos, characterized by unique patterns and textual elements, are\nhard to establish shared knowledge within diffusion models, thus presenting a\nunique challenge. To bridge this gap, we introduce the task of logo insertion.\nOur goal is to insert logo identities into diffusion models and enable their\nseamless synthesis in varied contexts. We present a novel two-phase pipeline\nLogoSticker to tackle this task. First, we propose the actor-critic relation\npre-training algorithm, which addresses the nontrivial gaps in models'\nunderstanding of the potential spatial positioning of logos and interactions\nwith other objects. Second, we propose a decoupled identity learning algorithm,\nwhich enables precise localization and identity extraction of logos.\nLogoSticker can generate logos accurately and harmoniously in diverse contexts.\nWe comprehensively validate the effectiveness of LogoSticker over customization\nmethods and large models such as DALLE~3.\n\\href{https://mingkangz.github.io/logosticker}{Project page}.\n","authors":["Mingkang Zhu","Xi Chen","Zhongdao Wang","Hengshuang Zhao","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2407.13752v1.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2407.13750v1","updated":"2024-07-18T17:53:51Z","published":"2024-07-18T17:53:51Z","title":"Pose-guided multi-task video transformer for driver action recognition","summary":"  We investigate the task of identifying situations of distracted driving\nthrough analysis of in-car videos. To tackle this challenge we introduce a\nmulti-task video transformer that predicts both distracted actions and driver\npose. Leveraging VideoMAEv2, a large pre-trained architecture, our approach\nincorporates semantic information from human keypoint locations to enhance\naction recognition and decrease computational overhead by minimizing the number\nof spatio-temporal tokens. By guiding token selection with pose and class\ninformation, we notably reduce the model's computational requirements while\npreserving the baseline accuracy. Our model surpasses existing state-of-the art\nresults in driver action recognition while exhibiting superior efficiency\ncompared to current video transformer-based approaches.\n","authors":["Ricardo Pizarro","Roberto Valle","Luis Miguel Bergasa","José M. Buenaposada","Luis Baumela"],"pdf_url":"https://arxiv.org/pdf/2407.13750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13748v1","updated":"2024-07-18T17:52:08Z","published":"2024-07-18T17:52:08Z","title":"General Geometry-aware Weakly Supervised 3D Object Detection","summary":"  3D object detection is an indispensable component for scene understanding.\nHowever, the annotation of large-scale 3D datasets requires significant human\neffort. To tackle this problem, many methods adopt weakly supervised 3D object\ndetection that estimates 3D boxes by leveraging 2D boxes and\nscene/class-specific priors. However, these approaches generally depend on\nsophisticated manual priors, which is hard to generalize to novel categories\nand scenes. In this paper, we are motivated to propose a general approach,\nwhich can be easily adapted to new scenes and/or classes. A unified framework\nis developed for learning 3D object detectors from RGB images and associated 2D\nboxes. In specific, we propose three general components: prior injection module\nto obtain general object geometric priors from LLM model, 2D space projection\nconstraint to minimize the discrepancy between the boundaries of projected 3D\nboxes and their corresponding 2D boxes on the image plane, and 3D space\ngeometry constraint to build a Point-to-Box alignment loss to further refine\nthe pose of estimated 3D boxes. Experiments on KITTI and SUN-RGBD datasets\ndemonstrate that our method yields surprisingly high-quality 3D bounding boxes\nwith only 2D annotation. The source code is available at\nhttps://github.com/gwenzhang/GGA.\n","authors":["Guowen Zhang","Junsong Fan","Liyi Chen","Zhaoxiang Zhang","Zhen Lei","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.13748v1.pdf","comment":"Accepted to ECCV24"},{"id":"http://arxiv.org/abs/2407.13745v1","updated":"2024-07-18T17:50:03Z","published":"2024-07-18T17:50:03Z","title":"MaRINeR: Enhancing Novel Views by Matching Rendered Images with Nearby\n  References","summary":"  Rendering realistic images from 3D reconstruction is an essential task of\nmany Computer Vision and Robotics pipelines, notably for mixed-reality\napplications as well as training autonomous agents in simulated environments.\nHowever, the quality of novel views heavily depends of the source\nreconstruction which is often imperfect due to noisy or missing geometry and\nappearance. Inspired by the recent success of reference-based super-resolution\nnetworks, we propose MaRINeR, a refinement method that leverages information of\na nearby mapping image to improve the rendering of a target viewpoint. We first\nestablish matches between the raw rendered image of the scene geometry from the\ntarget viewpoint and the nearby reference based on deep features, followed by\nhierarchical detail transfer. We show improved renderings in quantitative\nmetrics and qualitative examples from both explicit and implicit scene\nrepresentations. We further employ our method on the downstream tasks of\npseudo-ground-truth validation, synthetic data enhancement and detail recovery\nfor renderings of reduced 3D reconstructions.\n","authors":["Lukas Bösiger","Mihai Dusmanu","Marc Pollefeys","Zuria Bauer"],"pdf_url":"https://arxiv.org/pdf/2407.13745v1.pdf","comment":"Accepted to ECCV 2024; Project Page: see\n  https://boelukas.github.io/mariner/"},{"id":"http://arxiv.org/abs/2407.10920v2","updated":"2024-07-18T17:49:16Z","published":"2024-07-15T17:21:41Z","title":"Benchmarking Vision Language Models for Cultural Understanding","summary":"  Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.\n","authors":["Shravan Nayak","Kanishk Jain","Rabiul Awal","Siva Reddy","Sjoerd van Steenkiste","Lisa Anne Hendricks","Karolina Stańczak","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.10920v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07320v2","updated":"2024-07-18T17:43:12Z","published":"2024-06-11T14:49:04Z","title":"A Framework for Efficient Model Evaluation through Stratification,\n  Sampling, and Estimation","summary":"  Model performance evaluation is a critical and expensive task in machine\nlearning and computer vision. Without clear guidelines, practitioners often\nestimate model accuracy using a one-time completely random selection of the\ndata. However, by employing tailored sampling and estimation strategies, one\ncan obtain more precise estimates and reduce annotation costs. In this paper,\nwe propose a statistical framework for model evaluation that includes\nstratification, sampling, and estimation components. We examine the statistical\nproperties of each component and evaluate their efficiency (precision). One key\nresult of our work is that stratification via k-means clustering based on\naccurate predictions of model performance yields efficient estimators. Our\nexperiments on computer vision datasets show that this method consistently\nprovides more precise accuracy estimates than the traditional simple random\nsampling, even with substantial efficiency gains of 10x. We also find that\nmodel-assisted estimators, which leverage predictions of model accuracy on the\nunlabeled portion of the dataset, are generally more efficient than the\ntraditional estimates based solely on the labeled data.\n","authors":["Riccardo Fogliato","Pratik Patil","Mathew Monfort","Pietro Perona"],"pdf_url":"https://arxiv.org/pdf/2406.07320v2.pdf","comment":"To appear at ECCV 2024"},{"id":"http://arxiv.org/abs/2406.09648v2","updated":"2024-07-18T17:31:20Z","published":"2024-06-14T00:40:31Z","title":"An Intrinsic Vector Heat Network","summary":"  Vector fields are widely used to represent and model flows for many science\nand engineering applications. This paper introduces a novel neural network\narchitecture for learning tangent vector fields that are intrinsically defined\non manifold surfaces embedded in 3D. Previous approaches to learning vector\nfields on surfaces treat vectors as multi-dimensional scalar fields, using\ntraditional scalar-valued architectures to process channels individually, thus\nfail to preserve fundamental intrinsic properties of the vector field. The core\nidea of this work is to introduce a trainable vector heat diffusion module to\nspatially propagate vector-valued feature data across the surface, which we\nincorporate into our proposed architecture that consists of vector-valued\nneurons. Our architecture is invariant to rigid motion of the input, isometric\ndeformation, and choice of local tangent bases, and is robust to\ndiscretizations of the surface. We evaluate our Vector Heat Network on triangle\nmeshes, and empirically validate its invariant properties. We also demonstrate\nthe effectiveness of our method on the useful industrial application of\nquadrilateral mesh generation.\n","authors":["Alexander Gao","Maurice Chu","Mubbasir Kapadia","Ming C. Lin","Hsueh-Ti Derek Liu"],"pdf_url":"https://arxiv.org/pdf/2406.09648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13719v1","updated":"2024-07-18T17:18:25Z","published":"2024-07-18T17:18:25Z","title":"HazeCLIP: Towards Language Guided Real-World Image Dehazing","summary":"  Existing methods have achieved remarkable performance in single image\ndehazing, particularly on synthetic datasets. However, they often struggle with\nreal-world hazy images due to domain shift, limiting their practical\napplicability. This paper introduces HazeCLIP, a language-guided adaptation\nframework designed to enhance the real-world performance of pre-trained\ndehazing networks. Inspired by the Contrastive Language-Image Pre-training\n(CLIP) model's ability to distinguish between hazy and clean images, we utilize\nit to evaluate dehazing results. Combined with a region-specific dehazing\ntechnique and tailored prompt sets, CLIP model accurately identifies hazy\nareas, providing a high-quality, human-like prior that guides the fine-tuning\nprocess of pre-trained networks. Extensive experiments demonstrate that\nHazeCLIP achieves the state-of-the-art performance in real-word image dehazing,\nevaluated through both visual quality and no-reference quality assessments. The\ncode is available: https://github.com/Troivyn/HazeCLIP .\n","authors":["Ruiyi Wang","Wenhao Li","Xiaohong Liu","Chunyi Li","Zicheng Zhang","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2407.13719v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.13715v1","updated":"2024-07-18T17:11:29Z","published":"2024-07-18T17:11:29Z","title":"Attention Based Simple Primitives for Open World Compositional Zero-Shot\n  Learning","summary":"  Compositional Zero-Shot Learning (CZSL) aims to predict unknown compositions\nmade up of attribute and object pairs. Predicting compositions unseen during\ntraining is a challenging task. We are exploring Open World Compositional\nZero-Shot Learning (OW-CZSL) in this study, where our test space encompasses\nall potential combinations of attributes and objects. Our approach involves\nutilizing the self-attention mechanism between attributes and objects to\nachieve better generalization from seen to unseen compositions. Utilizing a\nself-attention mechanism facilitates the model's ability to identify\nrelationships between attribute and objects. The similarity between the\nself-attended textual and visual features is subsequently calculated to\ngenerate predictions during the inference phase. The potential test space may\nencompass implausible object-attribute combinations arising from unrestricted\nattribute-object pairings. To mitigate this issue, we leverage external\nknowledge from ConceptNet to restrict the test space to realistic compositions.\nOur proposed model, Attention-based Simple Primitives (ASP), demonstrates\ncompetitive performance, achieving results comparable to the state-of-the-art.\n","authors":["Ans Munir","Faisal Z. Qureshi","Muhammad Haris Khan","Mohsen Ali"],"pdf_url":"https://arxiv.org/pdf/2407.13715v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.13708v1","updated":"2024-07-18T17:07:32Z","published":"2024-07-18T17:07:32Z","title":"Are We Ready for Out-of-Distribution Detection in Digital Pathology?","summary":"  The detection of semantic and covariate out-of-distribution (OOD) examples is\na critical yet overlooked challenge in digital pathology (DP). Recently,\nsubstantial insight and methods on OOD detection were presented by the ML\ncommunity, but how do they fare in DP applications? To this end, we establish a\nbenchmark study, our highlights being: 1) the adoption of proper evaluation\nprotocols, 2) the comparison of diverse detectors in both a single and\nmulti-model setting, and 3) the exploration into advanced ML settings like\ntransfer learning (ImageNet vs. DP pre-training) and choice of architecture\n(CNNs vs. transformers). Through our comprehensive experiments, we contribute\nnew insights and guidelines, paving the way for future research and discussion.\n","authors":["Ji-Hun Oh","Kianoush Falahkheirkhah","Rohit Bhargava"],"pdf_url":"https://arxiv.org/pdf/2407.13708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00148v2","updated":"2024-07-18T17:07:17Z","published":"2024-06-28T17:57:12Z","title":"Localizing Anomalies via Multiscale Score Matching Analysis","summary":"  Anomaly detection and localization in medical imaging remain critical\nchallenges in healthcare. This paper introduces Spatial-MSMA (Multiscale Score\nMatching Analysis), a novel unsupervised method for anomaly localization in\nvolumetric brain MRIs. Building upon the MSMA framework, our approach\nincorporates spatial information and conditional likelihoods to enhance anomaly\ndetection capabilities. We employ a flexible normalizing flow model conditioned\non patch positions and global image features to estimate patch-wise anomaly\nscores. The method is evaluated on a dataset of 1,650 T1- and T2-weighted brain\nMRIs from typically developing children, with simulated lesions added to the\ntest set. Spatial-MSMA significantly outperforms existing methods, including\nreconstruction-based, generative-based, and interpretation-based approaches, in\nlesion detection and segmentation tasks. Our model achieves superior\nperformance in both distance-based metrics (99th percentile Hausdorff Distance:\n$7.05 \\pm 0.61$, Mean Surface Distance: $2.10 \\pm 0.43$) and component-wise\nmetrics (True Positive Rate: $0.83 \\pm 0.01$, Positive Predictive Value: $0.96\n\\pm 0.01$). These results demonstrate Spatial-MSMA's potential for accurate and\ninterpretable anomaly localization in medical imaging, with implications for\nimproved diagnosis and treatment planning in clinical settings. Our code is\navailable at~\\url{https://github.com/ahsanMah/sade/}.\n","authors":["Ahsan Mahmood","Junier Oliva","Martin Styner"],"pdf_url":"https://arxiv.org/pdf/2407.00148v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13700v1","updated":"2024-07-18T17:01:10Z","published":"2024-07-18T17:01:10Z","title":"Cross-Task Attack: A Self-Supervision Generative Framework Based on\n  Attention Shift","summary":"  Studying adversarial attacks on artificial intelligence (AI) systems helps\ndiscover model shortcomings, enabling the construction of a more robust system.\nMost existing adversarial attack methods only concentrate on single-task\nsingle-model or single-task cross-model scenarios, overlooking the multi-task\ncharacteristic of artificial intelligence systems. As a result, most of the\nexisting attacks do not pose a practical threat to a comprehensive and\ncollaborative AI system. However, implementing cross-task attacks is highly\ndemanding and challenging due to the difficulty in obtaining the real labels of\ndifferent tasks for the same picture and harmonizing the loss functions across\ndifferent tasks. To address this issue, we propose a self-supervised Cross-Task\nAttack framework (CTA), which utilizes co-attention and anti-attention maps to\ngenerate cross-task adversarial perturbation. Specifically, the co-attention\nmap reflects the area to which different visual task models pay attention,\nwhile the anti-attention map reflects the area that different visual task\nmodels neglect. CTA generates cross-task perturbations by shifting the\nattention area of samples away from the co-attention map and closer to the\nanti-attention map. We conduct extensive experiments on multiple vision tasks\nand the experimental results confirm the effectiveness of the proposed design\nfor adversarial attacks.\n","authors":["Qingyuan Zeng","Yunpeng Gong","Min Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.13700v1.pdf","comment":"Has been accepted by IJCNN2024"},{"id":"http://arxiv.org/abs/2406.02774v2","updated":"2024-07-18T16:59:08Z","published":"2024-06-04T20:43:26Z","title":"Diffusion-Refined VQA Annotations for Semi-Supervised Gaze Following","summary":"  Training gaze following models requires a large number of images with gaze\ntarget coordinates annotated by human annotators, which is a laborious and\ninherently ambiguous process. We propose the first semi-supervised method for\ngaze following by introducing two novel priors to the task. We obtain the first\nprior using a large pretrained Visual Question Answering (VQA) model, where we\ncompute Grad-CAM heatmaps by `prompting' the VQA model with a gaze following\nquestion. These heatmaps can be noisy and not suited for use in training. The\nneed to refine these noisy annotations leads us to incorporate a second prior.\nWe utilize a diffusion model trained on limited human annotations and modify\nthe reverse sampling process to refine the Grad-CAM heatmaps. By tuning the\ndiffusion process we achieve a trade-off between the human annotation prior and\nthe VQA heatmap prior, which retains the useful VQA prior information while\nexhibiting similar properties to the training data distribution. Our method\noutperforms simple pseudo-annotation generation baselines on the GazeFollow\nimage dataset. More importantly, our pseudo-annotation strategy, applied to a\nwidely used supervised gaze following model (VAT), reduces the annotation need\nby 50%. Our method also performs the best on the VideoAttentionTarget dataset.\n","authors":["Qiaomu Miao","Alexandros Graikos","Jingwei Zhang","Sounak Mondal","Minh Hoai","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2406.02774v2.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13689v1","updated":"2024-07-18T16:57:11Z","published":"2024-07-18T16:57:11Z","title":"Shaded Route Planning Using Active Segmentation and Identification of\n  Satellite Images","summary":"  Heatwaves pose significant health risks, particularly due to prolonged\nexposure to high summer temperatures. Vulnerable groups, especially pedestrians\nand cyclists on sun-exposed sidewalks, motivate the development of a route\nplanning method that incorporates somatosensory temperature effects through\nshade ratio consideration. This paper is the first to introduce a pipeline that\nutilizes segmentation foundation models to extract shaded areas from\nhigh-resolution satellite images. These areas are then integrated into a\nmulti-layered road map, enabling users to customize routes based on a balance\nbetween distance and shade exposure, thereby enhancing comfort and health\nduring outdoor activities. Specifically, we construct a graph-based\nrepresentation of the road map, where links indicate connectivity and are\nupdated with shade ratio data for dynamic route planning. This system is\nalready implemented online, with a video demonstration, and will be\nspecifically adapted to assist travelers during the 2024 Olympic Games in\nParis.\n","authors":["Longchao Da","Rohan Chhibba","Rushabh Jaiswal","Ariane Middel","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2407.13689v1.pdf","comment":"Paper accepted to CIKM24 demo track"},{"id":"http://arxiv.org/abs/2407.13680v1","updated":"2024-07-18T16:54:02Z","published":"2024-07-18T16:54:02Z","title":"HPix: Generating Vector Maps from Satellite Images","summary":"  Vector maps find widespread utility across diverse domains due to their\ncapacity to not only store but also represent discrete data boundaries such as\nbuilding footprints, disaster impact analysis, digitization, urban planning,\nlocation points, transport links, and more. Although extensive research exists\non identifying building footprints and road types from satellite imagery, the\ngeneration of vector maps from such imagery remains an area with limited\nexploration. Furthermore, conventional map generation techniques rely on\nlabor-intensive manual feature extraction or rule-based approaches, which\nimpose inherent limitations. To surmount these limitations, we propose a novel\nmethod called HPix, which utilizes modified Generative Adversarial Networks\n(GANs) to generate vector tile map from satellite images. HPix incorporates two\nhierarchical frameworks: one operating at the global level and the other at the\nlocal level, resulting in a comprehensive model. Through empirical evaluations,\nour proposed approach showcases its effectiveness in producing highly accurate\nand visually captivating vector tile maps derived from satellite images. We\nfurther extend our study's application to include mapping of road intersections\nand building footprints cluster based on their area.\n","authors":["Aditya Taparia","Keshab Nath"],"pdf_url":"https://arxiv.org/pdf/2407.13680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13677v1","updated":"2024-07-18T16:52:45Z","published":"2024-07-18T16:52:45Z","title":"PASTA: Controllable Part-Aware Shape Generation with Autoregressive\n  Transformers","summary":"  The increased demand for tools that automate the 3D content creation process\nled to tremendous progress in deep generative models that can generate diverse\n3D objects of high fidelity. In this paper, we present PASTA, an autoregressive\ntransformer architecture for generating high quality 3D shapes. PASTA comprises\ntwo main components: An autoregressive transformer that generates objects as a\nsequence of cuboidal primitives and a blending network, implemented with a\ntransformer decoder that composes the sequences of cuboids and synthesizes high\nquality meshes for each object. Our model is trained in two stages: First we\ntrain our autoregressive generative model using only annotated cuboidal parts\nas supervision and next, we train our blending network using explicit 3D\nsupervision, in the form of watertight meshes. Evaluations on various ShapeNet\nobjects showcase the ability of our model to perform shape generation from\ndiverse inputs \\eg from scratch, from a partial object, from text and images,\nas well size-guided generation, by explicitly conditioning on a bounding box\nthat defines the object's boundaries. Moreover, as our model considers the\nunderlying part-based structure of a 3D object, we are able to select a\nspecific part and produce shapes with meaningful variations of this part. As\nevidenced by our experiments, our model generates 3D shapes that are both more\nrealistic and diverse than existing part-based and non part-based methods,\nwhile at the same time is simpler to implement and train.\n","authors":["Songlin Li","Despoina Paschalidou","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2407.13677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13676v1","updated":"2024-07-18T16:51:15Z","published":"2024-07-18T16:51:15Z","title":"Aligning Sight and Sound: Advanced Sound Source Localization Through\n  Audio-Visual Alignment","summary":"  Recent studies on learning-based sound source localization have mainly\nfocused on the localization performance perspective. However, prior work and\nexisting benchmarks overlook a crucial aspect: cross-modal interaction, which\nis essential for interactive sound source localization. Cross-modal interaction\nis vital for understanding semantically matched or mismatched audio-visual\nevents, such as silent objects or off-screen sounds. In this paper, we first\ncomprehensively examine the cross-modal interaction of existing methods,\nbenchmarks, evaluation metrics, and cross-modal understanding tasks. Then, we\nidentify the limitations of previous studies and make several contributions to\novercome the limitations. First, we introduce a new synthetic benchmark for\ninteractive sound source localization. Second, we introduce new evaluation\nmetrics to rigorously assess sound source localization methods, focusing on\naccurately evaluating both localization performance and cross-modal interaction\nability. Third, we propose a learning framework with a cross-modal alignment\nstrategy to enhance cross-modal interaction. Lastly, we evaluate both\ninteractive sound source localization and auxiliary cross-modal retrieval tasks\ntogether to thoroughly assess cross-modal interaction capabilities and\nbenchmark competing methods. Our new benchmarks and evaluation metrics reveal\npreviously overlooked issues in sound source localization studies. Our proposed\nnovel method, with enhanced cross-modal alignment, shows superior sound source\nlocalization performance. This work provides the most comprehensive analysis of\nsound source localization to date, with extensive validation of competing\nmethods on both existing and new benchmarks using new and standard evaluation\nmetrics.\n","authors":["Arda Senocak","Hyeonggon Ryu","Junsik Kim","Tae-Hyun Oh","Hanspeter Pfister","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2407.13676v1.pdf","comment":"Journal Extension of ICCV 2023 paper (arXiV:2309.10724). Code is\n  available at https://github.com/kaistmm/SSLalignment"},{"id":"http://arxiv.org/abs/2407.13675v1","updated":"2024-07-18T16:50:59Z","published":"2024-07-18T16:50:59Z","title":"MeshSegmenter: Zero-Shot Mesh Semantic Segmentation via Texture\n  Synthesis","summary":"  We present MeshSegmenter, a simple yet effective framework designed for\nzero-shot 3D semantic segmentation. This model successfully extends the\npowerful capabilities of 2D segmentation models to 3D meshes, delivering\naccurate 3D segmentation across diverse meshes and segment descriptions.\nSpecifically, our model leverages the Segment Anything Model (SAM) model to\nsegment the target regions from images rendered from the 3D shape. In light of\nthe importance of the texture for segmentation, we also leverage the pretrained\nstable diffusion model to generate images with textures from 3D shape, and\nleverage SAM to segment the target regions from images with textures. Textures\nsupplement the shape for segmentation and facilitate accurate 3D segmentation\neven in geometrically non-prominent areas, such as segmenting a car door within\na car mesh. To achieve the 3D segments, we render 2D images from different\nviews and conduct segmentation for both textured and untextured images. Lastly,\nwe develop a multi-view revoting scheme that integrates 2D segmentation results\nand confidence scores from various views onto the 3D mesh, ensuring the 3D\nconsistency of segmentation results and eliminating inaccuracies from specific\nperspectives. Through these innovations, MeshSegmenter offers stable and\nreliable 3D segmentation results both quantitatively and qualitatively,\nhighlighting its potential as a transformative tool in the field of 3D\nzero-shot segmentation. The code is available at\n\\url{https://github.com/zimingzhong/MeshSegmenter}.\n","authors":["Ziming Zhong","Yanxu Xu","Jing Li","Jiale Xu","Zhengxin Li","Chaohui Yu","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2407.13675v1.pdf","comment":"The paper was accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2403.04739v2","updated":"2024-07-18T16:49:12Z","published":"2024-03-07T18:46:01Z","title":"I Can't Believe It's Not Scene Flow!","summary":"  Current scene flow methods broadly fail to describe motion on small objects,\nand current scene flow evaluation protocols hide this failure by averaging over\nmany points, with most drawn larger objects. To fix this evaluation failure, we\npropose a new evaluation protocol, Bucket Normalized EPE, which is class-aware\nand speed-normalized, enabling contextualized error comparisons between object\ntypes that move at vastly different speeds. To highlight current method\nfailures, we propose a frustratingly simple supervised scene flow baseline,\nTrackFlow, built by bolting a high-quality pretrained detector (trained using\nmany class rebalancing techniques) onto a simple tracker, that produces\nstate-of-the-art performance on current standard evaluations and large\nimprovements over prior art on our new evaluation. Our results make it clear\nthat all scene flow evaluations must be class and speed aware, and supervised\nscene flow methods must address point class imbalances. We release the\nevaluation code publicly at\nhttps://github.com/kylevedder/BucketedSceneFlowEval.\n","authors":["Ishan Khatri","Kyle Vedder","Neehar Peri","Deva Ramanan","James Hays"],"pdf_url":"https://arxiv.org/pdf/2403.04739v2.pdf","comment":"Accepted to ECCV 2024. Project page at https://vedder.io/trackflow"},{"id":"http://arxiv.org/abs/2403.05521v2","updated":"2024-07-18T16:35:23Z","published":"2024-03-08T18:43:28Z","title":"Probabilistic Image-Driven Traffic Modeling via Remote Sensing","summary":"  This work addresses the task of modeling spatiotemporal traffic patterns\ndirectly from overhead imagery, which we refer to as image-driven traffic\nmodeling. We extend this line of work and introduce a multi-modal, multi-task\ntransformer-based segmentation architecture that can be used to create dense\ncity-scale traffic models. Our approach includes a geo-temporal positional\nencoding module for integrating geo-temporal context and a probabilistic\nobjective function for estimating traffic speeds that naturally models temporal\nvariations. We evaluate our method extensively using the Dynamic Traffic Speeds\n(DTS) benchmark dataset and significantly improve the state-of-the-art.\nFinally, we introduce the DTS++ dataset to support mobility-related location\nadaptation experiments.\n","authors":["Scott Workman","Armin Hadzic"],"pdf_url":"https://arxiv.org/pdf/2403.05521v2.pdf","comment":"European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2406.13515v2","updated":"2024-07-18T16:34:10Z","published":"2024-06-19T13:02:17Z","title":"MVSBoost: An Efficient Point Cloud-based 3D Reconstruction","summary":"  Efficient and accurate 3D reconstruction is crucial for various applications,\nincluding augmented and virtual reality, medical imaging, and cinematic special\neffects. While traditional Multi-View Stereo (MVS) systems have been\nfundamental in these applications, using neural implicit fields in implicit 3D\nscene modeling has introduced new possibilities for handling complex topologies\nand continuous surfaces. However, neural implicit fields often suffer from\ncomputational inefficiencies, overfitting, and heavy reliance on data quality,\nlimiting their practical use. This paper presents an enhanced MVS framework\nthat integrates multi-view 360-degree imagery with robust camera pose\nestimation via Structure from Motion (SfM) and advanced image processing for\npoint cloud densification, mesh reconstruction, and texturing. Our approach\nsignificantly improves upon traditional MVS methods, offering superior accuracy\nand precision as validated using Chamfer distance metrics on the Realistic\nSynthetic 360 dataset. The developed MVS technique enhances the detail and\nclarity of 3D reconstructions and demonstrates superior computational\nefficiency and robustness in complex scene reconstruction, effectively handling\nocclusions and varying viewpoints. These improvements suggest that our MVS\nframework can compete with and potentially exceed current state-of-the-art\nneural implicit field methods, especially in scenarios requiring real-time\nprocessing and scalability.\n","authors":["Umair Haroon","Ahmad AlMughrabi","Ricardo Marques","Petia Radeva"],"pdf_url":"https://arxiv.org/pdf/2406.13515v2.pdf","comment":"The work is under review"},{"id":"http://arxiv.org/abs/2312.07485v3","updated":"2024-07-18T16:31:40Z","published":"2023-12-12T18:21:36Z","title":"MinD-3D: Reconstruct High-quality 3D objects in Human Brain","summary":"  In this paper, we introduce Recon3DMind, an innovative task aimed at\nreconstructing 3D visuals from Functional Magnetic Resonance Imaging (fMRI)\nsignals, marking a significant advancement in the fields of cognitive\nneuroscience and computer vision. To support this pioneering task, we present\nthe fMRI-Shape dataset, which includes data from 14 participants and features\n360-degree videos of 3D objects to enable comprehensive fMRI signal capture\nacross various settings, thereby laying a foundation for future research.\nFurthermore, we propose MinD-3D, a novel and effective three-stage framework\nspecifically designed to decode the brain's 3D visual information from fMRI\nsignals, demonstrating the feasibility of this challenging task. The framework\nbegins by extracting and aggregating features from fMRI frames through a\nneuro-fusion encoder, subsequently employs a feature bridge diffusion model to\ngenerate visual features, and ultimately recovers the 3D object via a\ngenerative transformer decoder. We assess the performance of MinD-3D using a\nsuite of semantic and structural metrics and analyze the correlation between\nthe features extracted by our model and the visual regions of interest (ROIs)\nin fMRI signals. Our findings indicate that MinD-3D not only reconstructs 3D\nobjects with high semantic relevance and spatial similarity but also\nsignificantly enhances our understanding of the human brain's capabilities in\nprocessing 3D visual information. Project page at:\nhttps://jianxgao.github.io/MinD-3D.\n","authors":["Jianxiong Gao","Yuqian Fu","Yun Wang","Xuelin Qian","Jianfeng Feng","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2312.07485v3.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13646v1","updated":"2024-07-18T16:25:16Z","published":"2024-07-18T16:25:16Z","title":"Beyond Dropout: Robust Convolutional Neural Networks Based on Local\n  Feature Masking","summary":"  In the contemporary of deep learning, where models often grapple with the\nchallenge of simultaneously achieving robustness against adversarial attacks\nand strong generalization capabilities, this study introduces an innovative\nLocal Feature Masking (LFM) strategy aimed at fortifying the performance of\nConvolutional Neural Networks (CNNs) on both fronts. During the training phase,\nwe strategically incorporate random feature masking in the shallow layers of\nCNNs, effectively alleviating overfitting issues, thereby enhancing the model's\ngeneralization ability and bolstering its resilience to adversarial attacks.\nLFM compels the network to adapt by leveraging remaining features to compensate\nfor the absence of certain semantic features, nurturing a more elastic feature\nlearning mechanism. The efficacy of LFM is substantiated through a series of\nquantitative and qualitative assessments, collectively showcasing a consistent\nand significant improvement in CNN's generalization ability and resistance\nagainst adversarial attacks--a phenomenon not observed in current and prior\nmethodologies. The seamless integration of LFM into established CNN frameworks\nunderscores its potential to advance both generalization and adversarial\nrobustness within the deep learning paradigm. Through comprehensive\nexperiments, including robust person re-identification baseline generalization\nexperiments and adversarial attack experiments, we demonstrate the substantial\nenhancements offered by LFM in addressing the aforementioned challenges. This\ncontribution represents a noteworthy stride in advancing robust neural network\narchitectures.\n","authors":["Yunpeng Gong","Chuangliang Zhang","Yongjie Hou","Lifei Chen","Min Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.13646v1.pdf","comment":"It has been accepted by IJCNN 2024"},{"id":"http://arxiv.org/abs/2407.13642v1","updated":"2024-07-18T16:20:56Z","published":"2024-07-18T16:20:56Z","title":"Open-Vocabulary 3D Semantic Segmentation with Text-to-Image Diffusion\n  Models","summary":"  In this paper, we investigate the use of diffusion models which are\npre-trained on large-scale image-caption pairs for open-vocabulary 3D semantic\nunderstanding. We propose a novel method, namely Diff2Scene, which leverages\nfrozen representations from text-image generative models, along with\nsalient-aware and geometric-aware masks, for open-vocabulary 3D semantic\nsegmentation and visual grounding tasks. Diff2Scene gets rid of any labeled 3D\ndata and effectively identifies objects, appearances, materials, locations and\ntheir compositions in 3D scenes. We show that it outperforms competitive\nbaselines and achieves significant improvements over state-of-the-art methods.\nIn particular, Diff2Scene improves the state-of-the-art method on ScanNet200 by\n12%.\n","authors":["Xiaoyu Zhu","Hao Zhou","Pengfei Xing","Long Zhao","Hao Xu","Junwei Liang","Alexander Hauptmann","Ting Liu","Andrew Gallagher"],"pdf_url":"https://arxiv.org/pdf/2407.13642v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13640v1","updated":"2024-07-18T16:18:58Z","published":"2024-07-18T16:18:58Z","title":"Beyond Augmentation: Empowering Model Robustness under Extreme Capture\n  Environments","summary":"  Person Re-identification (re-ID) in computer vision aims to recognize and\ntrack individuals across different cameras. While previous research has mainly\nfocused on challenges like pose variations and lighting changes, the impact of\nextreme capture conditions is often not adequately addressed. These extreme\nconditions, including varied lighting, camera styles, angles, and image\ndistortions, can significantly affect data distribution and re-ID accuracy.\n  Current research typically improves model generalization under normal\nshooting conditions through data augmentation techniques such as adjusting\nbrightness and contrast. However, these methods pay less attention to the\nrobustness of models under extreme shooting conditions. To tackle this, we\npropose a multi-mode synchronization learning (MMSL) strategy . This approach\ninvolves dividing images into grids, randomly selecting grid blocks, and\napplying data augmentation methods like contrast and brightness adjustments.\nThis process introduces diverse transformations without altering the original\nimage structure, helping the model adapt to extreme variations. This method\nimproves the model's generalization under extreme conditions and enables\nlearning diverse features, thus better addressing the challenges in re-ID.\nExtensive experiments on a simulated test set under extreme conditions have\ndemonstrated the effectiveness of our method. This approach is crucial for\nenhancing model robustness and adaptability in real-world scenarios, supporting\nthe future development of person re-identification technology.\n","authors":["Yunpeng Gong","Yongjie Hou","Chuangliang Zhang","Min Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.13640v1.pdf","comment":"It has been accepted by IJCNN 2024"},{"id":"http://arxiv.org/abs/2407.11921v2","updated":"2024-07-18T16:10:19Z","published":"2024-07-16T17:11:43Z","title":"IPA-NeRF: Illusory Poisoning Attack Against Neural Radiance Fields","summary":"  Neural Radiance Field (NeRF) represents a significant advancement in computer\nvision, offering implicit neural network-based scene representation and novel\nview synthesis capabilities. Its applications span diverse fields including\nrobotics, urban mapping, autonomous navigation, virtual reality/augmented\nreality, etc., some of which are considered high-risk AI applications. However,\ndespite its widespread adoption, the robustness and security of NeRF remain\nlargely unexplored. In this study, we contribute to this area by introducing\nthe Illusory Poisoning Attack against Neural Radiance Fields (IPA-NeRF). This\nattack involves embedding a hidden backdoor view into NeRF, allowing it to\nproduce predetermined outputs, i.e. illusory, when presented with the specified\nbackdoor view while maintaining normal performance with standard inputs. Our\nattack is specifically designed to deceive users or downstream models at a\nparticular position while ensuring that any abnormalities in NeRF remain\nundetectable from other viewpoints. Experimental results demonstrate the\neffectiveness of our Illusory Poisoning Attack, successfully presenting the\ndesired illusory on the specified viewpoint without impacting other views.\nNotably, we achieve this attack by introducing small perturbations solely to\nthe training set. The code can be found at\nhttps://github.com/jiang-wenxiang/IPA-NeRF.\n","authors":["Wenxiang Jiang","Hanwei Zhang","Shuo Zhao","Zhongwen Guo","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2407.11921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10663v2","updated":"2024-07-18T16:10:07Z","published":"2024-03-15T20:12:41Z","title":"Not Just Change the Labels, Learn the Features: Watermarking Deep Neural\n  Networks with Multi-View Data","summary":"  With the increasing prevalence of Machine Learning as a Service (MLaaS)\nplatforms, there is a growing focus on deep neural network (DNN) watermarking\ntechniques. These methods are used to facilitate the verification of ownership\nfor a target DNN model to protect intellectual property. One of the most widely\nemployed watermarking techniques involves embedding a trigger set into the\nsource model. Unfortunately, existing methodologies based on trigger sets are\nstill susceptible to functionality-stealing attacks, potentially enabling\nadversaries to steal the functionality of the source model without a reliable\nmeans of verifying ownership. In this paper, we first introduce a novel\nperspective on trigger set-based watermarking methods from a feature learning\nperspective. Specifically, we demonstrate that by selecting data exhibiting\nmultiple features, also referred to as \\emph{multi-view data}, it becomes\nfeasible to effectively defend functionality stealing attacks. Based on this\nperspective, we introduce a novel watermarking technique based on Multi-view\ndATa, called MAT, for efficiently embedding watermarks within DNNs. This\napproach involves constructing a trigger set with multi-view data and\nincorporating a simple feature-based regularization method for training the\nsource model. We validate our method across various benchmarks and demonstrate\nits efficacy in defending against model extraction attacks, surpassing relevant\nbaselines by a significant margin. The code is available at:\n\\href{https://github.com/liyuxuan-github/MAT}{https://github.com/liyuxuan-github/MAT}.\n","authors":["Yuxuan Li","Sarthak Kumar Maharana","Yunhui Guo"],"pdf_url":"https://arxiv.org/pdf/2403.10663v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2404.03613v2","updated":"2024-07-18T16:08:29Z","published":"2024-04-04T17:34:41Z","title":"Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian\n  Splatting","summary":"  As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view\nsynthesis, it is a natural extension to deform a canonical 3DGS to multiple\nframes for representing a dynamic scene. However, previous works fail to\naccurately reconstruct complex dynamic scenes. We attribute the failure to the\ndesign of the deformation field, which is built as a coordinate-based function.\nThis approach is problematic because 3DGS is a mixture of multiple fields\ncentered at the Gaussians, not just a single coordinate-based framework. To\nresolve this problem, we define the deformation as a function of per-Gaussian\nembeddings and temporal embeddings. Moreover, we decompose deformations as\ncoarse and fine deformations to model slow and fast movements, respectively.\nAlso, we introduce a local smoothness regularization for per-Gaussian embedding\nto improve the details in dynamic regions. Project page:\nhttps://jeongminb.github.io/e-d3dgs/\n","authors":["Jeongmin Bae","Seoha Kim","Youngsik Yun","Hahyun Lee","Gun Bang","Youngjung Uh"],"pdf_url":"https://arxiv.org/pdf/2404.03613v2.pdf","comment":"ECCV 2024. Project page: https://jeongminb.github.io/e-d3dgs/"},{"id":"http://arxiv.org/abs/2312.00844v3","updated":"2024-07-18T16:05:55Z","published":"2023-12-01T06:04:49Z","title":"Sparse Beats Dense: Rethinking Supervision in Radar-Camera Depth\n  Completion","summary":"  It is widely believed that sparse supervision is worse than dense supervision\nin the field of depth completion, but the underlying reasons for this are\nrarely discussed. To this end, we revisit the task of radar-camera depth\ncompletion and present a new method with sparse LiDAR supervision to outperform\nprevious dense LiDAR supervision methods in both accuracy and speed.\nSpecifically, when trained by sparse LiDAR supervision, depth completion models\nusually output depth maps containing significant stripe-like artifacts. We find\nthat such a phenomenon is caused by the implicitly learned positional\ndistribution pattern from sparse LiDAR supervision, termed as LiDAR\nDistribution Leakage (LDL) in this paper. Based on such understanding, we\npresent a novel Disruption-Compensation radar-camera depth completion framework\nto address this issue. The Disruption part aims to deliberately disrupt the\nlearning of LiDAR distribution from sparse supervision, while the Compensation\npart aims to leverage 3D spatial and 2D semantic information to compensate for\nthe information loss of previous disruptions. Extensive experimental results\ndemonstrate that by reducing the impact of LDL, our framework with sparse\nsupervision outperforms the state-of-the-art dense supervision methods with\n11.6% improvement in Mean Absolute Error (MAE)} and 1.6x speedup in Frame Per\nSecond (FPS)}. The code is available at\nhttps://github.com/megvii-research/Sparse-Beats-Dense.\n","authors":["Huadong Li","Minhao Jing","Jiajun Liang","Haoqiang Fan","Renhe Ji"],"pdf_url":"https://arxiv.org/pdf/2312.00844v3.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2311.12085v2","updated":"2024-07-18T16:04:19Z","published":"2023-11-20T11:24:21Z","title":"Pyramid Diffusion for Fine 3D Large Scene Generation","summary":"  Diffusion models have shown remarkable results in generating 2D images and\nsmall-scale 3D objects. However, their application to the synthesis of\nlarge-scale 3D scenes has been rarely explored. This is mainly due to the\ninherent complexity and bulky size of 3D scenery data, particularly outdoor\nscenes, and the limited availability of comprehensive real-world datasets,\nwhich makes training a stable scene diffusion model challenging. In this work,\nwe explore how to effectively generate large-scale 3D scenes using the\ncoarse-to-fine paradigm. We introduce a framework, the Pyramid Discrete\nDiffusion model (PDD), which employs scale-varied diffusion models to\nprogressively generate high-quality outdoor scenes. Experimental results of PDD\ndemonstrate our successful exploration in generating 3D scenes both\nunconditionally and conditionally. We further showcase the data compatibility\nof the PDD model, due to its multi-scale architecture: a PDD model trained on\none dataset can be easily fine-tuned with another dataset. Code is available at\nhttps://github.com/yuhengliu02/pyramid-discrete-diffusion.\n","authors":["Yuheng Liu","Xinke Li","Xueting Li","Lu Qi","Chongshou Li","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2311.12085v2.pdf","comment":"Accepted by ECCV 24. Project page:\n  https://yuheng.ink/project-page/pyramid-discrete-diffusion"},{"id":"http://arxiv.org/abs/2407.13632v1","updated":"2024-07-18T16:03:59Z","published":"2024-07-18T16:03:59Z","title":"Data Alchemy: Mitigating Cross-Site Model Variability Through Test Time\n  Data Calibration","summary":"  Deploying deep learning-based imaging tools across various clinical sites\nposes significant challenges due to inherent domain shifts and regulatory\nhurdles associated with site-specific fine-tuning. For histopathology, stain\nnormalization techniques can mitigate discrepancies, but they often fall short\nof eliminating inter-site variations. Therefore, we present Data Alchemy, an\nexplainable stain normalization method combined with test time data calibration\nvia a template learning framework to overcome barriers in cross-site analysis.\nData Alchemy handles shifts inherent to multi-site data and minimizes them\nwithout needing to change the weights of the normalization or classifier\nnetworks. Our approach extends to unseen sites in various clinical settings\nwhere data domain discrepancies are unknown. Extensive experiments highlight\nthe efficacy of our framework in tumor classification in hematoxylin and\neosin-stained patches. Our explainable normalization method boosts\nclassification tasks' area under the precision-recall curve(AUPR) by 0.165,\n0.545 to 0.710. Additionally, Data Alchemy further reduces the multisite\nclassification domain gap, by improving the 0.710 AUPR an additional 0.142,\nelevating classification performance further to 0.852, from 0.545. Our Data\nAlchemy framework can popularize precision medicine with minimal operational\noverhead by allowing for the seamless integration of pre-trained deep\nlearning-based clinical tools across multiple sites.\n","authors":["Abhijeet Parida","Antonia Alomar","Zhifan Jiang","Pooneh Roshanitabrizi","Austin Tapp","Maria Ledesma-Carbayo","Ziyue Xu","Syed Muhammed Anwar","Marius George Linguraru","Holger R. Roth"],"pdf_url":"https://arxiv.org/pdf/2407.13632v1.pdf","comment":"accepted to Machine Learning in Medical Imaging (MLMI 2024)"},{"id":"http://arxiv.org/abs/2407.08165v2","updated":"2024-07-18T15:52:26Z","published":"2024-07-11T04:02:05Z","title":"Explicit-NeRF-QA: A Quality Assessment Database for Explicit NeRF Model\n  Compression","summary":"  In recent years, Neural Radiance Fields (NeRF) have demonstrated significant\nadvantages in representing and synthesizing 3D scenes. Explicit NeRF models\nfacilitate the practical NeRF applications with faster rendering speed, and\nalso attract considerable attention in NeRF compression due to its huge storage\ncost. To address the challenge of the NeRF compression study, in this paper, we\nconstruct a new dataset, called Explicit-NeRF-QA. We use 22 3D objects with\ndiverse geometries, textures, and material complexities to train four typical\nexplicit NeRF models across five parameter levels. Lossy compression is\nintroduced during the model generation, pivoting the selection of key\nparameters such as hash table size for InstantNGP and voxel grid resolution for\nPlenoxels. By rendering NeRF samples to processed video sequences (PVS), a\nlarge scale subjective experiment with lab environment is conducted to collect\nsubjective scores from 21 viewers. The diversity of content, accuracy of mean\nopinion scores (MOS), and characteristics of NeRF distortion are\ncomprehensively presented, establishing the heterogeneity of the proposed\ndataset. The state-of-the-art objective metrics are tested in the new dataset.\nBest Person correlation, which is around 0.85, is collected from the\nfull-reference objective metric. All tested no-reference metrics report very\npoor results with 0.4 to 0.6 correlations, demonstrating the need for further\ndevelopment of more robust no-reference metrics. The dataset, including NeRF\nsamples, source 3D objects, multiview images for NeRF generation, PVSs, MOS, is\nmade publicly available at the following location:\nhttps://github.com/LittlericeChloe/Explicit_NeRF_QA.\n","authors":["Yuke Xing","Qi Yang","Kaifa Yang","Yilin Xu","Zhu Li"],"pdf_url":"https://arxiv.org/pdf/2407.08165v2.pdf","comment":"5 pages, 4 figures, 2 tables, conference"},{"id":"http://arxiv.org/abs/2407.13609v1","updated":"2024-07-18T15:48:07Z","published":"2024-07-18T15:48:07Z","title":"Training-free Composite Scene Generation for Layout-to-Image Synthesis","summary":"  Recent breakthroughs in text-to-image diffusion models have significantly\nadvanced the generation of high-fidelity, photo-realistic images from textual\ndescriptions. Yet, these models often struggle with interpreting spatial\narrangements from text, hindering their ability to produce images with precise\nspatial configurations. To bridge this gap, layout-to-image generation has\nemerged as a promising direction. However, training-based approaches are\nlimited by the need for extensively annotated datasets, leading to high data\nacquisition costs and a constrained conceptual scope. Conversely, training-free\nmethods face challenges in accurately locating and generating semantically\nsimilar objects within complex compositions. This paper introduces a novel\ntraining-free approach designed to overcome adversarial semantic intersections\nduring the diffusion conditioning phase. By refining intra-token loss with\nselective sampling and enhancing the diffusion process with attention\nredistribution, we propose two innovative constraints: 1) an inter-token\nconstraint that resolves token conflicts to ensure accurate concept synthesis;\nand 2) a self-attention constraint that improves pixel-to-pixel relationships.\nOur evaluations confirm the effectiveness of leveraging layout information for\nguiding the diffusion process, generating content-rich images with enhanced\nfidelity and complexity. Code is available at\nhttps://github.com/Papple-F/csg.git.\n","authors":["Jiaqi Liu","Tao Huang","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2407.13609v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2209.13094v4","updated":"2024-07-18T15:42:44Z","published":"2022-09-27T01:03:36Z","title":"Efficient Image Denoising by Low-Rank Singular Vector Approximations of\n  Geodesics' Gramian Matrix","summary":"  With the advent of sophisticated cameras, the urge to capture high-quality\nimages has grown enormous. However, the noise contamination of the images\nresults in substandard expectations among the people; thus, image denoising is\nan essential pre-processing step. While the algebraic image processing\nframeworks are sometimes inefficient for this denoising task as they may\nrequire processing of matrices of order equivalent to some power of the order\nof the original image, the neural network image processing frameworks are\nsometimes not robust as they require a lot of similar training samples. Thus,\nhere we present a manifold-based noise filtering method that mainly exploits a\nfew prominent singular vectors of the geodesics' Gramian matrix. Especially,\nthe framework partitions an image, say that of size $n \\times n$, into $n^2$\noverlapping patches of known size such that one patch is centered at each\npixel. Then, the prominent singular vectors, of the Gramian matrix of size $n^2\n\\times n^2$ of the geodesic distances computed over the patch space, are\nutilized to denoise the image. Here, the prominent singular vectors are\nrevealed by efficient, but diverse, approximation techniques, rather than\nexplicitly computing them using frameworks like Singular Value Decomposition\n(SVD) which encounters $\\mathcal{O}(n^6)$ operations. Finally, we compare both\ncomputational time and the noise filtration performance of the proposed\ndenoising algorithm with and without singular vector approximation techniques.\n","authors":["Kelum Gajamannage","Yonggi Park","S. M. Mallikarjunaiah","Sunil Mathur"],"pdf_url":"https://arxiv.org/pdf/2209.13094v4.pdf","comment":"19 pages, 3 figures, submitted to ACM Transactions on Architecture\n  and Code Optimization"},{"id":"http://arxiv.org/abs/2407.13596v1","updated":"2024-07-18T15:35:00Z","published":"2024-07-18T15:35:00Z","title":"EarthMarker: A Visual Prompt Learning Framework for Region-level and\n  Point-level Remote Sensing Imagery Comprehension","summary":"  Recent advances in visual prompting in the natural image area have allowed\nusers to interact with artificial intelligence (AI) tools through various\nvisual marks such as box, point, and free-form shapes. However, due to the\nsignificant difference between the natural and remote sensing (RS) images,\nexisting visual prompting models face challenges in RS scenarios. Moreover, RS\nMLLMs mainly focus on interpreting image-level RS data and only support\ninteraction with language instruction, restricting flexibility applications in\nthe real world. To address those limitations, a novel visual prompting model\nnamed EarthMarker is proposed, which excels in image-level, region-level, and\npoint-level RS imagery interpretation. Specifically, the visual prompts\nalongside images and text instruction input into the large language model\n(LLM), adapt models toward specific predictions and tasks. Subsequently, a\nsharing visual encoding method is introduced to refine multi-scale image\nfeatures and visual prompt information uniformly. Furthermore, to endow the\nEarthMarker with versatile multi-granularity visual perception abilities, the\ncross-domain phased learning strategy is developed, and the disjoint parameters\nare optimized in a lightweight manner by leveraging both the natural and RS\ndomain-specific knowledge. In addition, to tackle the lack of RS visual\nprompting data, a dataset named RSVP featuring multi-modal fine-grained visual\nprompting instruction is constructed. Extensive experiments are conducted to\ndemonstrate the proposed EarthMarker's competitive performance, representing a\nsignificant advance in multi-granularity RS imagery interpretation under the\nvisual prompting learning framework.\n","authors":["Wei Zhang","Miaoxin Cai","Tong Zhang","Yin Zhuang","Xuerui Mao"],"pdf_url":"https://arxiv.org/pdf/2407.13596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06642v2","updated":"2024-07-18T15:34:04Z","published":"2024-07-09T08:11:53Z","title":"Powerful and Flexible: Personalized Text-to-Image Generation via\n  Reinforcement Learning","summary":"  Personalized text-to-image models allow users to generate varied styles of\nimages (specified with a sentence) for an object (specified with a set of\nreference images). While remarkable results have been achieved using\ndiffusion-based generation models, the visual structure and details of the\nobject are often unexpectedly changed during the diffusion process. One major\nreason is that these diffusion-based approaches typically adopt a simple\nreconstruction objective during training, which can hardly enforce appropriate\nstructural consistency between the generated and the reference images. To this\nend, in this paper, we design a novel reinforcement learning framework by\nutilizing the deterministic policy gradient method for personalized\ntext-to-image generation, with which various objectives, differential or even\nnon-differential, can be easily incorporated to supervise the diffusion models\nto improve the quality of the generated images. Experimental results on\npersonalized text-to-image generation benchmark datasets demonstrate that our\nproposed approach outperforms existing state-of-the-art methods by a large\nmargin on visual fidelity while maintaining text-alignment. Our code is\navailable at: \\url{https://github.com/wfanyue/DPG-T2I-Personalization}.\n","authors":["Fanyue Wei","Wei Zeng","Zhenyang Li","Dawei Yin","Lixin Duan","Wen Li"],"pdf_url":"https://arxiv.org/pdf/2407.06642v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13592v1","updated":"2024-07-18T15:29:48Z","published":"2024-07-18T15:29:48Z","title":"MeshFeat: Multi-Resolution Features for Neural Fields on Meshes","summary":"  Parametric feature grid encodings have gained significant attention as an\nencoding approach for neural fields since they allow for much smaller MLPs,\nwhich significantly decreases the inference time of the models. In this work,\nwe propose MeshFeat, a parametric feature encoding tailored to meshes, for\nwhich we adapt the idea of multi-resolution feature grids from Euclidean space.\nWe start from the structure provided by the given vertex topology and use a\nmesh simplification algorithm to construct a multi-resolution feature\nrepresentation directly on the mesh. The approach allows the usage of small\nMLPs for neural fields on meshes, and we show a significant speed-up compared\nto previous representations while maintaining comparable reconstruction quality\nfor texture reconstruction and BRDF representation. Given its intrinsic\ncoupling to the vertices, the method is particularly well-suited for\nrepresentations on deforming meshes, making it a good fit for object animation.\n","authors":["Mihir Mahajan","Florian Hofherr","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2407.13592v1.pdf","comment":"To appear at European Conference on Computer Vision (ECCV), 2024"},{"id":"http://arxiv.org/abs/2407.13588v1","updated":"2024-07-18T15:27:56Z","published":"2024-07-18T15:27:56Z","title":"Robust Calibration of Large Vision-Language Adapters","summary":"  This paper addresses the critical issue of miscalibration in CLIP-based model\nadaptation, particularly in the challenging scenario of out-of-distribution\n(OOD) samples, which has been overlooked in the existing literature on CLIP\nadaptation. We empirically demonstrate that popular CLIP adaptation approaches,\nsuch as Adapters, Prompt Learning, and Test-Time Adaptation, substantially\ndegrade the calibration capabilities of the zero-shot baseline in the presence\nof distributional drift. We identify the increase in logit ranges as the\nunderlying cause of miscalibration of CLIP adaptation methods, contrasting with\nprevious work on calibrating fully-supervised models. Motivated by these\nobservations, we present a simple and model-agnostic solution to mitigate\nmiscalibration, by scaling the logit range of each sample to its zero-shot\nprediction logits. We explore three different alternatives to achieve this,\nwhich can be either integrated during adaptation or directly used at inference\ntime. Comprehensive experiments on popular OOD classification benchmarks\ndemonstrate the effectiveness of the proposed approaches in mitigating\nmiscalibration while maintaining discriminative performance, whose improvements\nare consistent across the three families of these increasingly popular\napproaches. The code is publicly available at:\nhttps://github.com/Bala93/CLIPCalib\n","authors":["Balamurali Murugesan","Julio Silva-Rodriguez","Ismail Ben Ayed","Jose Dolz"],"pdf_url":"https://arxiv.org/pdf/2407.13588v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13584v1","updated":"2024-07-18T15:25:41Z","published":"2024-07-18T15:25:41Z","title":"Connecting Consistency Distillation to Score Distillation for Text-to-3D\n  Generation","summary":"  Although recent advancements in text-to-3D generation have significantly\nimproved generation quality, issues like limited level of detail and low\nfidelity still persist, which requires further improvement. To understand the\nessence of those issues, we thoroughly analyze current score distillation\nmethods by connecting theories of consistency distillation to score\ndistillation. Based on the insights acquired through analysis, we propose an\noptimization framework, Guided Consistency Sampling (GCS), integrated with 3D\nGaussian Splatting (3DGS) to alleviate those issues. Additionally, we have\nobserved the persistent oversaturation in the rendered views of generated 3D\nassets. From experiments, we find that it is caused by unwanted accumulated\nbrightness in 3DGS during optimization. To mitigate this issue, we introduce a\nBrightness-Equalized Generation (BEG) scheme in 3DGS rendering. Experimental\nresults demonstrate that our approach generates 3D assets with more details and\nhigher fidelity than state-of-the-art methods. The codes are released at\nhttps://github.com/LMozart/ECCV2024-GCS-BEG.\n","authors":["Zongrui Li","Minghui Hu","Qian Zheng","Xudong Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.13584v1.pdf","comment":"Paper accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.13571v1","updated":"2024-07-18T15:14:35Z","published":"2024-07-18T15:14:35Z","title":"New Capability to Look Up an ASL Sign from a Video Example","summary":"  Looking up an unknown sign in an ASL dictionary can be difficult. Most ASL\ndictionaries are organized based on English glosses, despite the fact that (1)\nthere is no convention for assigning English-based glosses to ASL signs; and\n(2) there is no 1-1 correspondence between ASL signs and English words.\nFurthermore, what if the user does not know either the meaning of the target\nsign or its possible English translation(s)? Some ASL dictionaries enable\nsearching through specification of articulatory properties, such as handshapes,\nlocations, movement properties, etc. However, this is a cumbersome process and\ndoes not always result in successful lookup. Here we describe a new system,\npublicly shared on the Web, to enable lookup of a video of an ASL sign (e.g., a\nwebcam recording or a clip from a continuous signing video). The user submits a\nvideo for analysis and is presented with the five most likely sign matches, in\ndecreasing order of likelihood, so that the user can confirm the selection and\nthen be taken to our ASLLRP Sign Bank entry for that sign. Furthermore, this\nvideo lookup is also integrated into our newest version of SignStream(R)\nsoftware to facilitate linguistic annotation of ASL video data, enabling the\nuser to directly look up a sign in the video being annotated, and, upon\nconfirmation of the match, to directly enter into the annotation the gloss and\nfeatures of that sign, greatly increasing the efficiency and consistency of\nlinguistic annotations of ASL video data.\n","authors":["Carol Neidle","Augustine Opoku","Carey Ballard","Yang Zhou","Xiaoxiao He","Gregory Dimitriadis","Dimitris Metaxas"],"pdf_url":"https://arxiv.org/pdf/2407.13571v1.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.13567v1","updated":"2024-07-18T14:40:33Z","published":"2024-07-18T14:40:33Z","title":"Hyp2Nav: Hyperbolic Planning and Curiosity for Crowd Navigation","summary":"  Autonomous robots are increasingly becoming a strong fixture in social\nenvironments. Effective crowd navigation requires not only safe yet fast\nplanning, but should also enable interpretability and computational efficiency\nfor working in real-time on embedded devices. In this work, we advocate for\nhyperbolic learning to enable crowd navigation and we introduce Hyp2Nav.\nDifferent from conventional reinforcement learning-based crowd navigation\nmethods, Hyp2Nav leverages the intrinsic properties of hyperbolic geometry to\nbetter encode the hierarchical nature of decision-making processes in\nnavigation tasks. We propose a hyperbolic policy model and a hyperbolic\ncuriosity module that results in effective social navigation, best success\nrates, and returns across multiple simulation settings, using up to 6 times\nfewer parameters than competitor state-of-the-art models. With our approach, it\nbecomes even possible to obtain policies that work in 2-dimensional embedding\nspaces, opening up new possibilities for low-resource crowd navigation and\nmodel interpretability. Insightfully, the internal hyperbolic representation of\nHyp2Nav correlates with how much attention the robot pays to the surrounding\ncrowds, e.g. due to multiple people occluding its pathway or to a few of them\nshowing colliding plans, rather than to its own planned route.\n","authors":["Alessandro Flaborea","Guido Maria D'Amely di Melendugno","Pascal Mettes","Fabio Galasso"],"pdf_url":"https://arxiv.org/pdf/2407.13567v1.pdf","comment":"Accepted at IROS 2024"},{"id":"http://arxiv.org/abs/2407.13559v1","updated":"2024-07-18T14:31:09Z","published":"2024-07-18T14:31:09Z","title":"Qalam : A Multimodal LLM for Arabic Optical Character and Handwriting\n  Recognition","summary":"  Arabic Optical Character Recognition (OCR) and Handwriting Recognition (HWR)\npose unique challenges due to the cursive and context-sensitive nature of the\nArabic script. This study introduces Qalam, a novel foundation model designed\nfor Arabic OCR and HWR, built on a SwinV2 encoder and RoBERTa decoder\narchitecture. Our model significantly outperforms existing methods, achieving a\nWord Error Rate (WER) of just 0.80% in HWR tasks and 1.18% in OCR tasks. We\ntrain Qalam on a diverse dataset, including over 4.5 million images from Arabic\nmanuscripts and a synthetic dataset comprising 60k image-text pairs. Notably,\nQalam demonstrates exceptional handling of Arabic diacritics, a critical\nfeature in Arabic scripts. Furthermore, it shows a remarkable ability to\nprocess high-resolution inputs, addressing a common limitation in current OCR\nsystems. These advancements underscore Qalam's potential as a leading solution\nfor Arabic script recognition, offering a significant leap in accuracy and\nefficiency.\n","authors":["Gagan Bhatia","El Moatez Billah Nagoudi","Fakhraddin Alwajih","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2407.13559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13555v1","updated":"2024-07-18T14:28:31Z","published":"2024-07-18T14:28:31Z","title":"PetFace: A Large-Scale Dataset and Benchmark for Animal Identification","summary":"  Automated animal face identification plays a crucial role in the monitoring\nof behaviors, conducting of surveys, and finding of lost animals. Despite the\nadvancements in human face identification, the lack of datasets and benchmarks\nin the animal domain has impeded progress. In this paper, we introduce the\nPetFace dataset, a comprehensive resource for animal face identification\nencompassing 257,484 unique individuals across 13 animal families and 319 breed\ncategories, including both experimental and pet animals. This large-scale\ncollection of individuals facilitates the investigation of unseen animal face\nverification, an area that has not been sufficiently explored in existing\ndatasets due to the limited number of individuals. Moreover, PetFace also has\nfine-grained annotations such as sex, breed, color, and pattern. We provide\nmultiple benchmarks including re-identification for seen individuals and\nverification for unseen individuals. The models trained on our dataset\noutperform those trained on prior datasets, even for detailed breed variations\nand unseen animal families. Our result also indicates that there is some room\nto improve the performance of integrated identification on multiple animal\nfamilies. We hope the PetFace dataset will facilitate animal face\nidentification and encourage the development of non-invasive animal automatic\nidentification methods.\n","authors":["Risa Shinoda","Kaede Shiohara"],"pdf_url":"https://arxiv.org/pdf/2407.13555v1.pdf","comment":"ECCV 2024. Dataset and code: https://dahlian00.github.io/PetFacePage/"},{"id":"http://arxiv.org/abs/2407.13553v1","updated":"2024-07-18T14:27:54Z","published":"2024-07-18T14:27:54Z","title":"SAM-Driven Weakly Supervised Nodule Segmentation with Uncertainty-Aware\n  Cross Teaching","summary":"  Automated nodule segmentation is essential for computer-assisted diagnosis in\nultrasound images. Nevertheless, most existing methods depend on precise\npixel-level annotations by medical professionals, a process that is both costly\nand labor-intensive. Recently, segmentation foundation models like SAM have\nshown impressive generalizability on natural images, suggesting their potential\nas pseudo-labelers. However, accurate prompts remain crucial for their success\nin medical images. In this work, we devise a novel weakly supervised framework\nthat effectively utilizes the segmentation foundation model to generate\npseudo-labels from aspect ration annotations for automatic nodule segmentation.\nSpecifically, we develop three types of bounding box prompts based on scalable\nshape priors, followed by an adaptive pseudo-label selection module to fully\nexploit the prediction capabilities of the foundation model for nodules. We\nalso present a SAM-driven uncertainty-aware cross-teaching strategy. This\napproach integrates SAM-based uncertainty estimation and label-space\nperturbations into cross-teaching to mitigate the impact of pseudo-label\ninaccuracies on model training. Extensive experiments on two clinically\ncollected ultrasound datasets demonstrate the superior performance of our\nproposed method.\n","authors":["Xingyue Zhao","Peiqi Li","Xiangde Luo","Meng Yang","Shi Chang","Zhongyu Li"],"pdf_url":"https://arxiv.org/pdf/2407.13553v1.pdf","comment":"ISBI 2024 Oral"},{"id":"http://arxiv.org/abs/2407.13545v1","updated":"2024-07-18T14:20:04Z","published":"2024-07-18T14:20:04Z","title":"DiffuX2CT: Diffusion Learning to Reconstruct CT Images from Biplanar\n  X-Rays","summary":"  Computed tomography (CT) is widely utilized in clinical settings because it\ndelivers detailed 3D images of the human body. However, performing CT scans is\nnot always feasible due to radiation exposure and limitations in certain\nsurgical environments. As an alternative, reconstructing CT images from\nultra-sparse X-rays offers a valuable solution and has gained significant\ninterest in scientific research and medical applications. However, it presents\ngreat challenges as it is inherently an ill-posed problem, often compromised by\nartifacts resulting from overlapping structures in X-ray images. In this paper,\nwe propose DiffuX2CT, which models CT reconstruction from orthogonal biplanar\nX-rays as a conditional diffusion process. DiffuX2CT is established with a 3D\nglobal coherence denoising model with a new, implicit conditioning mechanism.\nWe realize the conditioning mechanism by a newly designed tri-plane decoupling\ngenerator and an implicit neural decoder. By doing so, DiffuX2CT achieves\nstructure-controllable reconstruction, which enables 3D structural information\nto be recovered from 2D X-rays, therefore producing faithful textures in CT\nimages. As an extra contribution, we collect a real-world lumbar CT dataset,\ncalled LumbarV, as a new benchmark to verify the clinical significance and\nperformance of CT reconstruction from X-rays. Extensive experiments on this\ndataset and three more publicly available datasets demonstrate the\neffectiveness of our proposal.\n","authors":["Xuhui Liu","Zhi Qiao","Runkun Liu","Hong Li","Juan Zhang","Xiantong Zhen","Zhen Qian","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.13545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13541v1","updated":"2024-07-18T14:18:03Z","published":"2024-07-18T14:18:03Z","title":"On the Discriminability of Self-Supervised Representation Learning","summary":"  Self-supervised learning (SSL) has recently achieved significant success in\ndownstream visual tasks. However, a notable gap still exists between SSL and\nsupervised learning (SL), especially in complex downstream tasks. In this\npaper, we show that the features learned by SSL methods suffer from the\ncrowding problem, where features of different classes are not distinctly\nseparated, and features within the same class exhibit large intra-class\nvariance. In contrast, SL ensures a clear separation between classes. We\nanalyze this phenomenon and conclude that SSL objectives do not constrain the\nrelationships between different samples and their augmentations. Our\ntheoretical analysis delves into how SSL objectives fail to enforce the\nnecessary constraints between samples and their augmentations, leading to poor\nperformance in complex tasks. We provide a theoretical framework showing that\nthe performance gap between SSL and SL mainly stems from the inability of SSL\nmethods to capture the aggregation of similar augmentations and the separation\nof dissimilar augmentations. To address this issue, we propose a learnable\nregulator called Dynamic Semantic Adjuster (DSA). DSA aggregates and separates\nsamples in the feature space while being robust to outliers. Through extensive\nempirical evaluations on multiple benchmark datasets, we demonstrate the\nsuperiority of DSA in enhancing feature aggregation and separation, ultimately\nclosing the performance gap between SSL and SL.\n","authors":["Zeen Song","Wenwen Qiang","Changwen Zheng","Fuchun Sun","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2407.13541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13537v1","updated":"2024-07-18T14:09:03Z","published":"2024-07-18T14:09:03Z","title":"GlobalPointer: Large-Scale Plane Adjustment with Bi-Convex Relaxation","summary":"  Plane adjustment (PA) is crucial for many 3D applications, involving\nsimultaneous pose estimation and plane recovery. Despite recent advancements,\nit remains a challenging problem in the realm of multi-view point cloud\nregistration. Current state-of-the-art methods can achieve globally optimal\nconvergence only with good initialization. Furthermore, their high time\ncomplexity renders them impractical for large-scale problems. To address these\nchallenges, we first exploit a novel optimization strategy termed\n\\textit{Bi-Convex Relaxation}, which decouples the original problem into two\nsimpler sub-problems, reformulates each sub-problem using a convex relaxation\ntechnique, and alternately solves each one until the original problem\nconverges. Building on this strategy, we propose two algorithmic variants for\nsolving the plane adjustment problem, namely \\textit{GlobalPointer} and\n\\textit{GlobalPointer++}, based on point-to-plane and plane-to-plane errors,\nrespectively. Extensive experiments on both synthetic and real datasets\ndemonstrate that our method can perform large-scale plane adjustment with\nlinear time complexity, larger convergence region, and robustness to poor\ninitialization, while achieving similar accuracy as prior methods. The code is\navailable at\n\\href{https://github.com/wu-cvgl/GlobalPointer}{github.com/wu-cvgl/GlobalPointer}\n","authors":["Bangyan Liao","Zhenjun Zhao","Lu Chen","Haoang Li","Daniel Cremers","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2407.13537v1.pdf","comment":"Accepted to ECCV 2024. The first two authors contributed equally to\n  this work. Code: https://github.com/wu-cvgl/GlobalPointer"},{"id":"http://arxiv.org/abs/2401.09802v2","updated":"2024-07-18T14:01:51Z","published":"2024-01-18T08:46:02Z","title":"Efficient Training for Multilingual Visual Speech Recognition:\n  Pre-training with Discretized Visual Speech Representation","summary":"  This paper explores sentence-level multilingual Visual Speech Recognition\n(VSR) that can recognize different languages with a single trained model. As\nthe massive multilingual modeling of visual data requires huge computational\ncosts, we propose a novel training strategy, processing with visual speech\nunits. Motivated by the recent success of the audio speech unit, we propose to\nuse a visual speech unit that can be obtained by discretizing the visual speech\nfeatures extracted from the self-supervised visual speech model. Through\nanalysis, we verify that the visual speech units mainly contain viseme\ninformation while suppressing non-linguistic information. By using the visual\nspeech units as the inputs of our system, we propose to pre-train a VSR model\nto predict corresponding text outputs on multilingual data constructed by\nmerging several VSR databases. As both the inputs (i.e., visual speech units)\nand outputs (i.e., text) are discrete, we can greatly improve the training\nefficiency compared to the standard VSR training. Specifically, the input data\nsize is reduced to 0.016% of the original video inputs. In order to complement\nthe insufficient visual information in speech recognition, we apply curriculum\nlearning where the inputs of the system begin with audio-visual speech units\nand gradually change to visual speech units. After pre-training, the model is\nfinetuned on continuous features. We set new state-of-the-art multilingual VSR\nperformances by achieving comparable performances to the previous\nlanguage-specific VSR models, with a single trained model.\n","authors":["Minsu Kim","Jeong Hun Yeo","Se Jin Park","Hyeongseop Rha","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2401.09802v2.pdf","comment":"ACMMM 2024"},{"id":"http://arxiv.org/abs/2407.13524v1","updated":"2024-07-18T13:58:42Z","published":"2024-07-18T13:58:42Z","title":"Enhancing Source-Free Domain Adaptive Object Detection with\n  Low-confidence Pseudo Label Distillation","summary":"  Source-Free domain adaptive Object Detection (SFOD) is a promising strategy\nfor deploying trained detectors to new, unlabeled domains without accessing\nsource data, addressing significant concerns around data privacy and\nefficiency. Most SFOD methods leverage a Mean-Teacher (MT) self-training\nparadigm relying heavily on High-confidence Pseudo Labels (HPL). However, these\nHPL often overlook small instances that undergo significant appearance changes\nwith domain shifts. Additionally, HPL ignore instances with low confidence due\nto the scarcity of training samples, resulting in biased adaptation toward\nfamiliar instances from the source domain. To address this limitation, we\nintroduce the Low-confidence Pseudo Label Distillation (LPLD) loss within the\nMean-Teacher based SFOD framework. This novel approach is designed to leverage\nthe proposals from Region Proposal Network (RPN), which potentially encompasses\nhard-to-detect objects in unfamiliar domains. Initially, we extract HPL using a\nstandard pseudo-labeling technique and mine a set of Low-confidence Pseudo\nLabels (LPL) from proposals generated by RPN, leaving those that do not overlap\nsignificantly with HPL. These LPL are further refined by leveraging\nclass-relation information and reducing the effect of inherent noise for the\nLPLD loss calculation. Furthermore, we use feature distance to adaptively\nweight the LPLD loss to focus on LPL containing a larger foreground area. Our\nmethod outperforms previous SFOD methods on four cross-domain object detection\nbenchmarks. Extensive experiments demonstrate that our LPLD loss leads to\neffective adaptation by reducing false negatives and facilitating the use of\ndomain-invariant knowledge from the source model. Code is available at\nhttps://github.com/junia3/LPLD.\n","authors":["Ilhoon Yoon","Hyeongjun Kwon","Jin Kim","Junyoung Park","Hyunsung Jang","Kwanghoon Sohn"],"pdf_url":"https://arxiv.org/pdf/2407.13524v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13520v1","updated":"2024-07-18T13:55:54Z","published":"2024-07-18T13:55:54Z","title":"EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian\n  Splatting","summary":"  3D deblurring reconstruction techniques have recently seen significant\nadvancements with the development of Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS). Although these techniques can recover relatively\nclear 3D reconstructions from blurry image inputs, they still face limitations\nin handling severe blurring and complex camera motion. To address these issues,\nwe propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting\n(EaDeblur-GS), which integrates event camera data to enhance the robustness of\n3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE)\nnetwork to estimate Gaussian center deviations and using novel loss functions,\nEaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating\nperformance comparable to state-of-the-art methods.\n","authors":["Yuchen Weng","Zhengwen Shen","Ruofan Chen","Qi Wang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13519v1","updated":"2024-07-18T13:53:15Z","published":"2024-07-18T13:53:15Z","title":"GPSFormer: A Global Perception and Local Structure Fitting-based\n  Transformer for Point Cloud Understanding","summary":"  Despite the significant advancements in pre-training methods for point cloud\nunderstanding, directly capturing intricate shape information from irregular\npoint clouds without reliance on external data remains a formidable challenge.\nTo address this problem, we propose GPSFormer, an innovative Global Perception\nand Local Structure Fitting-based Transformer, which learns detailed shape\ninformation from point clouds with remarkable precision. The core of GPSFormer\nis the Global Perception Module (GPM) and the Local Structure Fitting\nConvolution (LSFConv). Specifically, GPM utilizes Adaptive Deformable Graph\nConvolution (ADGConv) to identify short-range dependencies among similar\nfeatures in the feature space and employs Multi-Head Attention (MHA) to learn\nlong-range dependencies across all positions within the feature space,\nultimately enabling flexible learning of contextual representations. Inspired\nby Taylor series, we design LSFConv, which learns both low-order fundamental\nand high-order refinement information from explicitly encoded local geometric\nstructures. Integrating the GPM and LSFConv as fundamental components, we\nconstruct GPSFormer, a cutting-edge Transformer that effectively captures\nglobal and local structures of point clouds. Extensive experiments validate\nGPSFormer's effectiveness in three point cloud tasks: shape classification,\npart segmentation, and few-shot learning. The code of GPSFormer is available at\n\\url{https://github.com/changshuowang/GPSFormer}.\n","authors":["Changshuo Wang","Meiqing Wu","Siew-Kei Lam","Xin Ning","Shangshu Yu","Ruiping Wang","Weijun Li","Thambipillai Srikanthan"],"pdf_url":"https://arxiv.org/pdf/2407.13519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13517v1","updated":"2024-07-18T13:48:52Z","published":"2024-07-18T13:48:52Z","title":"Mask2Map: Vectorized HD Map Construction Using Bird's Eye View\n  Segmentation Masks","summary":"  In this paper, we introduce Mask2Map, a novel end-to-end online HD map\nconstruction method designed for autonomous driving applications. Our approach\nfocuses on predicting the class and ordered point set of map instances within a\nscene, represented in the bird's eye view (BEV). Mask2Map consists of two\nprimary components: the Instance-Level Mask Prediction Network (IMPNet) and the\nMask-Driven Map Prediction Network (MMPNet). IMPNet generates Mask-Aware\nQueries and BEV Segmentation Masks to capture comprehensive semantic\ninformation globally. Subsequently, MMPNet enhances these query features using\nlocal contextual information through two submodules: the Positional Query\nGenerator (PQG) and the Geometric Feature Extractor (GFE). PQG extracts\ninstance-level positional queries by embedding BEV positional information into\nMask-Aware Queries, while GFE utilizes BEV Segmentation Masks to generate\npoint-level geometric features. However, we observed limited performance in\nMask2Map due to inter-network inconsistency stemming from different predictions\nto Ground Truth (GT) matching between IMPNet and MMPNet. To tackle this\nchallenge, we propose the Inter-network Denoising Training method, which guides\nthe model to denoise the output affected by both noisy GT queries and perturbed\nGT Segmentation Masks. Our evaluation conducted on nuScenes and Argoverse2\nbenchmarks demonstrates that Mask2Map achieves remarkable performance\nimprovements over previous state-of-the-art methods, with gains of 10.1% mAP\nand 4.1 mAP, respectively. Our code can be found at\nhttps://github.com/SehwanChoi0307/Mask2Map.\n","authors":["Sehwan Choi","Jungho Kim","Hongjae Shin","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2407.13517v1.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2402.14000v3","updated":"2024-07-18T13:43:41Z","published":"2024-02-21T18:36:26Z","title":"Real-time 3D-aware Portrait Editing from a Single Image","summary":"  This work presents 3DPE, a practical method that can efficiently edit a face\nimage following given prompts, like reference images or text descriptions, in a\n3D-aware manner. To this end, a lightweight module is distilled from a 3D\nportrait generator and a text-to-image model, which provide prior knowledge of\nface geometry and superior editing capability, respectively. Such a design\nbrings two compelling advantages over existing approaches. First, our method\nachieves real-time editing with a feedforward network (i.e., ~0.04s per image),\nover 100x faster than the second competitor. Second, thanks to the powerful\npriors, our module could focus on the learning of editing-related variations,\nsuch that it manages to handle various types of editing simultaneously in the\ntraining phase and further supports fast adaptation to user-specified\ncustomized types of editing during inference (e.g., with ~5min fine-tuning per\nstyle).\n","authors":["Qingyan Bai","Zifan Shi","Yinghao Xu","Hao Ouyang","Qiuyu Wang","Ceyuan Yang","Xuan Wang","Gordon Wetzstein","Yujun Shen","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2402.14000v3.pdf","comment":"ECCV 2024 camera-ready version. Project page:\n  https://github.com/EzioBy/3dpe"},{"id":"http://arxiv.org/abs/2407.13500v1","updated":"2024-07-18T13:32:36Z","published":"2024-07-18T13:32:36Z","title":"FADE: A Task-Agnostic Upsampling Operator for Encoder-Decoder\n  Architectures","summary":"  The goal of this work is to develop a task-agnostic feature upsampling\noperator for dense prediction where the operator is required to facilitate not\nonly region-sensitive tasks like semantic segmentation but also\ndetail-sensitive tasks such as image matting. Prior upsampling operators often\ncan work well in either type of the tasks, but not both. We argue that\ntask-agnostic upsampling should dynamically trade off between semantic\npreservation and detail delineation, instead of having a bias between the two\nproperties. In this paper, we present FADE, a novel, plug-and-play,\nlightweight, and task-agnostic upsampling operator by fusing the assets of\ndecoder and encoder features at three levels: i) considering both the encoder\nand decoder feature in upsampling kernel generation; ii) controlling the\nper-point contribution of the encoder/decoder feature in upsampling kernels\nwith an efficient semi-shift convolutional operator; and iii) enabling the\nselective pass of encoder features with a decoder-dependent gating mechanism\nfor compensating details. To improve the practicality of FADE, we additionally\nstudy parameter- and memory-efficient implementations of semi-shift\nconvolution. We analyze the upsampling behavior of FADE on toy data and show\nthrough large-scale experiments that FADE is task-agnostic with consistent\nperformance improvement on a number of dense prediction tasks with little extra\ncost. For the first time, we demonstrate robust feature upsampling on both\nregion- and detail-sensitive tasks successfully. Code is made available at:\nhttps://github.com/poppinace/fade\n","authors":["Hao Lu","Wenze Liu","Hongtao Fu","Zhiguo Cao"],"pdf_url":"https://arxiv.org/pdf/2407.13500v1.pdf","comment":"Accepted to International Journal of Computer Vision. Extended\n  version of ECCV 2022 paper at arXiv:2207.10392"},{"id":"http://arxiv.org/abs/2404.02697v2","updated":"2024-07-18T13:25:04Z","published":"2024-04-03T12:54:16Z","title":"Which Model Generated This Image? A Model-Agnostic Approach for Origin\n  Attribution","summary":"  Recent progress in visual generative models enables the generation of\nhigh-quality images. To prevent the misuse of generated images, it is important\nto identify the origin model that generates them. In this work, we study the\norigin attribution of generated images in a practical setting where only a few\nimages generated by a source model are available and the source model cannot be\naccessed. The goal is to check if a given image is generated by the source\nmodel. We first formulate this problem as a few-shot one-class classification\ntask. To solve the task, we propose OCC-CLIP, a CLIP-based framework for\nfew-shot one-class classification, enabling the identification of an image's\nsource model, even among multiple candidates. Extensive experiments\ncorresponding to various generative models verify the effectiveness of our\nOCC-CLIP framework. Furthermore, an experiment based on the recently released\nDALL-E 3 API verifies the real-world applicability of our solution.\n","authors":["Fengyuan Liu","Haochen Luo","Yiming Li","Philip Torr","Jindong Gu"],"pdf_url":"https://arxiv.org/pdf/2404.02697v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14627v2","updated":"2024-07-18T13:10:22Z","published":"2024-03-21T17:59:58Z","title":"MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images","summary":"  We introduce MVSplat, an efficient model that, given sparse multi-view images\nas input, predicts clean feed-forward 3D Gaussians. To accurately localize the\nGaussian centers, we build a cost volume representation via plane sweeping,\nwhere the cross-view feature similarities stored in the cost volume can provide\nvaluable geometry cues to the estimation of depth. We also learn other Gaussian\nprimitives' parameters jointly with the Gaussian centers while only relying on\nphotometric supervision. We demonstrate the importance of the cost volume\nrepresentation in learning feed-forward Gaussians via extensive experimental\nevaluations. On the large-scale RealEstate10K and ACID benchmarks, MVSplat\nachieves state-of-the-art performance with the fastest feed-forward inference\nspeed (22~fps). More impressively, compared to the latest state-of-the-art\nmethod pixelSplat, MVSplat uses $10\\times$ fewer parameters and infers more\nthan $2\\times$ faster while providing higher appearance and geometry quality as\nwell as better cross-dataset generalization.\n","authors":["Yuedong Chen","Haofei Xu","Chuanxia Zheng","Bohan Zhuang","Marc Pollefeys","Andreas Geiger","Tat-Jen Cham","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2403.14627v2.pdf","comment":"ECCV2024, Project page: https://donydchen.github.io/mvsplat, Code:\n  https://github.com/donydchen/mvsplat"},{"id":"http://arxiv.org/abs/2407.13488v1","updated":"2024-07-18T13:08:55Z","published":"2024-07-18T13:08:55Z","title":"Similarity over Factuality: Are we making progress on multimodal\n  out-of-context misinformation detection?","summary":"  Out-of-context (OOC) misinformation poses a significant challenge in\nmultimodal fact-checking, where images are paired with texts that misrepresent\ntheir original context to support false narratives. Recent research in\nevidence-based OOC detection has seen a trend towards increasingly complex\narchitectures, incorporating Transformers, foundation models, and large\nlanguage models. In this study, we introduce a simple yet robust baseline,\nwhich assesses MUltimodal SimilaritiEs (MUSE), specifically the similarity\nbetween image-text pairs and external image and text evidence. Our results\ndemonstrate that MUSE, when used with conventional classifiers like Decision\nTree, Random Forest, and Multilayer Perceptron, can compete with and even\nsurpass the state-of-the-art on the NewsCLIPpings and VERITE datasets.\nFurthermore, integrating MUSE in our proposed \"Attentive Intermediate\nTransformer Representations\" (AITR) significantly improved performance, by 3.3%\nand 7.5% on NewsCLIPpings and VERITE, respectively. Nevertheless, the success\nof MUSE, relying on surface-level patterns and shortcuts, without examining\nfactuality and logical inconsistencies, raises critical questions about how we\ndefine the task, construct datasets, collect external evidence and overall, how\nwe assess progress in the field. We release our code at:\nhttps://github.com/stevejpapad/outcontext-misinfo-progress\n","authors":["Stefanos-Iordanis Papadopoulos","Christos Koutlis","Symeon Papadopoulos","Panagiotis C. Petrantonakis"],"pdf_url":"https://arxiv.org/pdf/2407.13488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13483v1","updated":"2024-07-18T13:02:57Z","published":"2024-07-18T13:02:57Z","title":"SCAPE: A Simple and Strong Category-Agnostic Pose Estimator","summary":"  Category-Agnostic Pose Estimation (CAPE) aims to localize keypoints on an\nobject of any category given few exemplars in an in-context manner. Prior arts\ninvolve sophisticated designs, e.g., sundry modules for similarity calculation\nand a two-stage framework, or takes in extra heatmap generation and\nsupervision. We notice that CAPE is essentially a task about feature matching,\nwhich can be solved within the attention process. Therefore we first streamline\nthe architecture into a simple baseline consisting of several pure\nself-attention layers and an MLP regression head -- this simplification means\nthat one only needs to consider the attention quality to boost the performance\nof CAPE. Towards an effective attention process for CAPE, we further introduce\ntwo key modules: i) a global keypoint feature perceptor to inject global\nsemantic information into support keypoints, and ii) a keypoint attention\nrefiner to enhance inter-node correlation between keypoints. They jointly form\na Simple and strong Category-Agnostic Pose Estimator (SCAPE). Experimental\nresults show that SCAPE outperforms prior arts by 2.2 and 1.3 PCK under 1-shot\nand 5-shot settings with faster inference speed and lighter model capacity,\nexcelling in both accuracy and efficiency. Code and models are available at\nhttps://github.com/tiny-smart/SCAPE\n","authors":["Yujia Liang","Zixuan Ye","Wenze Liu","Hao Lu"],"pdf_url":"https://arxiv.org/pdf/2407.13483v1.pdf","comment":"Accepted to ECCV 2024. Code is available at\n  https://github.com/tiny-smart/SCAPE"},{"id":"http://arxiv.org/abs/2404.06859v3","updated":"2024-07-18T13:00:42Z","published":"2024-04-10T09:35:36Z","title":"Multi-Label Continual Learning for the Medical Domain: A Novel Benchmark","summary":"  Despite the critical importance of the medical domain in Deep Learning, most\nof the research in this area solely focuses on training models in static\nenvironments. It is only in recent years that research has begun to address\ndynamic environments and tackle the Catastrophic Forgetting problem through\nContinual Learning (CL) techniques. Previous studies have primarily focused on\nscenarios such as Domain Incremental Learning and Class Incremental Learning,\nwhich do not fully capture the complexity of real-world applications.\nTherefore, in this work, we propose a novel benchmark combining the challenges\nof new class arrivals and domain shifts in a single framework, by considering\nthe New Instances and New Classes (NIC) scenario. This benchmark aims to model\na realistic CL setting for the multi-label classification problem in medical\nimaging. Additionally, it encompasses a greater number of tasks compared to\npreviously tested scenarios. Specifically, our benchmark consists of two\ndatasets (NIH and CXP), nineteen classes, and seven tasks, a stream longer than\nthe previously tested ones. To solve common challenges (e.g., the task\ninference problem) found in the CIL and NIC scenarios, we propose a novel\napproach called Replay Consolidation with Label Propagation (RCLP). Our method\nsurpasses existing approaches, exhibiting superior performance with minimal\nforgetting.\n","authors":["Marina Ceccon","Davide Dalle Pezze","Alessandro Fabris","Gian Antonio Susto"],"pdf_url":"https://arxiv.org/pdf/2404.06859v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11426v2","updated":"2024-07-18T12:58:19Z","published":"2024-04-17T14:33:41Z","title":"SPAMming Labels: Efficient Annotations for the Trackers of Tomorrow","summary":"  Increasing the annotation efficiency of trajectory annotations from videos\nhas the potential to enable the next generation of data-hungry tracking\nalgorithms to thrive on large-scale datasets. Despite the importance of this\ntask, there are currently very few works exploring how to efficiently label\ntracking datasets comprehensively. In this work, we introduce SPAM, a video\nlabel engine that provides high-quality labels with minimal human intervention.\nSPAM is built around two key insights: i) most tracking scenarios can be easily\nresolved. To take advantage of this, we utilize a pre-trained model to generate\nhigh-quality pseudo-labels, reserving human involvement for a smaller subset of\nmore difficult instances; ii) handling the spatiotemporal dependencies of track\nannotations across time can be elegantly and efficiently formulated through\ngraphs. Therefore, we use a unified graph formulation to address the annotation\nof both detections and identity association for tracks across time. Based on\nthese insights, SPAM produces high-quality annotations with a fraction of\nground truth labeling cost. We demonstrate that trackers trained on SPAM labels\nachieve comparable performance to those trained on human annotations while\nrequiring only $3-20\\%$ of the human labeling effort. Hence, SPAM paves the way\ntowards highly efficient labeling of large-scale tracking datasets. We release\nall models and code.\n","authors":["Orcun Cetintas","Tim Meinhardt","Guillem Brasó","Laura Leal-Taixé"],"pdf_url":"https://arxiv.org/pdf/2404.11426v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13460v1","updated":"2024-07-18T12:35:46Z","published":"2024-07-18T12:35:46Z","title":"SA-DVAE: Improving Zero-Shot Skeleton-Based Action Recognition by\n  Disentangled Variational Autoencoders","summary":"  Existing zero-shot skeleton-based action recognition methods utilize\nprojection networks to learn a shared latent space of skeleton features and\nsemantic embeddings. The inherent imbalance in action recognition datasets,\ncharacterized by variable skeleton sequences yet constant class labels,\npresents significant challenges for alignment. To address the imbalance, we\npropose SA-DVAE -- Semantic Alignment via Disentangled Variational\nAutoencoders, a method that first adopts feature disentanglement to separate\nskeleton features into two independent parts -- one is semantic-related and\nanother is irrelevant -- to better align skeleton and semantic features. We\nimplement this idea via a pair of modality-specific variational autoencoders\ncoupled with a total correction penalty. We conduct experiments on three\nbenchmark datasets: NTU RGB+D, NTU RGB+D 120 and PKU-MMD, and our experimental\nresults show that SA-DAVE produces improved performance over existing methods.\nThe code is available at https://github.com/pha123661/SA-DVAE.\n","authors":["Sheng-Wei Li","Zi-Xiang Wei","Wei-Jie Chen","Yi-Hsin Yu","Chih-Yuan Yang","Jane Yung-jen Hsu"],"pdf_url":"https://arxiv.org/pdf/2407.13460v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2404.07202v2","updated":"2024-07-18T12:30:48Z","published":"2024-04-10T17:59:20Z","title":"UMBRAE: Unified Multimodal Brain Decoding","summary":"  We address prevailing challenges of the brain-powered research, departing\nfrom the observation that the literature hardly recover accurate spatial\ninformation and require subject-specific models. To address these challenges,\nwe propose UMBRAE, a unified multimodal decoding of brain signals. First, to\nextract instance-level conceptual and spatial details from neural signals, we\nintroduce an efficient universal brain encoder for multimodal-brain alignment\nand recover object descriptions at multiple levels of granularity from\nsubsequent multimodal large language model (MLLM). Second, we introduce a\ncross-subject training strategy mapping subject-specific features to a common\nfeature space. This allows a model to be trained on multiple subjects without\nextra resources, even yielding superior results compared to subject-specific\nmodels. Further, we demonstrate this supports weakly-supervised adaptation to\nnew subjects, with only a fraction of the total training data. Experiments\ndemonstrate that UMBRAE not only achieves superior results in the newly\nintroduced tasks but also outperforms methods in well established tasks. To\nassess our method, we construct and share with the community a comprehensive\nbrain understanding benchmark BrainHub. Our code and benchmark are available at\nhttps://weihaox.github.io/UMBRAE.\n","authors":["Weihao Xia","Raoul de Charette","Cengiz Öztireli","Jing-Hao Xue"],"pdf_url":"https://arxiv.org/pdf/2404.07202v2.pdf","comment":"ECCV 2024. Project: https://weihaox.github.io/UMBRAE"},{"id":"http://arxiv.org/abs/2403.11131v2","updated":"2024-07-18T12:21:15Z","published":"2024-03-17T07:47:26Z","title":"Omni-Recon: Harnessing Image-based Rendering for General-Purpose Neural\n  Radiance Fields","summary":"  Recent breakthroughs in Neural Radiance Fields (NeRFs) have sparked\nsignificant demand for their integration into real-world 3D applications.\nHowever, the varied functionalities required by different 3D applications often\nnecessitate diverse NeRF models with various pipelines, leading to tedious NeRF\ntraining for each target task and cumbersome trial-and-error experiments.\nDrawing inspiration from the generalization capability and adaptability of\nemerging foundation models, our work aims to develop one general-purpose NeRF\nfor handling diverse 3D tasks. We achieve this by proposing a framework called\nOmni-Recon, which is capable of (1) generalizable 3D reconstruction and\nzero-shot multitask scene understanding, and (2) adaptability to diverse\ndownstream 3D applications such as real-time rendering and scene editing. Our\nkey insight is that an image-based rendering pipeline, with accurate geometry\nand appearance estimation, can lift 2D image features into their 3D\ncounterparts, thus extending widely explored 2D tasks to the 3D world in a\ngeneralizable manner. Specifically, our Omni-Recon features a general-purpose\nNeRF model using image-based rendering with two decoupled branches: one complex\ntransformer-based branch that progressively fuses geometry and appearance\nfeatures for accurate geometry estimation, and one lightweight branch for\npredicting blending weights of source views. This design achieves\nstate-of-the-art (SOTA) generalizable 3D surface reconstruction quality with\nblending weights reusable across diverse tasks for zero-shot multitask scene\nunderstanding. In addition, it can enable real-time rendering after baking the\ncomplex geometry branch into meshes, swift adaptation to achieve SOTA\ngeneralizable 3D understanding performance, and seamless integration with 2D\ndiffusion models for text-guided 3D editing.\n","authors":["Yonggan Fu","Huaizhi Qu","Zhifan Ye","Chaojian Li","Kevin Zhao","Yingyan Lin"],"pdf_url":"https://arxiv.org/pdf/2403.11131v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13442v1","updated":"2024-07-18T12:11:12Z","published":"2024-07-18T12:11:12Z","title":"BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in\n  Vision-language Models","summary":"  Vision language models (VLMs) perceive the world through a combination of a\nvisual encoder and a large language model (LLM). The visual encoder,\npre-trained on large-scale vision-text datasets, provides zero-shot\ngeneralization to visual data, and the LLM endows its high reasoning ability to\nVLMs. It leads VLMs to achieve high performance on wide benchmarks without\nfine-tuning, exhibiting zero or few-shot capability. However, recent studies\nshow that VLMs are vulnerable to hallucination. This undesirable behavior\ndegrades reliability and credibility, thereby making users unable to fully\ntrust the output from VLMs. To enhance trustworthiness and better tackle the\nhallucination of VLMs, we curate a new evaluation dataset, called the\nBEfore-AFter hallucination dataset (BEAF), and introduce new metrics: True\nUnderstanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID).\nUnlike prior works that focus only on constructing questions and answers, the\nkey idea of our benchmark is to manipulate visual scene information by image\nediting models and to design the metrics based on scene changes. This allows us\nto clearly assess whether VLMs correctly understand a given scene by observing\nthe ability to perceive changes. We also visualize image-wise object\nrelationship by virtue of our two-axis view: vision and text. Upon evaluating\nVLMs with our dataset, we observed that our metrics reveal different aspects of\nVLM hallucination that have not been reported before. Project page:\n\\url{https://beafbench.github.io/}\n","authors":["Moon Ye-Bin","Nam Hyeon-Woo","Wonseok Choi","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2407.13442v1.pdf","comment":"Accepted at ECCV 2024. [Project Pages] https://beafbench.github.io/"},{"id":"http://arxiv.org/abs/2407.13437v1","updated":"2024-07-18T12:07:02Z","published":"2024-07-18T12:07:02Z","title":"FREST: Feature RESToration for Semantic Segmentation under Multiple\n  Adverse Conditions","summary":"  Robust semantic segmentation under adverse conditions is crucial in\nreal-world applications. To address this challenging task in practical\nscenarios where labeled normal condition images are not accessible in training,\nwe propose FREST, a novel feature restoration framework for source-free domain\nadaptation (SFDA) of semantic segmentation to adverse conditions. FREST\nalternates two steps: (1) learning the condition embedding space that only\nseparates the condition information from the features and (2) restoring\nfeatures of adverse condition images on the learned condition embedding space.\nBy alternating these two steps, FREST gradually restores features where the\neffect of adverse conditions is reduced. FREST achieved a state of the art on\ntwo public benchmarks (i.e., ACDC and RobotCar) for SFDA to adverse conditions.\nMoreover, it shows superior generalization ability on unseen datasets.\n","authors":["Sohyun Lee","Namyup Kim","Sungyeon Kim","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.13437v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13426v1","updated":"2024-07-18T11:51:01Z","published":"2024-07-18T11:51:01Z","title":"WiNet: Wavelet-based Incremental Learning for Efficient Medical Image\n  Registration","summary":"  Deep image registration has demonstrated exceptional accuracy and fast\ninference. Recent advances have adopted either multiple cascades or pyramid\narchitectures to estimate dense deformation fields in a coarse-to-fine manner.\nHowever, due to the cascaded nature and repeated composition/warping operations\non feature maps, these methods negatively increase memory usage during training\nand testing. Moreover, such approaches lack explicit constraints on the\nlearning process of small deformations at different scales, thus lacking\nexplainability. In this study, we introduce a model-driven WiNet that\nincrementally estimates scale-wise wavelet coefficients for the\ndisplacement/velocity field across various scales, utilizing the wavelet\ncoefficients derived from the original input image pair. By exploiting the\nproperties of the wavelet transform, these estimated coefficients facilitate\nthe seamless reconstruction of a full-resolution displacement/velocity field\nvia our devised inverse discrete wavelet transform (IDWT) layer. This approach\navoids the complexities of cascading networks or composition operations, making\nour WiNet an explainable and efficient competitor with other coarse-to-fine\nmethods. Extensive experimental results from two 3D datasets show that our\nWiNet is accurate and GPU efficient. The code is available at\nhttps://github.com/x-xc/WiNet .\n","authors":["Xinxing Cheng","Xi Jia","Wenqi Lu","Qiufu Li","Linlin Shen","Alexander Krull","Jinming Duan"],"pdf_url":"https://arxiv.org/pdf/2407.13426v1.pdf","comment":"Accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.13421v1","updated":"2024-07-18T11:43:26Z","published":"2024-07-18T11:43:26Z","title":"CycleMix: Mixing Source Domains for Domain Generalization in\n  Style-Dependent Data","summary":"  As deep learning-based systems have become an integral part of everyday life,\nlimitations in their generalization ability have begun to emerge. Machine\nlearning algorithms typically rely on the i.i.d. assumption, meaning that their\ntraining and validation data are expected to follow the same distribution,\nwhich does not necessarily hold in practice. In the case of image\nclassification, one frequent reason that algorithms fail to generalize is that\nthey rely on spurious correlations present in training data, such as\nassociating image styles with target classes. These associations may not be\npresent in the unseen test data, leading to significant degradation of their\neffectiveness. In this work, we attempt to mitigate this Domain Generalization\n(DG) problem by training a robust feature extractor which disregards features\nattributed to image-style but infers based on style-invariant image\nrepresentations. To achieve this, we train CycleGAN models to learn the\ndifferent styles present in the training data and randomly mix them together to\ncreate samples with novel style attributes to improve generalization.\nExperimental results on the PACS DG benchmark validate the proposed method.\n","authors":["Aristotelis Ballas","Christos Diou"],"pdf_url":"https://arxiv.org/pdf/2407.13421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13417v1","updated":"2024-07-18T11:40:43Z","published":"2024-07-18T11:40:43Z","title":"GDDS: A Single Domain Generalized Defect Detection Frame of Open World\n  Scenario using Gather and Distribute Domain-shift Suppression Network","summary":"  Efficient and intelligent surface defect detection of photovoltaic modules is\ncrucial for improving the quality of photovoltaic modules and ensuring the\nreliable operation of large-scale infrastructure. However, the scenario\ncharacteristics of data distribution deviation make the construction of defect\ndetection models for open world scenarios such as photovoltaic manufacturing\nand power plant inspections a challenge. Therefore, we propose the Gather and\nDistribute Domain shift Suppression Network (GDDS). It adopts a single domain\ngeneralized method that is completely independent of the test samples to\naddress the problem of distribution shift. Using a one-stage network as the\nbaseline network breaks through the limitations of traditional domain\ngeneralization methods that typically use two-stage networks. It not only\nbalances detection accuracy and speed but also simplifies the model deployment\nand application process. The GDDS includes two modules: DeepSpine Module and\nGather and Distribute Module. Specifically, the DeepSpine Module applies a\nwider range of contextual information and suppresses background style shift by\nacquiring and concatenating multi-scale features. The Gather and Distribute\nModule collects and distributes global information to achieve cross layer\ninteractive learning of multi-scale channel features and suppress defect\ninstance shift. Furthermore, the GDDS utilizes normalized Wasserstein distance\nfor similarity measurement, reducing measurement errors caused by bounding box\nposition deviations. We conducted a comprehensive evaluation of GDDS on the EL\nendogenous shift dataset and Photovoltaic inspection infrared image dataset.\nThe experimental results showed that GDDS can adapt to defect detection in open\nworld scenarios faster and better than other state-of-the-art methods.\n","authors":["Haiyong Chen","Yaxiu Zhang","Yan Zhang","Xin Zhang","Xingwei Yan"],"pdf_url":"https://arxiv.org/pdf/2407.13417v1.pdf","comment":"13 images"},{"id":"http://arxiv.org/abs/2401.04339v2","updated":"2024-07-18T11:38:17Z","published":"2024-01-09T03:42:08Z","title":"Memory-Efficient Fine-Tuning for Quantized Diffusion Model","summary":"  The emergence of billion-parameter diffusion models such as Stable Diffusion\nXL, Imagen, and DALL-E 3 has significantly propelled the domain of generative\nAI. However, their large-scale architecture presents challenges in fine-tuning\nand deployment due to high resource demands and slow inference speed. This\npaper explores the relatively unexplored yet promising realm of fine-tuning\nquantized diffusion models. Our analysis revealed that the baseline neglects\nthe distinct patterns in model weights and the different roles throughout time\nsteps when finetuning the diffusion model. To address these limitations, we\nintroduce a novel memory-efficient fine-tuning method specifically designed for\nquantized diffusion models, dubbed TuneQDM. Our approach introduces\nquantization scales as separable functions to consider inter-channel weight\npatterns. Then, it optimizes these scales in a timestep-specific manner for\neffective reflection of the role of each time step. TuneQDM achieves\nperformance on par with its full-precision counterpart while simultaneously\noffering significant memory efficiency. Experimental results demonstrate that\nour method consistently outperforms the baseline in both single-/multi-subject\ngenerations, exhibiting high subject fidelity and prompt fidelity comparable to\nthe full precision model.\n","authors":["Hyogon Ryu","Seohyun Lim","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2401.04339v2.pdf","comment":"Accepted by ECCV2024. Code will be released at\n  https://github.com/ugonfor/TuneQDM"},{"id":"http://arxiv.org/abs/2404.14055v3","updated":"2024-07-18T11:10:02Z","published":"2024-04-22T10:11:31Z","title":"RingID: Rethinking Tree-Ring Watermarking for Enhanced Multi-Key\n  Identification","summary":"  We revisit Tree-Ring Watermarking, a recent diffusion model watermarking\nmethod that demonstrates great robustness to various attacks. We conduct an\nin-depth study on it and reveal that the distribution shift unintentionally\nintroduced by the watermarking process, apart from watermark pattern matching,\ncontributes to its exceptional robustness. Our investigation further exposes\ninherent flaws in its original design, particularly in its ability to identify\nmultiple distinct keys, where distribution shift offers no assistance. Based on\nthese findings and analysis, we present RingID for enhanced multi-key\nidentification. It consists of a novel multi-channel heterogeneous watermarking\napproach designed to seamlessly amalgamate distinctive advantages from diverse\nwatermarks. Coupled with a series of suggested enhancements, RingID exhibits\nsubstantial advancements in multi-key identification. Github Page:\nhttps://github.com/showlab/RingID\n","authors":["Hai Ci","Pei Yang","Yiren Song","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2404.14055v3.pdf","comment":"27 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.09392v2","updated":"2024-07-18T11:03:59Z","published":"2024-07-12T16:16:24Z","title":"Open-Canopy: A Country-Scale Benchmark for Canopy Height Estimation at\n  Very High Resolution","summary":"  Estimating canopy height and canopy height change at meter resolution from\nsatellite imagery has numerous applications, such as monitoring forest health,\nlogging activities, wood resources, and carbon stocks. However, many existing\nforest datasets are based on commercial or closed data sources, restricting the\nreproducibility and evaluation of new approaches. To address this gap, we\nintroduce Open-Canopy, the first open-access and country-scale benchmark for\nvery high resolution (1.5 m) canopy height estimation. Covering more than\n87,000 km$^2$ across France, Open-Canopy combines SPOT satellite imagery with\nhigh resolution aerial LiDAR data. We also propose Open-Canopy-$\\Delta$, the\nfirst benchmark for canopy height change detection between two images taken at\ndifferent years, a particularly challenging task even for recent models. To\nestablish a robust foundation for these benchmarks, we evaluate a comprehensive\nlist of state-of-the-art computer vision models for canopy height estimation.\nThe dataset and associated codes can be accessed at\nhttps://github.com/fajwel/Open-Canopy.\n","authors":["Fajwel Fogel","Yohann Perron","Nikola Besic","Laurent Saint-André","Agnès Pellissier-Tanon","Martin Schwartz","Thomas Boudras","Ibrahim Fayad","Alexandre d'Aspremont","Loic Landrieu","Philippe Ciais"],"pdf_url":"https://arxiv.org/pdf/2407.09392v2.pdf","comment":"22 pages, 8 figures, Submitted to NeurIPS 2024 Datasets and\n  Benchmarks Track"},{"id":"http://arxiv.org/abs/2407.13394v1","updated":"2024-07-18T11:02:52Z","published":"2024-07-18T11:02:52Z","title":"PICASSO: A Feed-Forward Framework for Parametric Inference of CAD\n  Sketches via Rendering Self-Supervision","summary":"  We propose PICASSO, a novel framework CAD sketch parameterization from\nhand-drawn or precise sketch images via rendering self-supervision. Given a\ndrawing of a CAD sketch, the proposed framework turns it into parametric\nprimitives that can be imported into CAD software. Compared to existing\nmethods, PICASSO enables the learning of parametric CAD sketches from either\nprecise or hand-drawn sketch images, even in cases where annotations at the\nparameter level are scarce or unavailable. This is achieved by leveraging the\ngeometric characteristics of sketches as a learning cue to pre-train a CAD\nparameterization network. Specifically, PICASSO comprises two primary\ncomponents: (1) a Sketch Parameterization Network (SPN) that predicts a series\nof parametric primitives from CAD sketch images, and (2) a Sketch Rendering\nNetwork (SRN) that renders parametric CAD sketches in a differentiable manner.\nSRN facilitates the computation of a image-to-image loss, which can be utilized\nto pre-train SPN, thereby enabling zero- and few-shot learning scenarios for\nthe parameterization of hand-drawn sketches. Extensive evaluation on the widely\nused SketchGraphs dataset validates the effectiveness of the proposed\nframework.\n","authors":["Ahmet Serdar Karadeniz","Dimitrios Mallis","Nesryne Mejri","Kseniya Cherenkova","Anis Kacem","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2407.13394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00083v2","updated":"2024-07-18T11:01:46Z","published":"2023-11-30T07:16:11Z","title":"BAM-DETR: Boundary-Aligned Moment Detection Transformer for Temporal\n  Sentence Grounding in Videos","summary":"  Temporal sentence grounding aims to localize moments relevant to a language\ndescription. Recently, DETR-like approaches achieved notable progress by\npredicting the center and length of a target moment. However, they suffer from\nthe issue of center misalignment raised by the inherent ambiguity of moment\ncenters, leading to inaccurate predictions. To remedy this problem, we propose\na novel boundary-oriented moment formulation. In our paradigm, the model no\nlonger needs to find the precise center but instead suffices to predict any\nanchor point within the interval, from which the boundaries are directly\nestimated. Based on this idea, we design a boundary-aligned moment detection\ntransformer, equipped with a dual-pathway decoding process. Specifically, it\nrefines the anchor and boundaries within parallel pathways using global and\nboundary-focused attention, respectively. This separate design allows the model\nto focus on desirable regions, enabling precise refinement of moment\npredictions. Further, we propose a quality-based ranking method, ensuring that\nproposals with high localization qualities are prioritized over incomplete\nones. Experiments on three benchmarks validate the effectiveness of the\nproposed methods. The code is available at\nhttps://github.com/Pilhyeon/BAM-DETR.\n","authors":["Pilhyeon Lee","Hyeran Byun"],"pdf_url":"https://arxiv.org/pdf/2312.00083v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13392v1","updated":"2024-07-18T11:00:49Z","published":"2024-07-18T11:00:49Z","title":"Lightweight Uncertainty Quantification with Simplex Semantic\n  Segmentation for Terrain Traversability","summary":"  For navigation of robots, image segmentation is an important component to\ndetermining a terrain's traversability. For safe and efficient navigation, it\nis key to assess the uncertainty of the predicted segments. Current uncertainty\nestimation methods are limited to a specific choice of model architecture, are\ncostly in terms of training time, require large memory for inference\n(ensembles), or involve complex model architectures (energy-based, hyperbolic,\nmasking). In this paper, we propose a simple, light-weight module that can be\nconnected to any pretrained image segmentation model, regardless of its\narchitecture, with marginal additional computation cost because it reuses the\nmodel's backbone. Our module is based on maximum separation of the segmentation\nclasses by respective prototype vectors. This optimizes the probability that\nout-of-distribution segments are projected in between the prototype vectors.\nThe uncertainty value in the classification label is obtained from the distance\nto the nearest prototype. We demonstrate the effectiveness of our module for\nterrain segmentation.\n","authors":["Judith Dijk","Gertjan Burghouts","Kapil D. Katyal","Bryanna Y. Yeh","Craig T. Knuth","Ella Fokkinga","Tejaswi Kasarla","Pascal Mettes"],"pdf_url":"https://arxiv.org/pdf/2407.13392v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2407.13390v1","updated":"2024-07-18T10:57:29Z","published":"2024-07-18T10:57:29Z","title":"GeometrySticker: Enabling Ownership Claim of Recolorized Neural Radiance\n  Fields","summary":"  Remarkable advancements in the recolorization of Neural Radiance Fields\n(NeRF) have simplified the process of modifying NeRF's color attributes. Yet,\nwith the potential of NeRF to serve as shareable digital assets, there's a\nconcern that malicious users might alter the color of NeRF models and falsely\nclaim the recolorized version as their own. To safeguard against such breaches\nof ownership, enabling original NeRF creators to establish rights over\nrecolorized NeRF is crucial. While approaches like CopyRNeRF have been\nintroduced to embed binary messages into NeRF models as digital signatures for\ncopyright protection, the process of recolorization can remove these binary\nmessages. In our paper, we present GeometrySticker, a method for seamlessly\nintegrating binary messages into the geometry components of radiance fields,\nakin to applying a sticker. GeometrySticker can embed binary messages into NeRF\nmodels while preserving the effectiveness of these messages against\nrecolorization. Our comprehensive studies demonstrate that GeometrySticker is\nadaptable to prevalent NeRF architectures and maintains a commendable level of\nrobustness against various distortions. Project page:\nhttps://kevinhuangxf.github.io/GeometrySticker/.\n","authors":["Xiufeng Huang","Ka Chun Cheung","Simon See","Renjie Wan"],"pdf_url":"https://arxiv.org/pdf/2407.13390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09368v3","updated":"2024-07-18T10:51:27Z","published":"2023-07-18T15:50:04Z","title":"Audio-driven Talking Face Generation with Stabilized Synchronization\n  Loss","summary":"  Talking face generation aims to create realistic videos with accurate lip\nsynchronization and high visual quality, using given audio and reference video\nwhile preserving identity and visual characteristics. In this paper, we start\nby identifying several issues with existing synchronization learning methods.\nThese involve unstable training, lip synchronization, and visual quality issues\ncaused by lip-sync loss, SyncNet, and lip leaking from the identity reference.\nTo address these issues, we first tackle the lip leaking problem by introducing\na silent-lip generator, which changes the lips of the identity reference to\nalleviate leakage. We then introduce stabilized synchronization loss and\nAVSyncNet to overcome problems caused by lip-sync loss and SyncNet. Experiments\nshow that our model outperforms state-of-the-art methods in both visual quality\nand lip synchronization. Comprehensive ablation studies further validate our\nindividual contributions and their cohesive effects.\n","authors":["Dogucan Yaman","Fevziye Irem Eyiokur","Leonard Bärmann","Hazim Kemal Ekenel","Alexander Waibel"],"pdf_url":"https://arxiv.org/pdf/2307.09368v3.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2401.15636v2","updated":"2024-07-18T10:41:29Z","published":"2024-01-28T12:00:31Z","title":"FreeStyle: Free Lunch for Text-guided Style Transfer using Diffusion\n  Models","summary":"  The rapid development of generative diffusion models has significantly\nadvanced the field of style transfer. However, most current style transfer\nmethods based on diffusion models typically involve a slow iterative\noptimization process, e.g., model fine-tuning and textual inversion of style\nconcept. In this paper, we introduce FreeStyle, an innovative style transfer\nmethod built upon a pre-trained large diffusion model, requiring no further\noptimization. Besides, our method enables style transfer only through a text\ndescription of the desired style, eliminating the necessity of style images.\nSpecifically, we propose a dual-stream encoder and single-stream decoder\narchitecture, replacing the conventional U-Net in diffusion models. In the\ndual-stream encoder, two distinct branches take the content image and style\ntext prompt as inputs, achieving content and style decoupling. In the decoder,\nwe further modulate features from the dual streams based on a given content\nimage and the corresponding style text prompt for precise style transfer. Our\nexperimental results demonstrate high-quality synthesis and fidelity of our\nmethod across various content images and style text prompts. Compared with\nstate-of-the-art methods that require training, our FreeStyle approach notably\nreduces the computational burden by thousands of iterations, while achieving\ncomparable or superior performance across multiple evaluation metrics including\nCLIP Aesthetic Score, CLIP Score, and Preference. We have released the code\nanonymously at:\n\\href{https://anonymous.4open.science/r/FreeStyleAnonymous-0F9B}\n","authors":["Feihong He","Gang Li","Mengyuan Zhang","Leilei Yan","Lingyu Si","Fanzhang Li","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2401.15636v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13379v1","updated":"2024-07-18T10:38:24Z","published":"2024-07-18T10:38:24Z","title":"Removing cloud shadows from ground-based solar imagery","summary":"  The study and prediction of space weather entails the analysis of solar\nimages showing structures of the Sun's atmosphere. When imaged from the Earth's\nground, images may be polluted by terrestrial clouds which hinder the detection\nof solar structures. We propose a new method to remove cloud shadows, based on\na U-Net architecture, and compare classical supervision with conditional GAN.\nWe evaluate our method on two different imaging modalities, using both real\nimages and a new dataset of synthetic clouds. Quantitative assessments are\nobtained through image quality indices (RMSE, PSNR, SSIM, and FID). We\ndemonstrate improved results with regards to the traditional cloud removal\ntechnique and a sparse coding baseline, on different cloud types and textures.\n","authors":["Amal Chaoui","Jay Paul Morgan","Adeline Paiement","Jean Aboudarham"],"pdf_url":"https://arxiv.org/pdf/2407.13379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06159v2","updated":"2024-07-18T10:32:50Z","published":"2024-03-10T10:12:32Z","title":"Cracking the neural code for word recognition in convolutional neural\n  networks","summary":"  Learning to read places a strong challenge on the visual system. Years of\nexpertise lead to a remarkable capacity to separate highly similar letters and\nencode their relative positions, thus distinguishing words such as FORM and\nFROM, invariantly over a large range of sizes and absolute positions. How\nneural circuits achieve invariant word recognition remains unknown. Here, we\naddress this issue by training deep neural network models to recognize written\nwords and then analyzing how reading-specialized units emerge and operate\nacross different layers of the network. With literacy, a small subset of units\nbecomes specialized for word recognition in the learned script, similar to the\n\"visual word form area\" of the human brain. We show that these units are\nsensitive to specific letter identities and their distance from the blank space\nat the left or right of a word, thus acting as \"space bigrams\". These units\nspecifically encode ordinal positions and operate by pooling across low and\nhigh-frequency detector units from early layers of the network. The proposed\nneural code provides a mechanistic insight into how information on letter\nidentity and position is extracted and allow for invariant word recognition,\nand leads to predictions for reading behavior, error patterns, and the\nneurophysiology of reading.\n","authors":["Aakash Agrawal","Stanislas Dehaene"],"pdf_url":"https://arxiv.org/pdf/2403.06159v2.pdf","comment":"33 pages, 6 main figures, 4 supplementary figures"},{"id":"http://arxiv.org/abs/2311.17944v2","updated":"2024-07-18T10:31:53Z","published":"2023-11-29T02:17:27Z","title":"PALM: Predicting Actions through Language Models","summary":"  Understanding human activity is a crucial yet intricate task in egocentric\nvision, a field that focuses on capturing visual perspectives from the camera\nwearer's viewpoint. Traditional methods heavily rely on representation learning\nthat is trained on a large amount of video data. However, a major challenge\narises from the difficulty of obtaining effective video representation. This\ndifficulty stems from the complex and variable nature of human activities,\nwhich contrasts with the limited availability of data. In this study, we\nintroduce PALM, an approach that tackles the task of long-term action\nanticipation, which aims to forecast forthcoming sequences of actions over an\nextended period. Our method PALM incorporates an action recognition model to\ntrack previous action sequences and a vision-language model to articulate\nrelevant environmental details. By leveraging the context provided by these\npast events, we devise a prompting strategy for action anticipation using large\nlanguage models (LLMs). Moreover, we implement maximal marginal relevance for\nexample selection to facilitate in-context learning of the LLMs. Our\nexperimental results demonstrate that PALM surpasses the state-of-the-art\nmethods in the task of long-term action anticipation on the Ego4D benchmark. We\nfurther validate PALM on two additional benchmarks, affirming its capacity for\ngeneralization across intricate activities with different sets of taxonomies.\n","authors":["Sanghwan Kim","Daoji Huang","Yongqin Xian","Otmar Hilliges","Luc Van Gool","Xi Wang"],"pdf_url":"https://arxiv.org/pdf/2311.17944v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12702v2","updated":"2024-07-18T10:27:36Z","published":"2024-07-17T16:24:36Z","title":"TransCAD: A Hierarchical Transformer for CAD Sequence Inference from\n  Point Clouds","summary":"  3D reverse engineering, in which a CAD model is inferred given a 3D scan of a\nphysical object, is a research direction that offers many promising practical\napplications. This paper proposes TransCAD, an end-to-end transformer-based\narchitecture that predicts the CAD sequence from a point cloud. TransCAD\nleverages the structure of CAD sequences by using a hierarchical learning\nstrategy. A loop refiner is also introduced to regress sketch primitive\nparameters. Rigorous experimentation on the DeepCAD and Fusion360 datasets show\nthat TransCAD achieves state-of-the-art results. The result analysis is\nsupported with a proposed metric for CAD sequence, the mean Average Precision\nof CAD Sequence, that addresses the limitations of existing metrics.\n","authors":["Elona Dupont","Kseniya Cherenkova","Dimitrios Mallis","Gleb Gusev","Anis Kacem","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2407.12702v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13372v1","updated":"2024-07-18T10:26:53Z","published":"2024-07-18T10:26:53Z","title":"Any Image Restoration with Efficient Automatic Degradation Adaptation","summary":"  With the emergence of mobile devices, there is a growing demand for an\nefficient model to restore any degraded image for better perceptual quality.\nHowever, existing models often require specific learning modules tailored for\neach degradation, resulting in complex architectures and high computation\ncosts. Different from previous work, in this paper, we propose a unified manner\nto achieve joint embedding by leveraging the inherent similarities across\nvarious degradations for efficient and comprehensive restoration. Specifically,\nwe first dig into the sub-latent space of each input to analyze the key\ncomponents and reweight their contributions in a gated manner. The intrinsic\nawareness is further integrated with contextualized attention in an X-shaped\nscheme, maximizing local-global intertwining. Extensive comparison on\nbenchmarking all-in-one restoration setting validates our efficiency and\neffectiveness, i.e., our network sets new SOTA records while reducing model\ncomplexity by approximately -82% in trainable parameters and -85\\% in FLOPs.\nOur code will be made publicly available\nat:https://github.com/Amazingren/AnyIR.\n","authors":["Bin Ren","Eduard Zamfir","Yawei Li","Zongwei Wu","Danda Pani Paudel","Radu Timofte","Nicu Sebe","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2407.13372v1.pdf","comment":"Efficient Any Image Restoration"},{"id":"http://arxiv.org/abs/2407.13368v1","updated":"2024-07-18T10:24:22Z","published":"2024-07-18T10:24:22Z","title":"Affordance Perception by a Knowledge-Guided Vision-Language Model with\n  Efficient Error Correction","summary":"  Mobile robot platforms will increasingly be tasked with activities that\ninvolve grasping and manipulating objects in open world environments.\nAffordance understanding provides a robot with means to realise its goals and\nexecute its tasks, e.g. to achieve autonomous navigation in unknown buildings\nwhere it has to find doors and ways to open these. In order to get actionable\nsuggestions, robots need to be able to distinguish subtle differences between\nobjects, as they may result in different action sequences: doorknobs require\ngrasp and twist, while handlebars require grasp and push. In this paper, we\nimprove affordance perception for a robot in an open-world setting. Our\ncontribution is threefold: (1) We provide an affordance representation with\nprecise, actionable affordances; (2) We connect this knowledge base to a\nfoundational vision-language models (VLM) and prompt the VLM for a wider\nvariety of new and unseen objects; (3) We apply a human-in-the-loop for\ncorrections on the output of the VLM. The mix of affordance representation,\nimage detection and a human-in-the-loop is effective for a robot to search for\nobjects to achieve its goals. We have demonstrated this in a scenario of\nfinding various doors and the many different ways to open them.\n","authors":["Gertjan Burghouts","Marianne Schaaphok","Michael van Bekkum","Wouter Meijer","Fieke Hillerström","Jelle van Mil"],"pdf_url":"https://arxiv.org/pdf/2407.13368v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2402.01832v2","updated":"2024-07-18T10:21:29Z","published":"2024-02-02T18:59:58Z","title":"SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?","summary":"  We present SynthCLIP, a CLIP model trained on entirely synthetic text-image\npairs. Leveraging recent text-to-image (TTI) networks and large language models\n(LLM), we generate synthetic datasets of images and corresponding captions at\nscale, with no human intervention. In this work, we provide an analysis on CLIP\nmodels trained on synthetic data. We provide insights on the data generation\nstrategy, number of samples required, scaling trends, and resulting properties.\nWe also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million\ncaptioned images. Our code, trained models, and data, are released as open\nsource at https://github.com/hammoudhasan/SynthCLIP\n","authors":["Hasan Abed Al Kader Hammoud","Hani Itani","Fabio Pizzati","Philip Torr","Adel Bibi","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2402.01832v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2312.13308v2","updated":"2024-07-18T10:18:51Z","published":"2023-12-20T03:54:03Z","title":"SWinGS: Sliding Windows for Dynamic 3D Gaussian Splatting","summary":"  Novel view synthesis has shown rapid progress recently, with methods capable\nof producing increasingly photorealistic results. 3D Gaussian Splatting has\nemerged as a promising method, producing high-quality renderings of scenes and\nenabling interactive viewing at real-time frame rates. However, it is limited\nto static scenes. In this work, we extend 3D Gaussian Splatting to reconstruct\ndynamic scenes. We model a scene's dynamics using dynamic MLPs, learning\ndeformations from temporally-local canonical representations to per-frame 3D\nGaussians. To disentangle static and dynamic regions, tuneable parameters weigh\neach Gaussian's respective MLP parameters, improving the dynamics modelling of\nimbalanced scenes. We introduce a sliding window training strategy that\npartitions the sequence into smaller manageable windows to handle arbitrary\nlength scenes while maintaining high rendering quality. We propose an adaptive\nsampling strategy to determine appropriate window size hyperparameters based on\nthe scene's motion, balancing training overhead with visual quality. Training a\nseparate dynamic 3D Gaussian model for each sliding window allows the canonical\nrepresentation to change, enabling the reconstruction of scenes with\nsignificant geometric changes. Temporal consistency is enforced using a\nfine-tuning step with self-supervising consistency loss on randomly sampled\nnovel views. As a result, our method produces high-quality renderings of\ngeneral dynamic scenes with competitive quantitative performance, which can be\nviewed in real-time in our dynamic interactive viewer.\n","authors":["Richard Shaw","Michal Nazarczuk","Jifei Song","Arthur Moreau","Sibi Catley-Chandar","Helisa Dhamo","Eduardo Perez-Pellitero"],"pdf_url":"https://arxiv.org/pdf/2312.13308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13363v1","updated":"2024-07-18T10:14:49Z","published":"2024-07-18T10:14:49Z","title":"Learning from the Web: Language Drives Weakly-Supervised Incremental\n  Learning for Semantic Segmentation","summary":"  Current weakly-supervised incremental learning for semantic segmentation\n(WILSS) approaches only consider replacing pixel-level annotations with\nimage-level labels, while the training images are still from well-designed\ndatasets. In this work, we argue that widely available web images can also be\nconsidered for the learning of new classes. To achieve this, firstly we\nintroduce a strategy to select web images which are similar to previously seen\nexamples in the latent space using a Fourier-based domain discriminator. Then,\nan effective caption-driven reharsal strategy is proposed to preserve\npreviously learnt classes. To our knowledge, this is the first work to rely\nsolely on web images for both the learning of new concepts and the preservation\nof the already learned ones in WILSS. Experimental results show that the\nproposed approach can reach state-of-the-art performances without using\nmanually selected and annotated data in the incremental steps.\n","authors":["Chang Liu","Giulia Rizzoli","Pietro Zanuttigh","Fu Li","Yi Niu"],"pdf_url":"https://arxiv.org/pdf/2407.13363v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13362v1","updated":"2024-07-18T10:13:56Z","published":"2024-07-18T10:13:56Z","title":"Open Vocabulary 3D Scene Understanding via Geometry Guided\n  Self-Distillation","summary":"  The scarcity of large-scale 3D-text paired data poses a great challenge on\nopen vocabulary 3D scene understanding, and hence it is popular to leverage\ninternet-scale 2D data and transfer their open vocabulary capabilities to 3D\nmodels through knowledge distillation. However, the existing distillation-based\n3D scene understanding approaches rely on the representation capacity of 2D\nmodels, disregarding the exploration of geometric priors and inherent\nrepresentational advantages offered by 3D data. In this paper, we propose an\neffective approach, namely Geometry Guided Self-Distillation (GGSD), to learn\nsuperior 3D representations from 2D pre-trained models. Specifically, we first\ndesign a geometry guided distillation module to distill knowledge from 2D\nmodels, and then leverage the 3D geometric priors to alleviate the inherent\nnoise in 2D models and enhance the representation learning process. Due to the\nadvantages of 3D representation, the performance of the distilled 3D student\nmodel can significantly surpass that of the 2D teacher model. This motivates us\nto further leverage the representation advantages of 3D data through\nself-distillation. As a result, our proposed GGSD approach outperforms the\nexisting open vocabulary 3D scene understanding methods by a large margin, as\ndemonstrated by our experiments on both indoor and outdoor benchmark datasets.\n","authors":["Pengfei Wang","Yuxi Wang","Shuai Li","Zhaoxiang Zhang","Zhen Lei","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.13362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12863v2","updated":"2024-07-18T10:13:24Z","published":"2023-08-24T15:34:31Z","title":"SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection","summary":"  Multi-modal fusion is increasingly being used for autonomous driving tasks,\nas different modalities provide unique information for feature extraction.\nHowever, the existing two-stream networks are only fused at a specific network\nlayer, which requires a lot of manual attempts to set up. As the CNN goes\ndeeper, the two modal features become more and more advanced and abstract, and\nthe fusion occurs at the feature level with a large gap, which can easily hurt\nthe performance. To reduce the loss of height and depth information during the\nprocess of projecting point clouds into 2D space, we utilize calibration\nparameters to project the point cloud into Altitude Difference Images (ADIs),\nwhich exhibit more distinct road features. In this study, we propose a novel\nfusion architecture called Skip-cross Networks (SkipcrossNets), which combine\nadaptively ADIs and camera images without being bound to a certain fusion\nepoch. Specifically, skip-cross fusion strategy connects each layer to each\nlayer in a feed-forward manner, and for each layer, the feature maps of all\nprevious layers are used as input and its own feature maps are used as input to\nall subsequent layers for the other modality, enhancing feature propagation and\nmulti-modal features fusion. This strategy facilitates selection of the most\nsimilar feature layers from two modalities, enhancing feature reuse and\nproviding complementary effects for sparse point cloud features. The advantages\nof skip-cross fusion strategy is demonstrated through application to the KITTI\nand A2D2 datasets, achieving a MaxF score of 96.85% on KITTI and an F1 score of\n84.84% on A2D2. The model parameters require only 2.33 MB of memory at a speed\nof 68.24 FPS, which can be viable for mobile terminals and embedded devices.\n","authors":["Yan Gong","Xinyu Zhang","Hao Liu","Xinmin Jiang","Zhiwei Li","Xin Gao","Lei Lin","Dafeng Jin","Jun Li","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2308.12863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16432v3","updated":"2024-07-18T10:09:13Z","published":"2024-04-25T09:07:19Z","title":"Point-JEPA: A Joint Embedding Predictive Architecture for\n  Self-Supervised Learning on Point Cloud","summary":"  Recent advancements in self-supervised learning in the point cloud domain\nhave demonstrated significant potential. However, these methods often suffer\nfrom drawbacks, including lengthy pre-training time, the necessity of\nreconstruction in the input space, or the necessity of additional modalities.\nIn order to address these issues, we introduce Point-JEPA, a joint embedding\npredictive architecture designed specifically for point cloud data. To this\nend, we introduce a sequencer that orders point cloud tokens to efficiently\ncompute and utilize tokens proximity based on their indices during target and\ncontext selection. The sequencer also allows shared computations of the tokens\nproximity between context and target selection, further improving the\nefficiency. Experimentally, our method achieves competitive results with\nstate-of-the-art methods while avoiding the reconstruction in the input space\nor additional modality.\n","authors":["Ayumu Saito","Jiju Poovvancheri"],"pdf_url":"https://arxiv.org/pdf/2404.16432v3.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2402.18286v2","updated":"2024-07-18T09:58:03Z","published":"2024-02-28T12:25:01Z","title":"Self-Supervised Learning with Generative Adversarial Networks for\n  Electron Microscopy","summary":"  In this work, we explore the potential of self-supervised learning with\nGenerative Adversarial Networks (GANs) for electron microscopy datasets. We\nshow how self-supervised pretraining facilitates efficient fine-tuning for a\nspectrum of downstream tasks, including semantic segmentation, denoising, noise\n\\& background removal, and super-resolution. Experimentation with varying model\ncomplexities and receptive field sizes reveals the remarkable phenomenon that\nfine-tuned models of lower complexity consistently outperform more complex\nmodels with random weight initialization. We demonstrate the versatility of\nself-supervised pretraining across various downstream tasks in the context of\nelectron microscopy, allowing faster convergence and better performance. We\nconclude that self-supervised pretraining serves as a powerful catalyst, being\nespecially advantageous when limited annotated data are available and efficient\nscaling of computational cost is important.\n","authors":["Bashir Kazimi","Karina Ruzaeva","Stefan Sandfeld"],"pdf_url":"https://arxiv.org/pdf/2402.18286v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13342v1","updated":"2024-07-18T09:40:24Z","published":"2024-07-18T09:40:24Z","title":"Implicit Filtering for Learning Neural Signed Distance Functions from 3D\n  Point Clouds","summary":"  Neural signed distance functions (SDFs) have shown powerful ability in\nfitting the shape geometry. However, inferring continuous signed distance\nfields from discrete unoriented point clouds still remains a challenge. The\nneural network typically fits the shape with a rough surface and omits\nfine-grained geometric details such as shape edges and corners. In this paper,\nwe propose a novel non-linear implicit filter to smooth the implicit field\nwhile preserving high-frequency geometry details. Our novelty lies in that we\ncan filter the surface (zero level set) by the neighbor input points with\ngradients of the signed distance field. By moving the input raw point clouds\nalong the gradient, our proposed implicit filtering can be extended to non-zero\nlevel sets to keep the promise consistency between different level sets, which\nconsequently results in a better regularization of the zero level set. We\nconduct comprehensive experiments in surface reconstruction from objects and\ncomplex scene point clouds, the numerical and visual comparisons demonstrate\nour improvements over the state-of-the-art methods under the widely used\nbenchmarks.\n","authors":["Shengtao Li","Ge Gao","Yudong Liu","Ming Gu","Yu-Shen Liu"],"pdf_url":"https://arxiv.org/pdf/2407.13342v1.pdf","comment":"Accepted by ECCV 2024. Project page:\n  https://list17.github.io/ImplicitFilter"},{"id":"http://arxiv.org/abs/2407.13341v1","updated":"2024-07-18T09:37:55Z","published":"2024-07-18T09:37:55Z","title":"Hybrid Deep Learning-Based for Enhanced Occlusion Segmentation in PICU\n  Patient Monitoring","summary":"  Remote patient monitoring has emerged as a prominent non-invasive method,\nusing digital technologies and computer vision (CV) to replace traditional\ninvasive monitoring. While neonatal and pediatric departments embrace this\napproach, Pediatric Intensive Care Units (PICUs) face the challenge of\nocclusions hindering accurate image analysis and interpretation.\n\\textit{Objective}: In this study, we propose a hybrid approach to effectively\nsegment common occlusions encountered in remote monitoring applications within\nPICUs. Our approach centers on creating a deep-learning pipeline for limited\ntraining data scenarios. \\textit{Methods}: First, a combination of the\nwell-established Google DeepLabV3+ segmentation model with the\ntransformer-based Segment Anything Model (SAM) is devised for occlusion\nsegmentation mask proposal and refinement. We then train and validate this\npipeline using a small dataset acquired from real-world PICU settings with a\nMicrosoft Kinect camera, achieving an Intersection-over-Union (IoU) metric of\n85\\%. \\textit{Results}: Both quantitative and qualitative analyses underscore\nthe effectiveness of our proposed method. The proposed framework yields an\noverall classification performance with 92.5\\% accuracy, 93.8\\% recall, 90.3\\%\nprecision, and 92.0\\% F1-score. Consequently, the proposed method consistently\nimproves the predictions across all metrics, with an average of 2.75\\% gain in\nperformance compared to the baseline CNN-based framework. \\textit{Conclusions}:\nOur proposed hybrid approach significantly enhances the segmentation of\nocclusions in remote patient monitoring within PICU settings. This advancement\ncontributes to improving the quality of care for pediatric patients, addressing\na critical need in clinical practice by ensuring more accurate and reliable\nremote monitoring.\n","authors":["Mario Francisco Munoz","Hoang Vu Huy","Thanh-Dung Le"],"pdf_url":"https://arxiv.org/pdf/2407.13341v1.pdf","comment":"Under revision"},{"id":"http://arxiv.org/abs/2407.13338v1","updated":"2024-07-18T09:35:48Z","published":"2024-07-18T09:35:48Z","title":"Learn to Memorize and to Forget: A Continual Learning Perspective of\n  Dynamic SLAM","summary":"  Simultaneous localization and mapping (SLAM) with implicit neural\nrepresentations has received extensive attention due to the expressive\nrepresentation power and the innovative paradigm of continual learning.\nHowever, deploying such a system within a dynamic environment has not been\nwell-studied. Such challenges are intractable even for conventional algorithms\nsince observations from different views with dynamic objects involved break the\ngeometric and photometric consistency, whereas the consistency lays the\nfoundation for joint optimizing the camera pose and the map parameters. In this\npaper, we best exploit the characteristics of continual learning and propose a\nnovel SLAM framework for dynamic environments. While past efforts have been\nmade to avoid catastrophic forgetting by exploiting an experience replay\nstrategy, we view forgetting as a desirable characteristic. By adaptively\ncontrolling the replayed buffer, the ambiguity caused by moving objects can be\neasily alleviated through forgetting. We restrain the replay of the dynamic\nobjects by introducing a continually-learned classifier for dynamic object\nidentification. The iterative optimization of the neural map and the classifier\nnotably improves the robustness of the SLAM system under a dynamic environment.\nExperiments on challenging datasets verify the effectiveness of the proposed\nframework.\n","authors":["Baicheng Li","Zike Yan","Dong Wu","Hanqing Jiang","Hongbin Zha"],"pdf_url":"https://arxiv.org/pdf/2407.13338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13337v1","updated":"2024-07-18T09:34:47Z","published":"2024-07-18T09:34:47Z","title":"Long-Term 3D Point Tracking By Cost Volume Fusion","summary":"  Long-term point tracking is essential to understand non-rigid motion in the\nphysical world better. Deep learning approaches have recently been incorporated\ninto long-term point tracking, but most prior work predominantly functions in\n2D. Although these methods benefit from the well-established backbones and\nmatching frameworks, the motions they produce do not always make sense in the\n3D physical world. In this paper, we propose the first deep learning framework\nfor long-term point tracking in 3D that generalizes to new points and videos\nwithout requiring test-time fine-tuning. Our model contains a cost volume\nfusion module that effectively integrates multiple past appearances and motion\ninformation via a transformer architecture, significantly enhancing overall\ntracking performance. In terms of 3D tracking performance, our model\nsignificantly outperforms simple scene flow chaining and previous 2D point\ntracking methods, even if one uses ground truth depth and camera pose to\nbackproject 2D point tracks in a synthetic scenario.\n","authors":["Hung Nguyen","Chanho Kim","Rigved Naukarkar","Li Fuxin"],"pdf_url":"https://arxiv.org/pdf/2407.13337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13335v1","updated":"2024-07-18T09:33:17Z","published":"2024-07-18T09:33:17Z","title":"OAT: Object-Level Attention Transformer for Gaze Scanpath Prediction","summary":"  Visual search is important in our daily life. The efficient allocation of\nvisual attention is critical to effectively complete visual search tasks. Prior\nresearch has predominantly modelled the spatial allocation of visual attention\nin images at the pixel level, e.g. using a saliency map. However, emerging\nevidence shows that visual attention is guided by objects rather than pixel\nintensities. This paper introduces the Object-level Attention Transformer\n(OAT), which predicts human scanpaths as they search for a target object within\na cluttered scene of distractors. OAT uses an encoder-decoder architecture. The\nencoder captures information about the position and appearance of the objects\nwithin an image and about the target. The decoder predicts the gaze scanpath as\na sequence of object fixations, by integrating output features from both the\nencoder and decoder. We also propose a new positional encoding that better\nreflects spatial relationships between objects. We evaluated OAT on the Amazon\nbook cover dataset and a new dataset for visual search that we collected. OAT's\npredicted gaze scanpaths align more closely with human gaze patterns, compared\nto predictions by algorithms based on spatial attention on both established\nmetrics and a novel behavioural-based metric. Our results demonstrate the\ngeneralization ability of OAT, as it accurately predicts human scanpaths for\nunseen layouts and target objects.\n","authors":["Yini Fang","Jingling Yu","Haozheng Zhang","Ralf van der Lans","Bertram Shi"],"pdf_url":"https://arxiv.org/pdf/2407.13335v1.pdf","comment":"Accepted in ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13328v1","updated":"2024-07-18T09:29:02Z","published":"2024-07-18T09:29:02Z","title":"Unsupervised Domain Adaptive Lane Detection via Contextual Contrast and\n  Aggregation","summary":"  This paper focuses on two crucial issues in domain-adaptive lane detection,\ni.e., how to effectively learn discriminative features and transfer knowledge\nacross domains. Existing lane detection methods usually exploit a pixel-wise\ncross-entropy loss to train detection models. However, the loss ignores the\ndifference in feature representation among lanes, which leads to inefficient\nfeature learning. On the other hand, cross-domain context dependency crucial\nfor transferring knowledge across domains remains unexplored in existing lane\ndetection methods. This paper proposes a method of Domain-Adaptive lane\ndetection via Contextual Contrast and Aggregation (DACCA), consisting of two\nkey components, i.e., cross-domain contrastive loss and domain-level feature\naggregation, to realize domain-adaptive lane detection. The former can\neffectively differentiate feature representations among categories by taking\ndomain-level features as positive samples. The latter fuses the domain-level\nand pixel-level features to strengthen cross-domain context dependency.\nExtensive experiments show that DACCA significantly improves the detection\nmodel's performance and outperforms existing unsupervised domain adaptive lane\ndetection methods on six datasets, especially achieving the best performance\nwhen transferring from CULane to Tusimple (92.10% accuracy), Tusimple to CULane\n(41.9% F1 score), OpenLane to CULane (43.0% F1 score), and CULane to OpenLane\n(27.6% F1 score).\n","authors":["Kunyang Zhou","Yunjian Feng","Jun Li"],"pdf_url":"https://arxiv.org/pdf/2407.13328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09192v4","updated":"2024-07-18T09:25:59Z","published":"2024-03-14T09:06:49Z","title":"PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient\n  Task Adaptation","summary":"  Recently, the scale of transformers has grown rapidly, which introduces\nconsiderable challenges in terms of training overhead and inference efficiency\nin the scope of task adaptation. Existing works, namely Parameter-Efficient\nFine-Tuning (PEFT) and model compression, have separately investigated the\nchallenges. However, PEFT cannot guarantee the inference efficiency of the\noriginal backbone, especially for large-scale models. Model compression\nrequires significant training costs for structure searching and re-training.\nConsequently, a simple combination of them cannot guarantee accomplishing both\ntraining efficiency and inference efficiency with minimal costs. In this paper,\nwe propose a novel Parallel Yielding Re-Activation (PYRA) method for such a\nchallenge of training-inference efficient task adaptation. PYRA first utilizes\nparallel yielding adaptive weights to comprehensively perceive the data\ndistribution in downstream tasks. A re-activation strategy for token modulation\nis then applied for tokens to be merged, leading to calibrated token features.\nExtensive experiments demonstrate that PYRA outperforms all competing methods\nunder both low compression rate and high compression rate, demonstrating its\neffectiveness and superiority in maintaining both training efficiency and\ninference efficiency for large-scale foundation models. Our code is available\nat https://github.com/THU-MIG/PYRA.\n","authors":["Yizhe Xiong","Hui Chen","Tianxiang Hao","Zijia Lin","Jungong Han","Yuesong Zhang","Guoxin Wang","Yongjun Bao","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2403.09192v4.pdf","comment":"14 pages, 4 figures, Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2406.04898v2","updated":"2024-07-18T09:25:32Z","published":"2024-06-07T12:45:30Z","title":"Labeled Data Selection for Category Discovery","summary":"  Category discovery methods aim to find novel categories in unlabeled visual\ndata. At training time, a set of labeled and unlabeled images are provided,\nwhere the labels correspond to the categories present in the images. The\nlabeled data provides guidance during training by indicating what types of\nvisual properties and features are relevant for performing discovery in the\nunlabeled data. As a result, changing the categories present in the labeled set\ncan have a large impact on what is ultimately discovered in the unlabeled set.\nDespite its importance, the impact of labeled data selection has not been\nexplored in the category discovery literature to date. We show that changing\nthe labeled data can significantly impact discovery performance. Motivated by\nthis, we propose two new approaches for automatically selecting the most\nsuitable labeled data based on the similarity between the labeled and unlabeled\ndata. Our observation is that, unlike in conventional supervised transfer\nlearning, the best labeled is neither too similar, nor too dissimilar, to the\nunlabeled categories. Our resulting approaches obtains state-of-the-art\ndiscovery performance across a range of challenging fine-grained benchmark\ndatasets.\n","authors":["Bingchen Zhao","Nico Lang","Serge Belongie","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2406.04898v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11530v2","updated":"2024-07-18T09:23:36Z","published":"2024-03-18T07:33:56Z","title":"Continual Forgetting for Pre-trained Vision Models","summary":"  For privacy and security concerns, the need to erase unwanted information\nfrom pre-trained vision models is becoming evident nowadays. In real-world\nscenarios, erasure requests originate at any time from both users and model\nowners. These requests usually form a sequence. Therefore, under such a\nsetting, selective information is expected to be continuously removed from a\npre-trained model while maintaining the rest. We define this problem as\ncontinual forgetting and identify two key challenges. (i) For unwanted\nknowledge, efficient and effective deleting is crucial. (ii) For remaining\nknowledge, the impact brought by the forgetting procedure should be minimal. To\naddress them, we propose Group Sparse LoRA (GS-LoRA). Specifically, towards\n(i), we use LoRA modules to fine-tune the FFN layers in Transformer blocks for\neach forgetting task independently, and towards (ii), a simple group sparse\nregularization is adopted, enabling automatic selection of specific LoRA groups\nand zeroing out the others. GS-LoRA is effective, parameter-efficient,\ndata-efficient, and easy to implement. We conduct extensive experiments on face\nrecognition, object detection and image classification and demonstrate that\nGS-LoRA manages to forget specific classes with minimal impact on other\nclasses. Codes will be released on \\url{https://github.com/bjzhb666/GS-LoRA}.\n","authors":["Hongbo Zhao","Bolin Ni","Haochen Wang","Junsong Fan","Fei Zhu","Yuxi Wang","Yuntao Chen","Gaofeng Meng","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11530v2.pdf","comment":"Accepted by CVPR 2024, latest version ahead of carema ready version"},{"id":"http://arxiv.org/abs/2407.13322v1","updated":"2024-07-18T09:22:40Z","published":"2024-07-18T09:22:40Z","title":"Fully Test-Time rPPG Estimation via Synthetic Signal-Guided Feature\n  Learning","summary":"  Many remote photoplethysmography (rPPG) estimation models have achieved\npromising performance on the training domain but often fail to measure the\nphysiological signals or heart rates (HR) on test domains. Domain\ngeneralization (DG) or domain adaptation (DA) techniques are therefore adopted\nin the offline training stage to adapt the model to the unobserved or observed\ntest domain by referring to all the available source domain data. However, in\nrPPG estimation problems, the adapted model usually confronts challenges of\nestimating target data with various domain information, such as different video\ncapturing settings, individuals of different age ranges, or of different HR\ndistributions. In contrast, Test-Time Adaptation (TTA), by online adapting to\nunlabeled target data without referring to any source data, enables the model\nto adaptively estimate rPPG signals of various unseen domains. In this paper,\nwe first propose a novel TTA-rPPG benchmark, which encompasses various domain\ninformation and HR distributions, to simulate the challenges encountered in\nrPPG estimation. Next, we propose a novel synthetic signal-guided rPPG\nestimation framework with a two-fold purpose. First, we design an effective\nspectral-based entropy minimization to enforce the rPPG model to learn new\ntarget domain information. Second, we develop a synthetic signal-guided feature\nlearning, by synthesizing pseudo rPPG signals as pseudo ground-truths to guide\na conditional generator to generate latent rPPG features. The synthesized rPPG\nsignals and the generated rPPG features are used to guide the rPPG model to\nbroadly cover various HR distributions. Our extensive experiments on the\nTTA-rPPG benchmark show that the proposed method achieves superior performance\nand outperforms previous DG and DA methods across most protocols of the\nproposed TTA-rPPG benchmark.\n","authors":["Pei-Kai Huang","Tzu-Hsien Chen","Ya-Ting Chan","Kuan-Wen Chen","Chiou-Ting Hsu"],"pdf_url":"https://arxiv.org/pdf/2407.13322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13808v3","updated":"2024-07-18T09:15:00Z","published":"2024-03-20T17:59:58Z","title":"On Pretraining Data Diversity for Self-Supervised Learning","summary":"  We explore the impact of training with more diverse datasets, characterized\nby the number of unique samples, on the performance of self-supervised learning\n(SSL) under a fixed computational budget. Our findings consistently demonstrate\nthat increasing pretraining data diversity enhances SSL performance, albeit\nonly when the distribution distance to the downstream data is minimal. Notably,\neven with an exceptionally large pretraining data diversity achieved through\nmethods like web crawling or diffusion-generated data, among other ways, the\ndistribution shift remains a challenge. Our experiments are comprehensive with\nseven SSL methods using large-scale datasets such as ImageNet and YFCC100M\namounting to over 200 GPU days. Code and trained models are available at\nhttps://github.com/hammoudhasan/DiversitySSL\n","authors":["Hasan Abed Al Kader Hammoud","Tuhin Das","Fabio Pizzati","Philip Torr","Adel Bibi","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2403.13808v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2308.12919v2","updated":"2024-07-18T09:14:13Z","published":"2023-08-24T16:47:17Z","title":"Realistic Unsupervised CLIP Fine-tuning with Universal Entropy\n  Optimization","summary":"  The emergence of vision-language models, such as CLIP, has spurred a\nsignificant research effort towards their application for downstream supervised\nlearning tasks. Although some previous studies have explored the unsupervised\nfine-tuning of CLIP, they often rely on prior knowledge in the form of class\nnames associated with ground truth labels. This paper explores a realistic\nunsupervised fine-tuning scenario, considering the presence of\nout-of-distribution samples from unknown classes within the unlabeled data. In\nparticular, we focus on simultaneously enhancing out-of-distribution detection\nand the recognition of instances associated with known classes. To tackle this\nproblem, we present a simple, efficient, and effective approach called\nUniversal Entropy Optimization (UEO). UEO leverages sample-level confidence to\napproximately minimize the conditional entropy of confident instances and\nmaximize the marginal entropy of less confident instances. Apart from\noptimizing the textual prompt, UEO incorporates optimization of channel-wise\naffine transformations within the visual branch of CLIP. Extensive experiments\nacross 15 domains and 4 different types of prior knowledge validate the\neffectiveness of UEO compared to baseline methods. The code is publicly\navailable at \\url{https://github.com/tim-learn/UEO}.\n","authors":["Jian Liang","Lijun Sheng","Zhengbo Wang","Ran He","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2308.12919v2.pdf","comment":"ICML 2024 Highlight"},{"id":"http://arxiv.org/abs/2407.13311v1","updated":"2024-07-18T09:13:34Z","published":"2024-07-18T09:13:34Z","title":"General Vision Encoder Features as Guidance in Medical Image\n  Registration","summary":"  General vision encoders like DINOv2 and SAM have recently transformed\ncomputer vision. Even though they are trained on natural images, such encoder\nmodels have excelled in medical imaging, e.g., in classification, segmentation,\nand registration. However, no in-depth comparison of different state-of-the-art\ngeneral vision encoders for medical registration is available. In this work, we\ninvestigate how well general vision encoder features can be used in the\ndissimilarity metrics for medical image registration. We explore two encoders\nthat were trained on natural images as well as one that was fine-tuned on\nmedical data. We apply the features within the well-established B-spline FFD\nregistration framework. In extensive experiments on cardiac cine MRI data, we\nfind that using features as additional guidance for conventional metrics\nimproves the registration quality. The code is available at\ngithub.com/compai-lab/2024-miccai-koegl.\n","authors":["Fryderyk Kögl","Anna Reithmeir","Vasiliki Sideri-Lampretsa","Ines Machado","Rickmer Braren","Daniel Rückert","Julia A. Schnabel","Veronika A. Zimmer"],"pdf_url":"https://arxiv.org/pdf/2407.13311v1.pdf","comment":"Accepted at WBIR MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.13309v1","updated":"2024-07-18T09:13:08Z","published":"2024-07-18T09:13:08Z","title":"Exposure Completing for Temporally Consistent Neural High Dynamic Range\n  Video Rendering","summary":"  High dynamic range (HDR) video rendering from low dynamic range (LDR) videos\nwhere frames are of alternate exposure encounters significant challenges, due\nto the exposure change and absence at each time stamp. The exposure change and\nabsence make existing methods generate flickering HDR results. In this paper,\nwe propose a novel paradigm to render HDR frames via completing the absent\nexposure information, hence the exposure information is complete and\nconsistent. Our approach involves interpolating neighbor LDR frames in the time\ndimension to reconstruct LDR frames for the absent exposures. Combining the\ninterpolated and given LDR frames, the complete set of exposure information is\navailable at each time stamp. This benefits the fusing process for HDR results,\nreducing noise and ghosting artifacts therefore improving temporal consistency.\nExtensive experimental evaluations on standard benchmarks demonstrate that our\nmethod achieves state-of-the-art performance, highlighting the importance of\nabsent exposure completing in HDR video rendering. The code is available at\nhttps://github.com/cuijiahao666/NECHDR.\n","authors":["Jiahao Cui","Wei Jiang","Zhan Peng","Zhiyu Pan","Zhiguo Cao"],"pdf_url":"https://arxiv.org/pdf/2407.13309v1.pdf","comment":"9 pages, 6 figures, accepted by ACM-MM 2024"},{"id":"http://arxiv.org/abs/2405.13082v2","updated":"2024-07-18T09:12:08Z","published":"2024-05-21T06:44:40Z","title":"A Survey of Artificial Intelligence in Gait-Based Neurodegenerative\n  Disease Diagnosis","summary":"  Recent years have witnessed an increasing global population affected by\nneurodegenerative diseases (NDs), which traditionally require extensive\nhealthcare resources and human effort for medical diagnosis and monitoring. As\na crucial disease-related motor symptom, human gait can be exploited to\ncharacterize different NDs. The current advances in artificial intelligence\n(AI) models enable automatic gait analysis for NDs identification and\nclassification, opening a new avenue to facilitate faster and more\ncost-effective diagnosis of NDs. In this paper, we provide a comprehensive\nsurvey on recent progress of machine learning and deep learning based AI\ntechniques applied to diagnosis of five typical NDs through gait. We provide an\noverview of the process of AI-assisted NDs diagnosis, and present a systematic\ntaxonomy of existing gait data and AI models. Meanwhile, a novel quality\nevaluation criterion is proposed to quantitatively assess the quality of\nexisting studies. Through an extensive review and analysis of 164 studies, we\nidentify and discuss the challenges, potential solutions, and future directions\nin this field. Finally, we envision the prospective utilization of 3D skeleton\ndata for human gait representation and the development of more efficient AI\nmodels for NDs diagnosis. We provide a public resource repository to track and\nfacilitate developments in this emerging field:\nhttps://github.com/Kali-Hac/AI4NDD-Survey.\n","authors":["Haocong Rao","Minlin Zeng","Xuejiao Zhao","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2405.13082v2.pdf","comment":"Article: 46 pages, 9 figures, 7 tables, citing 274 papers. Appendix:\n  29 pages, 1 figure, 5 tables. A up-to-date resource (papers, data, etc.) of\n  this survey (AI4NDD) is provided at https://github.com/Kali-Hac/AI4NDD-Survey"},{"id":"http://arxiv.org/abs/2407.13307v1","updated":"2024-07-18T09:10:25Z","published":"2024-07-18T09:10:25Z","title":"Conformal Performance Range Prediction for Segmentation Output Quality\n  Control","summary":"  Recent works have introduced methods to estimate segmentation performance\nwithout ground truth, relying solely on neural network softmax outputs. These\ntechniques hold potential for intuitive output quality control. However, such\nperformance estimates rely on calibrated softmax outputs, which is often not\nthe case in modern neural networks. Moreover, the estimates do not take into\naccount inherent uncertainty in segmentation tasks. These limitations may\nrender precise performance predictions unattainable, restricting the practical\napplicability of performance estimation methods. To address these challenges,\nwe develop a novel approach for predicting performance ranges with statistical\nguarantees of containing the ground truth with a user specified probability.\nOur method leverages sampling-based segmentation uncertainty estimation to\nderive heuristic performance ranges, and applies split conformal prediction to\ntransform these estimates into rigorous prediction ranges that meet the desired\nguarantees. We demonstrate our approach on the FIVES retinal vessel\nsegmentation dataset and compare five commonly used sampling-based uncertainty\nestimation techniques. Our results show that it is possible to achieve the\ndesired coverage with small prediction ranges, highlighting the potential of\nperformance range prediction as a valuable tool for output quality control.\n","authors":["Anna M. Wundram","Paul Fischer","Michael Muehlebach","Lisa M. Koch","Christian F. Baumgartner"],"pdf_url":"https://arxiv.org/pdf/2407.13307v1.pdf","comment":"Accepted as an oral presentation at MICCAI UNSURE 2024"},{"id":"http://arxiv.org/abs/2308.00428v3","updated":"2024-07-18T09:09:55Z","published":"2023-08-01T10:14:43Z","title":"Multiscale Feature Learning Using Co-Tuplet Loss for Offline Handwritten\n  Signature Verification","summary":"  Handwritten signature verification, crucial for legal and financial\ninstitutions, faces challenges including inter-writer similarity, intra-writer\nvariations, and limited signature samples. To address these, we introduce a\nMultiScale Signature feature learning Network (MS-SigNet) with a novel metric\nlearning loss called the co-tuplet loss, designed for offline handwritten\nsignature verification. MS-SigNet learns both global and regional signature\nfeatures from multiple spatial scales, enhancing feature discrimination. This\napproach effectively distinguishes genuine signatures from skilled forgeries by\ncapturing overall strokes and detailed local differences. The co-tuplet loss,\nfocusing on multiple positive and negative examples, overcomes the limitations\nof typical metric learning losses by addressing inter-writer similarity and\nintra-writer variations and emphasizing informative examples. We also present\nHanSig, a large-scale Chinese signature dataset (available at\nhttps://github.com/hsinmin/HanSig) to support robust system development.\nExperimental results on four benchmark datasets in different languages\ndemonstrate the promising performance of our method in comparison to\nstate-of-the-art approaches.\n","authors":["Fu-Hsien Huang","Hsin-Min Lu"],"pdf_url":"https://arxiv.org/pdf/2308.00428v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02044v4","updated":"2024-07-18T09:08:52Z","published":"2024-01-04T03:09:39Z","title":"Multi-modal vision-language model for generalizable annotation-free\n  pathology localization and clinical diagnosis","summary":"  Defining pathologies automatically from medical images aids the understanding\nof the emergence and progression of diseases, and such an ability is crucial in\nclinical diagnostics. However, existing deep learning models heavily rely on\nexpert annotations and lack generalization capabilities in open clinical\nenvironments. In this study, we present a generalizable vision-language model\nfor Annotation-Free pathology Localization (AFLoc). The core strength of AFLoc\nlies in its extensive multi-level semantic structure-based contrastive\nlearning, which comprehensively aligns multi-granularity medical concepts from\nreports with abundant image features, to adapt to the diverse expressions of\npathologies and unseen pathologies without the reliance on image annotations\nfrom experts. We demonstrate the proof of concept on Chest X-ray images, with\nextensive experimental validation across 6 distinct external datasets,\nencompassing 13 types of chest pathologies. The results demonstrate that AFLoc\nsurpasses state-of-the-art methods in pathology localization and\nclassification, and even outperforms the human benchmark in locating 5\ndifferent pathologies. Additionally, we further verify its generalization\nability by applying it to retinal fundus images. Our approach showcases AFLoc's\nversatilities and underscores its suitability for clinical diagnosis in complex\nclinical environments.\n","authors":["Hao Yang","Hong-Yu Zhou","Zhihuan Li","Yuanxu Gao","Cheng Li","Weijian Huang","Jiarun Liu","Hairong Zheng","Kang Zhang","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02044v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13304v1","updated":"2024-07-18T09:07:23Z","published":"2024-07-18T09:07:23Z","title":"A Dataset and Benchmark for Shape Completion of Fruits for Agricultural\n  Robotics","summary":"  As the population is expected to reach 10 billion by 2050, our agricultural\nproduction system needs to double its productivity despite a decline of human\nworkforce in the agricultural sector. Autonomous robotic systems are one\npromising pathway to increase productivity by taking over labor-intensive\nmanual tasks like fruit picking. To be effective, such systems need to monitor\nand interact with plants and fruits precisely, which is challenging due to the\ncluttered nature of agricultural environments causing, for example, strong\nocclusions. Thus, being able to estimate the complete 3D shapes of objects in\npresence of occlusions is crucial for automating operations such as fruit\nharvesting. In this paper, we propose the first publicly available 3D shape\ncompletion dataset for agricultural vision systems. We provide an RGB-D dataset\nfor estimating the 3D shape of fruits. Specifically, our dataset contains RGB-D\nframes of single sweet peppers in lab conditions but also in a commercial\ngreenhouse. For each fruit, we additionally collected high-precision point\nclouds that we use as ground truth. For acquiring the ground truth shape, we\ndeveloped a measuring process that allows us to record data of real sweet\npepper plants, both in the lab and in the greenhouse with high precision, and\ndetermine the shape of the sensed fruits. We release our dataset, consisting of\nalmost 7000 RGB-D frames belonging to more than 100 different fruits. We\nprovide segmented RGB-D frames, with camera instrinsics to easily obtain\ncolored point clouds, together with the corresponding high-precision,\nocclusion-free point clouds obtained with a high-precision laser scanner. We\nadditionally enable evaluation ofshape completion approaches on a hidden test\nset through a public challenge on a benchmark server.\n","authors":["Federico Magistri","Thomas Läbe","Elias Marks","Sumanth Nagulavancha","Yue Pan","Claus Smitt","Lasse Klingbeil","Michael Halstead","Heiner Kuhlmann","Chris McCool","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2407.13304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16640v2","updated":"2024-07-18T09:01:52Z","published":"2024-05-26T17:31:21Z","title":"A Survey of Multimodal Large Language Model from A Data-centric\n  Perspective","summary":"  Multimodal large language models (MLLMs) enhance the capabilities of standard\nlarge language models by integrating and processing data from multiple\nmodalities, including text, vision, audio, video, and 3D environments. Data\nplays a pivotal role in the development and refinement of these models. In this\nsurvey, we comprehensively review the literature on MLLMs from a data-centric\nperspective. Specifically, we explore methods for preparing multimodal data\nduring the pretraining and adaptation phases of MLLMs. Additionally, we analyze\nthe evaluation methods for the datasets and review the benchmarks for\nevaluating MLLMs. Our survey also outlines potential future research\ndirections. This work aims to provide researchers with a detailed understanding\nof the data-driven aspects of MLLMs, fostering further exploration and\ninnovation in this field.\n","authors":["Tianyi Bai","Hao Liang","Binwang Wan","Yanran Xu","Xi Li","Shiyu Li","Ling Yang","Bozhou Li","Yifan Wang","Bin Cui","Ping Huang","Jiulong Shan","Conghui He","Binhang Yuan","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.16640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01819v4","updated":"2024-07-18T08:57:36Z","published":"2023-10-03T06:16:38Z","title":"TP2O: Creative Text Pair-to-Object Generation using Balance\n  Swap-Sampling","summary":"  Generating creative combinatorial objects from two seemingly unrelated object\ntexts is a challenging task in text-to-image synthesis, often hindered by a\nfocus on emulating existing data distributions. In this paper, we develop a\nstraightforward yet highly effective method, called \\textbf{balance\nswap-sampling}. First, we propose a swapping mechanism that generates a novel\ncombinatorial object image set by randomly exchanging intrinsic elements of two\ntext embeddings through a cutting-edge diffusion model. Second, we introduce a\nbalance swapping region to efficiently sample a small subset from the newly\ngenerated image set by balancing CLIP distances between the new images and\ntheir original generations, increasing the likelihood of accepting the\nhigh-quality combinations. Last, we employ a segmentation method to compare\nCLIP distances among the segmented components, ultimately selecting the most\npromising object from the sampled subset. Extensive experiments demonstrate\nthat our approach outperforms recent SOTA T2I methods. Surprisingly, our\nresults even rival those of human artists, such as frog-broccoli.\n","authors":["Jun Li","Zedong Zhang","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2310.01819v4.pdf","comment":"Accepted by ECCV2024, Project page:\n  https://njustzandyz.github.io/tp2o/"},{"id":"http://arxiv.org/abs/2403.17823v2","updated":"2024-07-18T08:56:11Z","published":"2024-03-26T16:04:19Z","title":"Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders","summary":"  Self-supervised pre-training of image encoders is omnipresent in the\nliterature, particularly following the introduction of Masked autoencoders\n(MAE). Current efforts attempt to learn object-centric representations from\nmotion in videos. In particular, SiamMAE recently introduced a Siamese network,\ntraining a shared-weight encoder from two frames of a video with a high\nasymmetric masking ratio (95%). In this work, we propose CropMAE, an\nalternative approach to the Siamese pre-training introduced by SiamMAE. Our\nmethod specifically differs by exclusively considering pairs of cropped images\nsourced from the same image but cropped differently, deviating from the\nconventional pairs of frames extracted from a video. CropMAE therefore\nalleviates the need for video datasets, while maintaining competitive\nperformances and drastically reducing pre-training and learning time.\nFurthermore, we demonstrate that CropMAE learns similar object-centric\nrepresentations without explicit motion, showing that current self-supervised\nlearning methods do not learn such representations from explicit object motion,\nbut rather thanks to the implicit image transformations that occur between the\ntwo views. Finally, CropMAE achieves the highest masking ratio to date (98.5%),\nenabling the reconstruction of images using only two visible patches. Our code\nis available at https://github.com/alexandre-eymael/CropMAE.\n","authors":["Alexandre Eymaël","Renaud Vandeghen","Anthony Cioppa","Silvio Giancola","Bernard Ghanem","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2403.17823v2.pdf","comment":"19 pages, 7 figures, 5 tables, 3 pages of supplementary material.\n  Paper accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.08265v2","updated":"2024-07-18T08:53:00Z","published":"2024-07-11T08:06:31Z","title":"Enhancing Thermal Infrared Tracking with Natural Language Modeling and\n  Coordinate Sequence Generation","summary":"  Thermal infrared tracking is an essential topic in computer vision tasks\nbecause of its advantage of all-weather imaging. However, most conventional\nmethods utilize only hand-crafted features, while deep learning-based\ncorrelation filtering methods are limited by simple correlation operations.\nTransformer-based methods ignore temporal and coordinate information, which is\ncritical for TIR tracking that lacks texture and color information. In this\npaper, to address these issues, we apply natural language modeling to TIR\ntracking and propose a novel model called NLMTrack, which enhances the\nutilization of coordinate and temporal information. NLMTrack applies an encoder\nthat unifies feature extraction and feature fusion, which simplifies the TIR\ntracking pipeline. To address the challenge of low detail and low contrast in\nTIR images, on the one hand, we design a multi-level progressive fusion module\nthat enhances the semantic representation and incorporates multi-scale\nfeatures. On the other hand, the decoder combines the TIR features and the\ncoordinate sequence features using a causal transformer to generate the target\nsequence step by step. Moreover, we explore an adaptive loss aimed at elevating\ntracking accuracy and a simple template update strategy to accommodate the\ntarget's appearance variations. Experiments show that NLMTrack achieves\nstate-of-the-art performance on multiple benchmarks. The Code is publicly\navailable at \\url{https://github.com/ELOESZHANG/NLMTrack}.\n","authors":["Miao Yan","Ping Zhang","Haofei Zhang","Ruqian Hao","Juanxiu Liu","Xiaoyang Wang","Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08265v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14582v3","updated":"2024-07-18T08:50:45Z","published":"2024-05-23T13:53:50Z","title":"PoseCrafter: One-Shot Personalized Video Synthesis Following Flexible\n  Pose Control","summary":"  In this paper, we introduce PoseCrafter, a one-shot method for personalized\nvideo generation following the control of flexible poses. Built upon Stable\nDiffusion and ControlNet, we carefully design an inference process to produce\nhigh-quality videos without the corresponding ground-truth frames. First, we\nselect an appropriate reference frame from the training video and invert it to\ninitialize all latent variables for generation. Then, we insert the\ncorresponding training pose into the target pose sequences to enhance\nfaithfulness through a trained temporal attention module. Furthermore, to\nalleviate the face and hand degradation resulting from discrepancies between\nposes of training videos and inference poses, we implement simple latent\nediting through an affine transformation matrix involving facial and hand\nlandmarks. Extensive experiments on several datasets demonstrate that\nPoseCrafter achieves superior results to baselines pre-trained on a vast\ncollection of videos under 8 commonly used metrics. Besides, PoseCrafter can\nfollow poses from different individuals or artificial edits and simultaneously\nretain the human identity in an open-domain training video. Our project page is\navailable at https://ml-gsai.github.io/PoseCrafter-demo/.\n","authors":["Yong Zhong","Min Zhao","Zebin You","Xiaofeng Yu","Changwang Zhang","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2405.14582v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12865v2","updated":"2024-07-18T08:45:12Z","published":"2023-09-22T13:39:24Z","title":"Bridging Sensor Gaps via Attention Gated Tuning for Hyperspectral Image\n  Classification","summary":"  Data-hungry HSI classification methods require high-quality labeled HSIs,\nwhich are often costly to obtain. This characteristic limits the performance\npotential of data-driven methods when dealing with limited annotated samples.\nBridging the domain gap between data acquired from different sensors allows us\nto utilize abundant labeled data across sensors to break this bottleneck. In\nthis paper, we propose a novel Attention-Gated Tuning (AGT) strategy and a\ntriplet-structured transformer model, Tri-Former, to address this issue. The\nAGT strategy serves as a bridge, allowing us to leverage existing labeled HSI\ndatasets, even RGB datasets to enhance the performance on new HSI datasets with\nlimited samples. Instead of inserting additional parameters inside the basic\nmodel, we train a lightweight auxiliary branch that takes intermediate features\nas input from the basic model and makes predictions. The proposed AGT resolves\nconflicts between heterogeneous and even cross-modal data by suppressing the\ndisturbing information and enhances the useful information through a soft gate.\nAdditionally, we introduce Tri-Former, a triplet-structured transformer with a\nspectral-spatial separation design that enhances parameter utilization and\ncomputational efficiency, enabling easier and flexible fine-tuning. Comparison\nexperiments conducted on three representative HSI datasets captured by\ndifferent sensors demonstrate the proposed Tri-Former achieves better\nperformance compared to several state-of-the-art methods. Homologous,\nheterologous and cross-modal tuning experiments verified the effectiveness of\nthe proposed AGT. Code has been released at:\n\\href{https://github.com/Cecilia-xue/AGT}{https://github.com/Cecilia-xue/AGT}.\n","authors":["Xizhe Xue","Haokui Zhang","Rong Xiao","Ying Li","Zongwen Bai","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2309.12865v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11962v2","updated":"2024-07-18T08:44:16Z","published":"2024-07-16T17:59:01Z","title":"Motion-Oriented Compositional Neural Radiance Fields for Monocular\n  Dynamic Human Modeling","summary":"  This paper introduces Motion-oriented Compositional Neural Radiance Fields\n(MoCo-NeRF), a framework designed to perform free-viewpoint rendering of\nmonocular human videos via novel non-rigid motion modeling approach. In the\ncontext of dynamic clothed humans, complex cloth dynamics generate non-rigid\nmotions that are intrinsically distinct from skeletal articulations and\ncritically important for the rendering quality. The conventional approach\nmodels non-rigid motions as spatial (3D) deviations in addition to skeletal\ntransformations. However, it is either time-consuming or challenging to achieve\noptimal quality due to its high learning complexity without a direct\nsupervision. To target this problem, we propose a novel approach of modeling\nnon-rigid motions as radiance residual fields to benefit from more direct color\nsupervision in the rendering and utilize the rigid radiance fields as a prior\nto reduce the complexity of the learning process. Our approach utilizes a\nsingle multiresolution hash encoding (MHE) to concurrently learn the canonical\nT-pose representation from rigid skeletal motions and the radiance residual\nfield for non-rigid motions. Additionally, to further improve both training\nefficiency and usability, we extend MoCo-NeRF to support simultaneous training\nof multiple subjects within a single framework, thanks to our effective design\nfor modeling non-rigid motions. This scalability is achieved through the\nintegration of a global MHE and learnable identity codes in addition to\nmultiple local MHEs. We present extensive results on ZJU-MoCap and MonoCap,\nclearly demonstrating state-of-the-art performance in both single- and\nmulti-subject settings. The code and model will be made publicly available at\nthe project page: https://stevejaehyeok.github.io/publications/moco-nerf.\n","authors":["Jaehyeok Kim","Dongyoon Wee","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2407.11962v2.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2307.13337v2","updated":"2024-07-18T08:37:08Z","published":"2023-07-25T08:50:01Z","title":"Overcoming Distribution Mismatch in Quantizing Image Super-Resolution\n  Networks","summary":"  Although quantization has emerged as a promising approach to reducing\ncomputational complexity across various high-level vision tasks, it inevitably\nleads to accuracy loss in image super-resolution (SR) networks. This is due to\nthe significantly divergent feature distributions across different channels and\ninput images of the SR networks, which complicates the selection of a fixed\nquantization range. Existing works address this distribution mismatch problem\nby dynamically adapting quantization ranges to the varying distributions during\ntest time. However, such a dynamic adaptation incurs additional computational\ncosts during inference. In contrast, we propose a new quantization-aware\ntraining scheme that effectively Overcomes the Distribution Mismatch problem in\nSR networks without the need for dynamic adaptation. Intuitively, this mismatch\ncan be mitigated by regularizing the distance between the feature and a fixed\nquantization range. However, we observe that such regularization can conflict\nwith the reconstruction loss during training, negatively impacting SR accuracy.\nTherefore, we opt to regularize the mismatch only when the gradients of the\nregularization are aligned with those of the reconstruction loss. Additionally,\nwe introduce a layer-wise weight clipping correction scheme to determine a more\nsuitable quantization range for layer-wise weights. Experimental results\ndemonstrate that our framework effectively reduces the distribution mismatch\nand achieves state-of-the-art performance with minimal computational overhead.\n","authors":["Cheeun Hong","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2307.13337v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13285v1","updated":"2024-07-18T08:37:08Z","published":"2024-07-18T08:37:08Z","title":"Collaborative real-time vision-based device for olive oil production\n  monitoring","summary":"  This paper proposes an innovative approach to improving quality control of\nolive oil manufacturing and preventing damage to the machinery caused by\nforeign objects. We developed a computer-vision-based system that monitors the\ninput of an olive grinder and promptly alerts operators if a foreign object is\ndetected, indicating it by using guided lasers, audio, and visual cues.\n","authors":["Matija Šuković","Igor Jovančević"],"pdf_url":"https://arxiv.org/pdf/2407.13285v1.pdf","comment":"6 pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.05679v2","updated":"2024-07-18T08:33:43Z","published":"2024-07-08T07:26:08Z","title":"BEVWorld: A Multimodal World Model for Autonomous Driving via Unified\n  BEV Latent Space","summary":"  World models are receiving increasing attention in autonomous driving for\ntheir ability to predict potential future scenarios. In this paper, we present\nBEVWorld, a novel approach that tokenizes multimodal sensor inputs into a\nunified and compact Bird's Eye View (BEV) latent space for environment\nmodeling. The world model consists of two parts: the multi-modal tokenizer and\nthe latent BEV sequence diffusion model. The multi-modal tokenizer first\nencodes multi-modality information and the decoder is able to reconstruct the\nlatent BEV tokens into LiDAR and image observations by ray-casting rendering in\na self-supervised manner. Then the latent BEV sequence diffusion model predicts\nfuture scenarios given action tokens as conditions. Experiments demonstrate the\neffectiveness of BEVWorld in autonomous driving tasks, showcasing its\ncapability in generating future scenes and benefiting downstream tasks such as\nperception and motion prediction. Code will be available at\nhttps://github.com/zympsyche/BevWorld.\n","authors":["Yumeng Zhang","Shi Gong","Kaixin Xiong","Xiaoqing Ye","Xiao Tan","Fan Wang","Jizhou Huang","Hua Wu","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.05679v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2407.13277v1","updated":"2024-07-18T08:31:55Z","published":"2024-07-18T08:31:55Z","title":"URCDM: Ultra-Resolution Image Synthesis in Histopathology","summary":"  Diagnosing medical conditions from histopathology data requires a thorough\nanalysis across the various resolutions of Whole Slide Images (WSI). However,\nexisting generative methods fail to consistently represent the hierarchical\nstructure of WSIs due to a focus on high-fidelity patches. To tackle this, we\npropose Ultra-Resolution Cascaded Diffusion Models (URCDMs) which are capable\nof synthesising entire histopathology images at high resolutions whilst\nauthentically capturing the details of both the underlying anatomy and\npathology at all magnification levels. We evaluate our method on three separate\ndatasets, consisting of brain, breast and kidney tissue, and surpass existing\nstate-of-the-art multi-resolution models. Furthermore, an expert evaluation\nstudy was conducted, demonstrating that URCDMs consistently generate outputs\nacross various resolutions that trained evaluators cannot distinguish from real\nimages. All code and additional examples can be found on GitHub.\n","authors":["Sarah Cechnicka","James Ball","Matthew Baugh","Hadrien Reynaud","Naomi Simmonds","Andrew P. T. Smith","Catherine Horsfield","Candice Roufosse","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2407.13277v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.01152"},{"id":"http://arxiv.org/abs/2109.11369v4","updated":"2024-07-18T08:27:05Z","published":"2021-09-23T13:30:18Z","title":"Recent Advances of Continual Learning in Computer Vision: An Overview","summary":"  In contrast to batch learning where all training data is available at once,\ncontinual learning represents a family of methods that accumulate knowledge and\nlearn continuously with data available in sequential order. Similar to the\nhuman learning process with the ability of learning, fusing, and accumulating\nnew knowledge coming at different time steps, continual learning is considered\nto have high practical significance. Hence, continual learning has been studied\nin various artificial intelligence tasks. In this paper, we present a\ncomprehensive review of the recent progress of continual learning in computer\nvision. In particular, the works are grouped by their representative\ntechniques, including regularization, knowledge distillation, memory,\ngenerative replay, parameter isolation, and a combination of the above\ntechniques. For each category of these techniques, both its characteristics and\napplications in computer vision are presented. At the end of this overview,\nseveral subareas, where continuous knowledge accumulation is potentially\nhelpful while continual learning has not been well studied, are discussed.\n","authors":["Haoxuan Qu","Hossein Rahmani","Li Xu","Bryan Williams","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2109.11369v4.pdf","comment":"June 2024 Version"},{"id":"http://arxiv.org/abs/2405.05079v3","updated":"2024-07-18T08:26:05Z","published":"2024-05-08T14:22:39Z","title":"Power Variable Projection for Initialization-Free Large-Scale Bundle\n  Adjustment","summary":"  Most Bundle Adjustment (BA) solvers like the Levenberg-Marquard algorithm\nrequire a good initialization. Instead, initialization-free BA remains a\nlargely uncharted territory. The under-explored Variable Projection algorithm\n(VarPro) exhibits a wide convergence basin even without initialization. Coupled\nwith object space error formulation, recent works have shown its ability to\nsolve small-scale initialization-free bundle adjustment problem. To make such\ninitialization-free BA approaches scalable, we introduce Power Variable\nProjection (PoVar), extending a recent inverse expansion method based on power\nseries. Importantly, we link the power series expansion to Riemannian manifold\noptimization. This projective framework is crucial to solve large-scale bundle\nadjustment problems without initialization. Using the real-world BAL dataset,\nwe experimentally demonstrate that our solver achieves state-of-the-art results\nin terms of speed and accuracy. To our knowledge, this work is the first to\naddress the scalability of BA without initialization opening new venues for\ninitialization-free structure-from-motion.\n","authors":["Simon Weber","Je Hyeong Hong","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2405.05079v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03310v3","updated":"2024-07-18T08:08:29Z","published":"2024-02-05T18:59:36Z","title":"V-IRL: Grounding Virtual Intelligence in Real Life","summary":"  There is a sensory gulf between the Earth that humans inhabit and the digital\nrealms in which modern AI agents are created. To develop AI agents that can\nsense, think, and act as flexibly as humans in real-world settings, it is\nimperative to bridge the realism gap between the digital and physical worlds.\nHow can we embody agents in an environment as rich and diverse as the one we\ninhabit, without the constraints imposed by real hardware and control? Towards\nthis end, we introduce V-IRL: a platform that enables agents to scalably\ninteract with the real world in a virtual yet realistic environment. Our\nplatform serves as a playground for developing agents that can accomplish\nvarious practical tasks and as a vast testbed for measuring progress in\ncapabilities spanning perception, decision-making, and interaction with\nreal-world data across the entire globe.\n","authors":["Jihan Yang","Runyu Ding","Ellis Brown","Xiaojuan Qi","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2402.03310v3.pdf","comment":"Project page: https://virl-platform.github.io"},{"id":"http://arxiv.org/abs/2407.13254v1","updated":"2024-07-18T08:08:04Z","published":"2024-07-18T08:08:04Z","title":"Make a Strong Teacher with Label Assistance: A Novel Knowledge\n  Distillation Approach for Semantic Segmentation","summary":"  In this paper, we introduce a novel knowledge distillation approach for the\nsemantic segmentation task. Unlike previous methods that rely on power-trained\nteachers or other modalities to provide additional knowledge, our approach does\nnot require complex teacher models or information from extra sensors.\nSpecifically, for the teacher model training, we propose to noise the label and\nthen incorporate it into input to effectively boost the lightweight teacher\nperformance. To ensure the robustness of the teacher model against the\nintroduced noise, we propose a dual-path consistency training strategy\nfeaturing a distance loss between the outputs of two paths. For the student\nmodel training, we keep it consistent with the standard distillation for\nsimplicity. Our approach not only boosts the efficacy of knowledge distillation\nbut also increases the flexibility in selecting teacher and student models. To\ndemonstrate the advantages of our Label Assisted Distillation (LAD) method, we\nconduct extensive experiments on five challenging datasets including\nCityscapes, ADE20K, PASCAL-VOC, COCO-Stuff 10K, and COCO-Stuff 164K, five\npopular models: FCN, PSPNet, DeepLabV3, STDC, and OCRNet, and results show the\neffectiveness and generalization of our approach. We posit that incorporating\nlabels into the input, as demonstrated in our work, will provide valuable\ninsights into related fields. Code is available at\nhttps://github.com/skyshoumeng/Label_Assisted_Distillation.\n","authors":["Shoumeng Qiu","Jie Chen","Xinrun Li","Ru Wan","Xiangyang Xue","Jian Pu"],"pdf_url":"https://arxiv.org/pdf/2407.13254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13252v1","updated":"2024-07-18T08:07:28Z","published":"2024-07-18T08:07:28Z","title":"Unveiling Structural Memorization: Structural Membership Inference\n  Attack for Text-to-Image Diffusion Models","summary":"  With the rapid advancements of large-scale text-to-image diffusion models,\nvarious practical applications have emerged, bringing significant convenience\nto society. However, model developers may misuse the unauthorized data to train\ndiffusion models. These data are at risk of being memorized by the models, thus\npotentially violating citizens' privacy rights. Therefore, in order to judge\nwhether a specific image is utilized as a member of a model's training set,\nMembership Inference Attack (MIA) is proposed to serve as a tool for privacy\nprotection. Current MIA methods predominantly utilize pixel-wise comparisons as\ndistinguishing clues, considering the pixel-level memorization characteristic\nof diffusion models. However, it is practically impossible for text-to-image\nmodels to memorize all the pixel-level information in massive training sets.\nTherefore, we move to the more advanced structure-level memorization.\nObservations on the diffusion process show that the structures of members are\nbetter preserved compared to those of nonmembers, indicating that diffusion\nmodels possess the capability to remember the structures of member images from\ntraining sets. Drawing on these insights, we propose a simple yet effective MIA\nmethod tailored for text-to-image diffusion models. Extensive experimental\nresults validate the efficacy of our approach. Compared to current pixel-level\nbaselines, our approach not only achieves state-of-the-art performance but also\ndemonstrates remarkable robustness against various distortions.\n","authors":["Qiao Li","Xiaomeng Fu","Xi Wang","Jin Liu","Xingyu Gao","Jiao Dai","Jizhong Han"],"pdf_url":"https://arxiv.org/pdf/2407.13252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13246v1","updated":"2024-07-18T08:00:08Z","published":"2024-07-18T08:00:08Z","title":"STS MICCAI 2023 Challenge: Grand challenge on 2D and 3D semi-supervised\n  tooth segmentation","summary":"  Computer-aided design (CAD) tools are increasingly popular in modern dental\npractice, particularly for treatment planning or comprehensive prognosis\nevaluation. In particular, the 2D panoramic X-ray image efficiently detects\ninvisible caries, impacted teeth and supernumerary teeth in children, while the\n3D dental cone beam computed tomography (CBCT) is widely used in orthodontics\nand endodontics due to its low radiation dose. However, there is no open-access\n2D public dataset for children's teeth and no open 3D dental CBCT dataset,\nwhich limits the development of automatic algorithms for segmenting teeth and\nanalyzing diseases. The Semi-supervised Teeth Segmentation (STS) Challenge, a\npioneering event in tooth segmentation, was held as a part of the MICCAI 2023\nToothFairy Workshop on the Alibaba Tianchi platform. This challenge aims to\ninvestigate effective semi-supervised tooth segmentation algorithms to advance\nthe field of dentistry. In this challenge, we provide two modalities including\nthe 2D panoramic X-ray images and the 3D CBCT tooth volumes. In Task 1, the\ngoal was to segment tooth regions in panoramic X-ray images of both adult and\npediatric teeth. Task 2 involved segmenting tooth sections using CBCT volumes.\nLimited labelled images with mostly unlabelled ones were provided in this\nchallenge prompt using semi-supervised algorithms for training. In the\npreliminary round, the challenge received registration and result submission by\n434 teams, with 64 advancing to the final round. This paper summarizes the\ndiverse methods employed by the top-ranking teams in the STS MICCAI 2023\nChallenge.\n","authors":["Yaqi Wang","Yifan Zhang","Xiaodiao Chen","Shuai Wang","Dahong Qian","Fan Ye","Feng Xu","Hongyuan Zhang","Qianni Zhang","Chengyu Wu","Yunxiang Li","Weiwei Cui","Shan Luo","Chengkai Wang","Tianhao Li","Yi Liu","Xiang Feng","Huiyu Zhou","Dongyun Liu","Qixuan Wang","Zhouhao Lin","Wei Song","Yuanlin Li","Bing Wang","Chunshi Wang","Qiupu Chen","Mingqian Li"],"pdf_url":"https://arxiv.org/pdf/2407.13246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00126v2","updated":"2024-07-18T07:59:36Z","published":"2024-01-31T19:11:58Z","title":"Common Sense Reasoning for Deepfake Detection","summary":"  State-of-the-art deepfake detection approaches rely on image-based features\nextracted via neural networks. While these approaches trained in a supervised\nmanner extract likely fake features, they may fall short in representing\nunnatural `non-physical' semantic facial attributes -- blurry hairlines, double\neyebrows, rigid eye pupils, or unnatural skin shading. However, such facial\nattributes are easily perceived by humans and used to discern the authenticity\nof an image based on human common sense. Furthermore, image-based feature\nextraction methods that provide visual explanations via saliency maps can be\nhard to interpret for humans. To address these challenges, we frame deepfake\ndetection as a Deepfake Detection VQA (DD-VQA) task and model human intuition\nby providing textual explanations that describe common sense reasons for\nlabeling an image as real or fake. We introduce a new annotated dataset and\npropose a Vision and Language Transformer-based framework for the DD-VQA task.\nWe also incorporate text and image-aware feature alignment formulation to\nenhance multi-modal representation learning. As a result, we improve upon\nexisting deepfake detection models by integrating our learned vision\nrepresentations, which reason over common sense knowledge from the DD-VQA task.\nWe provide extensive empirical results demonstrating that our method enhances\ndetection performance, generalization ability, and language-based\ninterpretability in the deepfake detection task.\n","authors":["Yue Zhang","Ben Colman","Xiao Guo","Ali Shahriyari","Gaurav Bharaj"],"pdf_url":"https://arxiv.org/pdf/2402.00126v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07042v2","updated":"2024-07-18T07:58:11Z","published":"2024-07-09T17:04:08Z","title":"ProtoSAM: One-Shot Medical Image Segmentation With Foundational Models","summary":"  This work introduces a new framework, ProtoSAM, for one-shot medical image\nsegmentation. It combines the use of prototypical networks, known for few-shot\nsegmentation, with SAM - a natural image foundation model. The method proposed\ncreates an initial coarse segmentation mask using the ALPnet prototypical\nnetwork, augmented with a DINOv2 encoder. Following the extraction of an\ninitial mask, prompts are extracted, such as points and bounding boxes, which\nare then input into the Segment Anything Model (SAM). State-of-the-art results\nare shown on several medical image datasets and demonstrate automated\nsegmentation capabilities using a single image example (one shot) with no need\nfor fine-tuning of the foundation model. Our code is available at:\nhttps://github.com/levayz/ProtoSAM\n","authors":["Lev Ayzenberg","Raja Giryes","Hayit Greenspan"],"pdf_url":"https://arxiv.org/pdf/2407.07042v2.pdf","comment":"12 pages, 3 figures, 4 tables, code:\n  https://github.com/levayz/ProtoSAM"},{"id":"http://arxiv.org/abs/2407.11335v2","updated":"2024-07-18T07:52:52Z","published":"2024-07-16T02:58:33Z","title":"LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction","summary":"  Existing methods enhance open-vocabulary object detection by leveraging the\nrobust open-vocabulary recognition capabilities of Vision-Language Models\n(VLMs), such as CLIP.However, two main challenges emerge:(1) A deficiency in\nconcept representation, where the category names in CLIP's text space lack\ntextual and visual knowledge.(2) An overfitting tendency towards base\ncategories, with the open vocabulary knowledge biased towards base categories\nduring the transfer from VLMs to detectors.To address these challenges, we\npropose the Language Model Instruction (LaMI) strategy, which leverages the\nrelationships between visual concepts and applies them within a simple yet\neffective DETR-like detector, termed LaMI-DETR.LaMI utilizes GPT to construct\nvisual concepts and employs T5 to investigate visual similarities across\ncategories.These inter-category relationships refine concept representation and\navoid overfitting to base categories.Comprehensive experiments validate our\napproach's superior performance over existing methods in the same rigorous\nsetting without reliance on external training resources.LaMI-DETR achieves a\nrare box AP of 43.4 on OV-LVIS, surpassing the previous best by 7.8 rare box\nAP.\n","authors":["Penghui Du","Yu Wang","Yifan Sun","Luting Wang","Yue Liao","Gang Zhang","Errui Ding","Yan Wang","Jingdong Wang","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2407.11335v2.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2405.06228v2","updated":"2024-07-18T07:52:18Z","published":"2024-05-10T03:42:46Z","title":"Context-Guided Spatial Feature Reconstruction for Efficient Semantic\n  Segmentation","summary":"  Semantic segmentation is an important task for numerous applications but it\nis still quite challenging to achieve advanced performance with limited\ncomputational costs. In this paper, we present CGRSeg, an efficient yet\ncompetitive segmentation framework based on context-guided spatial feature\nreconstruction. A Rectangular Self-Calibration Module is carefully designed for\nspatial feature reconstruction and pyramid context extraction. It captures the\naxial global context in both horizontal and vertical directions to explicitly\nmodel rectangular key areas. A shape self-calibration function is designed to\nmake the key areas closer to foreground objects. Besides, a lightweight Dynamic\nPrototype Guided head is proposed to improve the classification of foreground\nobjects by explicit class embedding. Our CGRSeg is extensively evaluated on\nADE20K, COCO-Stuff, and Pascal Context benchmarks, and achieves\nstate-of-the-art semantic performance. Specifically, it achieves $43.6\\%$ mIoU\non ADE20K with only $4.0$ GFLOPs, which is $0.9\\%$ and $2.5\\%$ mIoU better than\nSeaFormer and SegNeXt but with about $38.0\\%$ fewer GFLOPs. Code is available\nat https://github.com/nizhenliang/CGRSeg.\n","authors":["Zhenliang Ni","Xinghao Chen","Yingjie Zhai","Yehui Tang","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2405.06228v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13241v1","updated":"2024-07-18T07:50:46Z","published":"2024-07-18T07:50:46Z","title":"NODER: Image Sequence Regression Based on Neural Ordinary Differential\n  Equations","summary":"  Regression on medical image sequences can capture temporal image pattern\nchanges and predict images at missing or future time points. However, existing\ngeodesic regression methods limit their regression performance by a strong\nunderlying assumption of linear dynamics, while diffusion-based methods have\nhigh computational costs and lack constraints to preserve image topology. In\nthis paper, we propose an optimization-based new framework called NODER, which\nleverages neural ordinary differential equations to capture complex underlying\ndynamics and reduces its high computational cost of handling high-dimensional\nimage volumes by introducing the latent space. We compare our NODER with two\nrecent regression methods, and the experimental results on ADNI and ACDC\ndatasets demonstrate that our method achieves the state-of-the-art performance\nin 3D image regression. Our model needs only a couple of images in a sequence\nfor prediction, which is practical, especially for clinical situations where\nextremely limited image time series are available for analysis. Our source code\nis available at https://github.com/ZedKing12138/NODER-pytorch.\n","authors":["Hao Bai","Yi Hong"],"pdf_url":"https://arxiv.org/pdf/2407.13241v1.pdf","comment":"MICCAI2024"},{"id":"http://arxiv.org/abs/2305.00510v4","updated":"2024-07-18T07:49:58Z","published":"2023-04-30T15:38:36Z","title":"Towards AI-Architecture Liberty: A Comprehensive Survey on Design and\n  Generation of Virtual Architecture by Deep Learning","summary":"  3D shape generation techniques leveraging deep learning have garnered\nsignificant interest from both the computer vision and architectural design\ncommunities, promising to enrich the content in the virtual environment.\nHowever, research on virtual architectural design remains limited, particularly\nregarding designer-AI collaboration and deep learning-assisted design. In our\nsurvey, we reviewed 149 related articles (81.2% of articles published between\n2019 and 2023) covering architectural design, 3D shape techniques, and virtual\nenvironments. Through scrutinizing the literature, we first identify the\nprinciples of virtual architecture and illuminate its current production\nchallenges, including datasets, multimodality, design intuition, and generative\nframeworks. We then introduce the latest approaches to designing and generating\nvirtual buildings leveraging 3D shape generation and summarize four\ncharacteristics of various approaches to virtual architecture. Based on our\nanalysis, we expound on four research agendas, including agency, communication,\nuser consideration, and integrating tools. Additionally, we highlight four\nimportant enablers of ubiquitous interaction with immersive systems in deep\nlearning-assisted architectural generation. Our work contributes to fostering\nunderstanding between designers and deep learning techniques, broadening access\nto designer-AI collaboration. We advocate for interdisciplinary efforts to\naddress this timely research topic, facilitating content designing and\ngeneration in the virtual environment.\n","authors":["Anqi Wang","Jiahua Dong","Lik-Hang Lee","Jiachuan Shen","Pan Hui"],"pdf_url":"https://arxiv.org/pdf/2305.00510v4.pdf","comment":"36 pages, 9 figures, and 5 tables"},{"id":"http://arxiv.org/abs/2406.01867v2","updated":"2024-07-18T07:45:54Z","published":"2024-06-04T00:38:44Z","title":"MoLA: Motion Generation and Editing with Latent Diffusion Enhanced by\n  Adversarial Training","summary":"  In motion generation, controllability as well as generation quality and speed\nis becoming more and more important. There are various motion editing tasks,\nsuch as in-betweening, upper body editing, and path-following, but existing\nmethods perform motion editing with a data-space diffusion model, which is slow\nin inference compared to a latent diffusion model. In this paper, we propose\nMoLA, which provides fast and high-quality motion generation and also can deal\nwith multiple editing tasks in a single framework. For high-quality and fast\ngeneration, we employ a variational autoencoder and latent diffusion model, and\nimprove the performance with adversarial training. In addition, we apply a\ntraining-free guided generation framework to achieve various editing tasks with\nmotion control inputs. We quantitatively show the effectiveness of adversarial\nlearning in text-to-motion generation, and demonstrate the applicability of our\nediting framework to multiple editing tasks in the motion domain.\n","authors":["Kengo Uchida","Takashi Shibuya","Yuhta Takida","Naoki Murata","Shusuke Takahashi","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2406.01867v2.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.06886v4","updated":"2024-07-18T07:41:12Z","published":"2024-07-09T14:14:47Z","title":"Aligning Cyber Space with Physical World: A Comprehensive Survey on\n  Embodied AI","summary":"  Embodied Artificial Intelligence (Embodied AI) is crucial for achieving\nArtificial General Intelligence (AGI) and serves as a foundation for various\napplications that bridge cyberspace and the physical world. Recently, the\nemergence of Multi-modal Large Models (MLMs) and World Models (WMs) have\nattracted significant attention due to their remarkable perception,\ninteraction, and reasoning capabilities, making them a promising architecture\nfor the brain of embodied agents. However, there is no comprehensive survey for\nEmbodied AI in the era of MLMs. In this survey, we give a comprehensive\nexploration of the latest advancements in Embodied AI. Our analysis firstly\nnavigates through the forefront of representative works of embodied robots and\nsimulators, to fully understand the research focuses and their limitations.\nThen, we analyze four main research targets: 1) embodied perception, 2)\nembodied interaction, 3) embodied agent, and 4) sim-to-real adaptation,\ncovering the state-of-the-art methods, essential paradigms, and comprehensive\ndatasets. Additionally, we explore the complexities of MLMs in virtual and real\nembodied agents, highlighting their significance in facilitating interactions\nin dynamic digital and physical environments. Finally, we summarize the\nchallenges and limitations of embodied AI and discuss their potential future\ndirections. We hope this survey will serve as a foundational reference for the\nresearch community and inspire continued innovation. The associated project can\nbe found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.\n","authors":["Yang Liu","Weixing Chen","Yongjie Bai","Jingzhou Luo","Xinshuai Song","Kaixuan Jiang","Zhida Li","Ganlong Zhao","Junyi Lin","Guanbin Li","Wen Gao","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2407.06886v4.pdf","comment":"The first comprehensive review of Embodied AI in the era of MLMs, 37\n  pages. We also provide the paper list for Embodied AI:\n  https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List"},{"id":"http://arxiv.org/abs/2311.04071v5","updated":"2024-07-18T07:24:04Z","published":"2023-11-07T15:35:56Z","title":"Energy-Calibrated VAE with Test Time Free Lunch","summary":"  In this paper, we propose a novel generative model that utilizes a\nconditional Energy-Based Model (EBM) for enhancing Variational Autoencoder\n(VAE), termed Energy-Calibrated VAE (EC-VAE). Specifically, VAEs often suffer\nfrom blurry generated samples due to the lack of a tailored training on the\nsamples generated in the generative direction. On the other hand, EBMs can\ngenerate high-quality samples but require expensive Markov Chain Monte Carlo\n(MCMC) sampling. To address these issues, we introduce a conditional EBM for\ncalibrating the generative direction of VAE during training, without requiring\nit for the generation at test time. In particular, we train EC-VAE upon both\nthe input data and the calibrated samples with adaptive weight to enhance\nefficacy while avoiding MCMC sampling at test time. Furthermore, we extend the\ncalibration idea of EC-VAE to variational learning and normalizing flows, and\napply EC-VAE to an additional application of zero-shot image restoration via\nneural transport prior and range-null theory. We evaluate the proposed method\nwith two applications, including image generation and zero-shot image\nrestoration, and the experimental results show that our method achieves\ncompetitive performance over single-step non-adversarial generation. Our code\nis available at https://github.com/DJ-LYH/EC-VAE.\n","authors":["Yihong Luo","Siya Qiu","Xingjian Tao","Yujun Cai","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2311.04071v5.pdf","comment":"ECCV 2024. Code is available at https://github.com/DJ-LYH/EC-VAE"},{"id":"http://arxiv.org/abs/2311.17717v3","updated":"2024-07-18T07:23:03Z","published":"2023-11-29T15:19:49Z","title":"Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via\n  Lightweight Erasers","summary":"  Concept erasure in text-to-image diffusion models aims to disable pre-trained\ndiffusion models from generating images related to a target concept. To perform\nreliable concept erasure, the properties of robustness and locality are\ndesirable. The former refrains the model from producing images associated with\nthe target concept for any paraphrased or learned prompts, while the latter\npreserves its ability in generating images with non-target concepts. In this\npaper, we propose Reliable Concept Erasing via Lightweight Erasers (Receler).\nIt learns a lightweight Eraser to perform concept erasing while satisfying the\nabove desirable properties through the proposed concept-localized\nregularization and adversarial prompt learning scheme. Experiments with various\nconcepts verify the superiority of Receler over previous methods.\n","authors":["Chi-Pin Huang","Kai-Po Chang","Chung-Ting Tsai","Yung-Hsuan Lai","Fu-En Yang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2311.17717v3.pdf","comment":"ECCV 2024. Project page:\n  https://jasper0314-huang.github.io/receler-concept-erasing/"},{"id":"http://arxiv.org/abs/2312.02228v3","updated":"2024-07-18T07:18:36Z","published":"2023-12-04T03:05:59Z","title":"PixelLM: Pixel Reasoning with Large Multimodal Model","summary":"  While large multimodal models (LMMs) have achieved remarkable progress,\ngenerating pixel-level masks for image reasoning tasks involving multiple\nopen-world targets remains a challenge. To bridge this gap, we introduce\nPixelLM, an effective and efficient LMM for pixel-level reasoning and\nunderstanding. Central to PixelLM is a novel, lightweight pixel decoder and a\ncomprehensive segmentation codebook. The decoder efficiently produces masks\nfrom the hidden embeddings of the codebook tokens, which encode detailed\ntarget-relevant information. With this design, PixelLM harmonizes with the\nstructure of popular LMMs and avoids the need for additional costly\nsegmentation models. Furthermore, we propose a target refinement loss to\nenhance the model's ability to differentiate between multiple targets, leading\nto substantially improved mask quality. To advance research in this area, we\nconstruct MUSE, a high-quality multi-target reasoning segmentation benchmark.\nPixelLM excels across various pixel-level image reasoning and understanding\ntasks, outperforming well-established methods in multiple benchmarks, including\nMUSE, single- and multi-referring segmentation. Comprehensive ablations confirm\nthe efficacy of each proposed component. All code, models, and datasets will be\npublicly available.\n","authors":["Zhongwei Ren","Zhicheng Huang","Yunchao Wei","Yao Zhao","Dongmei Fu","Jiashi Feng","Xiaojie Jin"],"pdf_url":"https://arxiv.org/pdf/2312.02228v3.pdf","comment":"(Accepted by CVPR 2024) Code and models are released at:\n  https://pixellm.github.io/"},{"id":"http://arxiv.org/abs/2407.12483v2","updated":"2024-07-18T07:18:23Z","published":"2024-07-17T11:09:03Z","title":"Towards AI-Powered Video Assistant Referee System (VARS) for Association\n  Football","summary":"  Over the past decade, the technology used by referees in football has\nimproved substantially, enhancing the fairness and accuracy of decisions. This\nprogress has culminated in the implementation of the Video Assistant Referee\n(VAR), an innovation that enables backstage referees to review incidents on the\npitch from multiple points of view. However, the VAR is currently limited to\nprofessional leagues due to its expensive infrastructure and the lack of\nreferees worldwide. In this paper, we present the semi-automated Video\nAssistant Referee System (VARS) that leverages the latest findings in\nmulti-view video analysis. VARS sets a new state-of-the-art on the\nSoccerNet-MVFoul dataset, a multi-view video dataset of football fouls. Our\nVARS achieves a new state-of-the-art on the SoccerNet-MVFoul dataset by\nrecognizing the type of foul in 50% of instances and the appropriate sanction\nin 46% of cases. Finally, we conducted a comparative study to investigate human\nperformance in classifying fouls and their corresponding severity and compared\nthese findings to our VARS. The results of our study highlight the potential of\nour VARS to reach human performance and support football refereeing across all\nlevels of professional and amateur federations.\n","authors":["Jan Held","Anthony Cioppa","Silvio Giancola","Abdullah Hamdi","Christel Devue","Bernard Ghanem","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2407.12483v2.pdf","comment":"The paper is subject to the peer review process of Sports Engineering"},{"id":"http://arxiv.org/abs/2403.09918v2","updated":"2024-07-18T07:12:05Z","published":"2024-03-14T23:31:41Z","title":"Attention-based Class-Conditioned Alignment for Multi-Source Domain\n  Adaptation of Object Detectors","summary":"  Domain adaptation methods for object detection (OD) strive to mitigate the\nimpact of distribution shifts by promoting feature alignment across source and\ntarget domains. Multi-source domain adaptation (MSDA) allows leveraging\nmultiple annotated source datasets and unlabeled target data to improve the\naccuracy and robustness of the detection model. Most state-of-the-art MSDA\nmethods for OD perform feature alignment in a class-agnostic manner. This is\nchallenging since the objects have unique modal information due to variations\nin object appearance across domains. A recent prototype-based approach proposed\na class-wise alignment, yet it suffers from error accumulation due to noisy\npseudo-labels that can negatively affect adaptation with imbalanced data. To\novercome these limitations, we propose an attention-based class-conditioned\nalignment method for MSDA that aligns instances of each object category across\ndomains. In particular, an attention module coupled with an adversarial domain\nclassifier allows learning domain-invariant and class-specific instance\nrepresentations. Experimental results on multiple benchmarking MSDA datasets\nindicate that our method outperforms the state-of-the-art methods and is robust\nto class imbalance using a conceptually simple class-conditioning method. Our\ncode is available at https://github.com/imatif17/ACIA.\n","authors":["Atif Belal","Akhil Meethal","Francisco Perdigon Romero","Marco Pedersoli","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2403.09918v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13221v1","updated":"2024-07-18T07:06:49Z","published":"2024-07-18T07:06:49Z","title":"Multimodal Label Relevance Ranking via Reinforcement Learning","summary":"  Conventional multi-label recognition methods often focus on label confidence,\nfrequently overlooking the pivotal role of partial order relations consistent\nwith human preference. To resolve these issues, we introduce a novel method for\nmultimodal label relevance ranking, named Label Relevance Ranking with Proximal\nPolicy Optimization (LR\\textsuperscript{2}PPO), which effectively discerns\npartial order relations among labels. LR\\textsuperscript{2}PPO first utilizes\npartial order pairs in the target domain to train a reward model, which aims to\ncapture human preference intrinsic to the specific scenario. Furthermore, we\nmeticulously design state representation and a policy loss tailored for ranking\ntasks, enabling LR\\textsuperscript{2}PPO to boost the performance of label\nrelevance ranking model and largely reduce the requirement of partial order\nannotation for transferring to new scenes. To assist in the evaluation of our\napproach and similar methods, we further propose a novel benchmark dataset,\nLRMovieNet, featuring multimodal labels and their corresponding partial order\ndata. Extensive experiments demonstrate that our LR\\textsuperscript{2}PPO\nalgorithm achieves state-of-the-art performance, proving its effectiveness in\naddressing the multimodal label relevance ranking problem. Codes and the\nproposed LRMovieNet dataset are publicly available at\n\\url{https://github.com/ChazzyGordon/LR2PPO}.\n","authors":["Taian Guo","Taolin Zhang","Haoqian Wu","Hanjun Li","Ruizhi Qiao","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2407.13221v1.pdf","comment":"Accepted to ECCV2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.13742v1","updated":"2024-07-18T17:48:46Z","published":"2024-07-18T17:48:46Z","title":"CellularLint: A Systematic Approach to Identify Inconsistent Behavior in\n  Cellular Network Specifications","summary":"  In recent years, there has been a growing focus on scrutinizing the security\nof cellular networks, often attributing security vulnerabilities to issues in\nthe underlying protocol design descriptions. These protocol design\nspecifications, typically extensive documents that are thousands of pages long,\ncan harbor inaccuracies, underspecifications, implicit assumptions, and\ninternal inconsistencies. In light of the evolving landscape, we introduce\nCellularLint--a semi-automatic framework for inconsistency detection within the\nstandards of 4G and 5G, capitalizing on a suite of natural language processing\ntechniques. Our proposed method uses a revamped few-shot learning mechanism on\ndomain-adapted large language models. Pre-trained on a vast corpus of cellular\nnetwork protocols, this method enables CellularLint to simultaneously detect\ninconsistencies at various levels of semantics and practical use cases. In\ndoing so, CellularLint significantly advances the automated analysis of\nprotocol specifications in a scalable fashion. In our investigation, we focused\non the Non-Access Stratum (NAS) and the security specifications of 4G and 5G\nnetworks, ultimately uncovering 157 inconsistencies with 82.67% accuracy. After\nverification of these inconsistencies on open-source implementations and 17\ncommercial devices, we confirm that they indeed have a substantial impact on\ndesign decisions, potentially leading to concerns related to privacy,\nintegrity, availability, and interoperability.\n","authors":["Mirza Masfiqur Rahman","Imtiaz Karim","Elisa Bertino"],"pdf_url":"https://arxiv.org/pdf/2407.13742v1.pdf","comment":"Accepted at USENIX Security 24"},{"id":"http://arxiv.org/abs/2407.13699v1","updated":"2024-07-18T17:00:53Z","published":"2024-07-18T17:00:53Z","title":"A Comprehensive Review of Recommender Systems: Transitioning from Theory\n  to Practice","summary":"  Recommender Systems (RS) play an integral role in enhancing user experiences\nby providing personalized item suggestions. This survey reviews the progress in\nRS inclusively from 2017 to 2024, effectively connecting theoretical advances\nwith practical applications. We explore the development from traditional RS\ntechniques like content-based and collaborative filtering to advanced methods\ninvolving deep learning, graph-based models, reinforcement learning, and large\nlanguage models. We also discuss specialized systems such as context-aware,\nreview-based, and fairness-aware RS. The primary goal of this survey is to\nbridge theory with practice. It addresses challenges across various sectors,\nincluding e-commerce, healthcare, and finance, emphasizing the need for\nscalable, real-time, and trustworthy solutions. Through this survey, we promote\nstronger partnerships between academic research and industry practices. The\ninsights offered by this survey aim to guide industry professionals in\noptimizing RS deployment and to inspire future research directions, especially\nin addressing emerging technological and societal trends\n","authors":["Shaina Raza","Mizanur Rahman","Safiullah Kamawal","Armin Toroghi","Ananya Raval","Farshad Navah","Amirmohammad Kazemeini"],"pdf_url":"https://arxiv.org/pdf/2407.13699v1.pdf","comment":"we quarterly update of this literature"},{"id":"http://arxiv.org/abs/2404.00972v2","updated":"2024-07-18T13:43:09Z","published":"2024-04-01T07:32:28Z","title":"Cross-channel Recommendation for Multi-channel Retail","summary":"  An increasing number of retailers are expanding their channels to the offline\nand online domains, transforming them into multi-channel retailers. This\ntransition emphasizes the need for cross-channel recommendations. Given that\neach retail channel represents a separate domain with a unique context, this\ncan be regarded as a cross-domain recommendation (CDR). However, existing\nstudies on CDR did not address the scenarios where both users and items\npartially overlap across multi-retail channels which we define as\n\"cross-channel retail recommendation (CCRR)\". This paper introduces our\noriginal work on CCRR using a real-world dataset from a multi-channel retail\nstore. Specifically, we study significant challenges in integrating user\npreferences across both channels and propose a novel model for CCRR using a\nchannel-wise attention mechanism. We empirically validate our model's\nsuperiority in addressing CCRR over existing models. Finally, we offer\nimplications for future research on CCRR, delving into our experiment results.\n","authors":["Yijin Choi","Jongkyung Shin","Chiehyeon Lim"],"pdf_url":"https://arxiv.org/pdf/2404.00972v2.pdf","comment":"5 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2312.16695v2","updated":"2024-07-18T13:02:16Z","published":"2023-12-27T19:24:26Z","title":"Performance Comparison of Session-based Recommendation Algorithms based\n  on GNNs","summary":"  In session-based recommendation settings, a recommender system has no access\nto long-term user profiles and thus has to base its suggestions on the user\ninteractions that are observed in an ongoing session. Since such sessions can\nconsist of only a small set of interactions, various approaches based on Graph\nNeural Networks (GNN) were recently proposed, as they allow us to integrate\nvarious types of side information about the items in a natural way.\nUnfortunately, a variety of evaluation settings are used in the literature,\ne.g., in terms of protocols, metrics and baselines, making it difficult to\nassess what represents the state of the art. In this work, we present the\nresults of an evaluation of eight recent GNN-based approaches that were\npublished in high-quality outlets. For a fair comparison, all models are\nsystematically tuned and tested under identical conditions using three common\ndatasets. We furthermore include k-nearest-neighbor and sequential rules-based\nmodels as baselines, as such models have previously exhibited competitive\nperformance results for similar settings. To our surprise, the evaluation\nshowed that the simple models outperform all recent GNN models in terms of the\nMean Reciprocal Rank, which we used as an optimization criterion, and were only\noutperformed in three cases in terms of the Hit Rate. Additional analyses\nfurthermore reveal that several other factors that are often not deeply\ndiscussed in papers, e.g., random seeds, can markedly impact the performance of\nGNN-based models. Our results therefore (a) point to continuing issues in the\ncommunity in terms of research methodology and (b) indicate that there is ample\nroom for improvement in session-based recommendation.\n","authors":["Faisal Shehzad","Dietmar Jannach"],"pdf_url":"https://arxiv.org/pdf/2312.16695v2.pdf","comment":"Accepted at ECIR 2024"},{"id":"http://arxiv.org/abs/2407.13416v1","updated":"2024-07-18T11:39:50Z","published":"2024-07-18T11:39:50Z","title":"The Language of Infographics: Toward Understanding Conceptual Metaphor\n  Use in Scientific Storytelling","summary":"  We apply an approach from cognitive linguistics by mapping Conceptual\nMetaphor Theory (CMT) to the visualization domain to address patterns of visual\nconceptual metaphors that are often used in science infographics. Metaphors\nplay an essential part in visual communication and are frequently employed to\nexplain complex concepts. However, their use is often based on intuition,\nrather than following a formal process. At present, we lack tools and language\nfor understanding and describing metaphor use in visualization to the extent\nwhere taxonomy and grammar could guide the creation of visual components, e.g.,\ninfographics. Our classification of the visual conceptual mappings within\nscientific representations is based on the breakdown of visual components in\nexisting scientific infographics. We demonstrate the development of this\nmapping through a detailed analysis of data collected from four domains\n(biomedicine, climate, space, and anthropology) that represent a diverse range\nof visual conceptual metaphors used in the visual communication of science.\nThis work allows us to identify patterns of visual conceptual metaphor use\nwithin the domains, resolve ambiguities about why specific conceptual metaphors\nare used, and develop a better overall understanding of visual metaphor use in\nscientific infographics. Our analysis shows that ontological and orientational\nconceptual metaphors are the most widely applied to translate complex\nscientific concepts. To support our findings we developed a visual exploratory\ntool based on the collected database that places the individual infographics on\na spatio-temporal scale and illustrates the breakdown of visual conceptual\nmetaphors.\n","authors":["Hana Pokojná","Tobias Isenberg","Stefan Bruckner","Barbora Kozlíková","Laura Garrison"],"pdf_url":"https://arxiv.org/pdf/2407.13416v1.pdf","comment":"11 pages, 8 figures, 1 table, accepted to IEEE VIS 2024 Conference"},{"id":"http://arxiv.org/abs/2407.12468v2","updated":"2024-07-18T10:11:09Z","published":"2024-07-17T10:40:39Z","title":"Search Engines, LLMs or Both? Evaluating Information Seeking Strategies\n  for Answering Health Questions","summary":"  Search engines have traditionally served as primary tools for information\nseeking. However, the new Large Language Models (LLMs) have recently\ndemonstrated remarkable capabilities in multiple tasks and, specifically, their\nadoption as question answering systems is becoming increasingly prevalent. It\nis expected that LLM-based conversational systems and traditional web engines\nwill continue to coexist in the future, supporting end users in various ways.\nBut there is a need for more scientific research on the effectiveness of both\ntypes of systems in facilitating accurate information seeking. In this study,\nwe focus on their merits in answering health questions. We conducted an\nextensive study comparing different web search engines, LLMs and\nretrieval-augmented (RAG) approaches. Our research reveals intriguing\nconclusions. For example, we observed that the quality of webpages potentially\nresponding to a health question does not decline as we navigate further down\nthe ranked lists. However, according to our evaluation, web engines are less\naccurate than LLMs in finding correct answers to health questions. On the other\nhand, LLMs are quite sensitive to the input prompts, and we also found out that\nRAG leads to highly effective information seeking methods.\n","authors":["Marcos Fernández-Pichel","Juan C. Pichel","David E. Losada"],"pdf_url":"https://arxiv.org/pdf/2407.12468v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13349v1","updated":"2024-07-18T09:49:13Z","published":"2024-07-18T09:49:13Z","title":"DCNv3: Towards Next Generation Deep Cross Network for CTR Prediction","summary":"  Deep & Cross Network and its derivative models have become an important\nparadigm in click-through rate (CTR) prediction due to their effective balance\nbetween computational cost and performance. However, these models face four\nmajor limitations: (1) while most models claim to capture high-order feature\ninteractions, they often do so implicitly and non-interpretably through deep\nneural networks (DNN), which limits the trustworthiness of the model's\npredictions; (2) the performance of existing explicit feature interaction\nmethods is often weaker than that of implicit DNN, undermining their necessity;\n(3) many models fail to adaptively filter noise while enhancing the order of\nfeature interactions; (4) the fusion methods of most models cannot provide\nsuitable supervision signals for their different interaction methods.\n  To address the identified limitations, this paper proposes the next\ngeneration Deep Cross Network (DCNv3) and Shallow & Deep Cross Network\n(SDCNv3). These models ensure interpretability in feature interaction modeling\nwhile exponentially increasing the order of feature interactions to achieve\ngenuine Deep Crossing rather than just Deep & Cross. Additionally, we employ a\nSelf-Mask operation to filter noise and reduce the number of parameters in the\ncross network by half. In the fusion layer, we use a simple yet effective loss\nweight calculation method called Tri-BCE to provide appropriate supervision\nsignals. Comprehensive experiments on six datasets demonstrate the\neffectiveness, efficiency, and interpretability of DCNv3 and SDCNv3. The code,\nrunning logs, and detailed hyperparameter configurations are available at:\nhttps://anonymous.4open.science/r/DCNv3-E352.\n","authors":["Honghao Li","Yiwen Zhang","Yi Zhang","Hanwei Li","Lei Sang"],"pdf_url":"https://arxiv.org/pdf/2407.13349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13284v1","updated":"2024-07-18T08:36:28Z","published":"2024-07-18T08:36:28Z","title":"Semantic-aware Representation Learning for Homography Estimation","summary":"  Homography estimation is the task of determining the transformation from an\nimage pair. Our approach focuses on employing detector-free feature matching\nmethods to address this issue. Previous work has underscored the importance of\nincorporating semantic information, however there still lacks an efficient way\nto utilize semantic information. Previous methods suffer from treating the\nsemantics as a pre-processing, causing the utilization of semantics overly\ncoarse-grained and lack adaptability when dealing with different tasks. In our\nwork, we seek another way to use the semantic information, that is\nsemantic-aware feature representation learning framework.Based on this, we\npropose SRMatcher, a new detector-free feature matching method, which\nencourages the network to learn integrated semantic feature\nrepresentation.Specifically, to capture precise and rich semantics, we leverage\nthe capabilities of recently popularized vision foundation models (VFMs)\ntrained on extensive datasets. Then, a cross-images Semantic-aware Fusion Block\n(SFB) is proposed to integrate its fine-grained semantic features into the\nfeature representation space. In this way, by reducing errors stemming from\nsemantic inconsistencies in matching pairs, our proposed SRMatcher is able to\ndeliver more accurate and realistic outcomes. Extensive experiments show that\nSRMatcher surpasses solid baselines and attains SOTA results on multiple\nreal-world datasets. Compared to the previous SOTA approach GeoFormer,\nSRMatcher increases the area under the cumulative curve (AUC) by about 11\\% on\nHPatches. Additionally, the SRMatcher could serve as a plug-and-play framework\nfor other matching methods like LoFTR, yielding substantial precision\nimprovement.\n","authors":["Yuhan Liu","Qianxin Huang","Siqi Hui","Jingwen Fu","Sanping Zhou","Kangyi Wu","Pengna Li","Jinjun Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13274v1","updated":"2024-07-18T08:29:55Z","published":"2024-07-18T08:29:55Z","title":"Aligning Explanations for Recommendation with Rating and Feature via\n  Maximizing Mutual Information","summary":"  Providing natural language-based explanations to justify recommendations\nhelps to improve users' satisfaction and gain users' trust. However, as current\nexplanation generation methods are commonly trained with an objective to mimic\nexisting user reviews, the generated explanations are often not aligned with\nthe predicted ratings or some important features of the recommended items, and\nthus, are suboptimal in helping users make informed decision on the\nrecommendation platform. To tackle this problem, we propose a flexible\nmodel-agnostic method named MMI (Maximizing Mutual Information) framework to\nenhance the alignment between the generated natural language explanations and\nthe predicted rating/important item features. Specifically, we propose to use\nmutual information (MI) as a measure for the alignment and train a neural MI\nestimator. Then, we treat a well-trained explanation generation model as the\nbackbone model and further fine-tune it through reinforcement learning with\nguidance from the MI estimator, which rewards a generated explanation that is\nmore aligned with the predicted rating or a pre-defined feature of the\nrecommended item. Experiments on three datasets demonstrate that our MMI\nframework can boost different backbone models, enabling them to outperform\nexisting baselines in terms of alignment with predicted ratings and item\nfeatures. Additionally, user studies verify that MI-enhanced explanations\nindeed facilitate users' decisions and are favorable compared with other\nbaselines due to their better alignment properties.\n","authors":["Yurou Zhao","Yiding Sun","Ruidong Han","Fei Jiang","Lu Guan","Xiang Li","Wei Lin","Jiaxin Mao"],"pdf_url":"https://arxiv.org/pdf/2407.13274v1.pdf","comment":"this paper has been accepted by cikm2024, and the camera-ready\n  version will be updated soon"},{"id":"http://arxiv.org/abs/2406.05666v5","updated":"2024-07-18T08:11:28Z","published":"2024-06-09T06:49:22Z","title":"General Distribution Learning: A theoretical framework for Deep Learning","summary":"  This paper introduces General Distribution Learning (GD learning), a novel\ntheoretical learning framework designed to address a comprehensive range of\nmachine learning and statistical tasks, including classification, regression,\nand parameter estimation. GD learning focuses on estimating the true underlying\nprobability distribution of dataset and using models to fit the estimated\nparameters of the distribution. The learning error in GD learning is thus\ndecomposed into two distinct categories: estimation error and fitting error.\nThe estimation error, which stems from the constraints of finite sampling,\nlimited prior knowledge, and the estimation algorithm's inherent limitations,\nquantifies the discrepancy between the true distribution and its estimate. The\nfitting error can be attributed to model's capacity limitation and the\nperformance limitation of the optimization algorithm, which evaluates the\ndeviation of the model output from the fitted objective. To address the\nchallenge of non-convexity in the optimization of learning error, we introduce\nthe standard loss function and demonstrate that, when employing this function,\nglobal optimal solutions in non-convex optimization can be approached by\nminimizing the gradient norm and the structural error. Moreover, we demonstrate\nthat the estimation error is determined by the uncertainty of the estimate $q$,\nand propose the minimum uncertainty principle to obtain an optimal estimate of\nthe true distribution. We further provide upper bounds for the estimation\nerror, fitting error, and learning error within the GD learning framework.\nUltimately, our findings are applied to offer theoretical explanations for\nseveral unanswered questions on deep learning, including overparameterization,\nnon-convex optimization, flat minima, dynamic isometry condition and other\ntechniques in deep learning.\n","authors":["Binchuan Qi"],"pdf_url":"https://arxiv.org/pdf/2406.05666v5.pdf","comment":"arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors"},{"id":"http://arxiv.org/abs/2407.13174v1","updated":"2024-07-18T05:28:50Z","published":"2024-07-18T05:28:50Z","title":"Compressed models are NOT miniature versions of large models","summary":"  Large neural models are often compressed before deployment. Model compression\nis necessary for many practical reasons, such as inference latency, memory\nfootprint, and energy consumption. Compressed models are assumed to be\nminiature versions of corresponding large neural models. However, we question\nthis belief in our work. We compare compressed models with corresponding large\nneural models using four model characteristics: prediction errors, data\nrepresentation, data distribution, and vulnerability to adversarial attack. We\nperform experiments using the BERT-large model and its five compressed\nversions. For all four model characteristics, compressed models significantly\ndiffer from the BERT-large model. Even among compressed models, they differ\nfrom each other on all four model characteristics. Apart from the expected loss\nin model performance, there are major side effects of using compressed models\nto replace large neural models.\n","authors":["Rohit Raj Rai","Rishant Pal","Amit Awekar"],"pdf_url":"https://arxiv.org/pdf/2407.13174v1.pdf","comment":"Accepted at the 33rd ACM International Conference on Information and\n  Knowledge Management (CIKM 2024) for the Short Research Paper track, 5 pages"},{"id":"http://arxiv.org/abs/2407.13166v1","updated":"2024-07-18T05:10:35Z","published":"2024-07-18T05:10:35Z","title":"Using LLMs to Investigate Correlations of Conversational Follow-up\n  Queries with User Satisfaction","summary":"  With large language models (LLMs), conversational search engines shift how\nusers retrieve information from the web by enabling natural conversations to\nexpress their search intents over multiple turns. Users' natural conversation\nembodies rich but implicit signals of users' search intents and evaluation of\nsearch results to understand user experience with the system. However, it is\nunderexplored how and why users ask follow-up queries to continue conversations\nwith conversational search engines and how the follow-up queries signal users'\nsatisfaction. From qualitative analysis of 250 conversational turns from an\nin-lab user evaluation of Naver Cue:, a commercial conversational search\nengine, we propose a taxonomy of 18 users' follow-up query patterns from\nconversational search, comprising two major axes: (1) users' motivations behind\ncontinuing conversations (N = 7) and (2) actions of follow-up queries (N = 11).\nCompared to the existing literature on query reformulations, we uncovered a new\nset of motivations and actions behind follow-up queries, including asking for\nsubjective opinions or providing natural language feedback on the engine's\nresponses. To analyze conversational search logs with our taxonomy in a\nscalable and efficient manner, we built an LLM-powered classifier (73%\naccuracy). With our classifier, we analyzed 2,061 conversational tuples\ncollected from real-world usage logs of Cue: and examined how the conversation\npatterns from our taxonomy correlates with satisfaction. Our initial findings\nsuggest some signals of dissatisfactions, such as Clarifying Queries, Excluding\nCondition, and Substituting Condition with follow-up queries. We envision our\napproach could contribute to automated evaluation of conversation search\nexperience by providing satisfaction signals and grounds for realistic user\nsimulations.\n","authors":["Hyunwoo Kim","Yoonseo Choi","Taehyun Yang","Honggu Lee","Chaneon Park","Yongju Lee","Jin Young Kim","Juho Kim"],"pdf_url":"https://arxiv.org/pdf/2407.13166v1.pdf","comment":"Accepted to LLM4Eval @ SIGIR 2024 - The First Workshop on Large\n  Language Models (LLMs) for Evaluation in Information Retrieval"},{"id":"http://arxiv.org/abs/2407.13163v1","updated":"2024-07-18T05:07:11Z","published":"2024-07-18T05:07:11Z","title":"ROLeR: Effective Reward Shaping in Offline Reinforcement Learning for\n  Recommender Systems","summary":"  Offline reinforcement learning (RL) is an effective tool for real-world\nrecommender systems with its capacity to model the dynamic interest of users\nand its interactive nature. Most existing offline RL recommender systems focus\non model-based RL through learning a world model from offline data and building\nthe recommendation policy by interacting with this model. Although these\nmethods have made progress in the recommendation performance, the effectiveness\nof model-based offline RL methods is often constrained by the accuracy of the\nestimation of the reward model and the model uncertainties, primarily due to\nthe extreme discrepancy between offline logged data and real-world data in user\ninteractions with online platforms. To fill this gap, a more accurate reward\nmodel and uncertainty estimation are needed for the model-based RL methods. In\nthis paper, a novel model-based Reward Shaping in Offline Reinforcement\nLearning for Recommender Systems, ROLeR, is proposed for reward and uncertainty\nestimation in recommendation systems. Specifically, a non-parametric reward\nshaping method is designed to refine the reward model. In addition, a flexible\nand more representative uncertainty penalty is designed to fit the needs of\nrecommendation systems. Extensive experiments conducted on four benchmark\ndatasets showcase that ROLeR achieves state-of-the-art performance compared\nwith existing baselines. The source code can be downloaded at\nhttps://github.com/ArronDZhang/ROLeR.\n","authors":["Yi Zhang","Ruihong Qiu","Jiajun Liu","Sen Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13163v1.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2407.13135v1","updated":"2024-07-18T03:46:21Z","published":"2024-07-18T03:46:21Z","title":"MLSA4Rec: Mamba Combined with Low-Rank Decomposed Self-Attention for\n  Sequential Recommendation","summary":"  In applications such as e-commerce, online education, and streaming services,\nsequential recommendation systems play a critical role. Despite the excellent\nperformance of self-attention-based sequential recommendation models in\ncapturing dependencies between items in user interaction history, their\nquadratic complexity and lack of structural bias limit their applicability.\nRecently, some works have replaced the self-attention module in sequential\nrecommenders with Mamba, which has linear complexity and structural bias.\nHowever, these works have not noted the complementarity between the two\napproaches. To address this issue, this paper proposes a new hybrid\nrecommendation framework, Mamba combined with Low-Rank decomposed\nSelf-Attention for Sequential Recommendation (MLSA4Rec), whose complexity is\nlinear with respect to the length of the user's historical interaction\nsequence. Specifically, MLSA4Rec designs an efficient Mamba-LSA interaction\nmodule. This module introduces a low-rank decomposed self-attention (LSA)\nmodule with linear complexity and injects structural bias into it through\nMamba. The LSA module analyzes user preferences from a different perspective\nand dynamically guides Mamba to focus on important information in user\nhistorical interactions through a gated information transmission mechanism.\nFinally, MLSA4Rec combines user preference information refined by the Mamba and\nLSA modules to accurately predict the user's next possible interaction. To our\nknowledge, this is the first study to combine Mamba and self-attention in\nsequential recommendation systems. Experimental results show that MLSA4Rec\noutperforms existing self-attention and Mamba-based sequential recommendation\nmodels in recommendation accuracy on three real-world datasets, demonstrating\nthe great potential of Mamba and self-attention working together.\n","authors":["Jinzhao Su","Zhenhua Huang"],"pdf_url":"https://arxiv.org/pdf/2407.13135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13091v1","updated":"2024-07-18T01:41:05Z","published":"2024-07-18T01:41:05Z","title":"On Causally Disentangled State Representation Learning for Reinforcement\n  Learning based Recommender Systems","summary":"  In Reinforcement Learning-based Recommender Systems (RLRS), the complexity\nand dynamism of user interactions often result in high-dimensional and noisy\nstate spaces, making it challenging to discern which aspects of the state are\ntruly influential in driving the decision-making process. This issue is\nexacerbated by the evolving nature of user preferences and behaviors, requiring\nthe recommender system to adaptively focus on the most relevant information for\ndecision-making while preserving generaliability. To tackle this problem, we\nintroduce an innovative causal approach for decomposing the state and\nextracting \\textbf{C}ausal-\\textbf{I}n\\textbf{D}ispensable \\textbf{S}tate\nRepresentations (CIDS) in RLRS. Our method concentrates on identifying the\n\\textbf{D}irectly \\textbf{A}ction-\\textbf{I}nfluenced \\textbf{S}tate Variables\n(DAIS) and \\textbf{A}ction-\\textbf{I}nfluence \\textbf{A}ncestors (AIA), which\nare essential for making effective recommendations. By leveraging conditional\nmutual information, we develop a framework that not only discerns the causal\nrelationships within the generative process but also isolates critical state\nvariables from the typically dense and high-dimensional state representations.\nWe provide theoretical evidence for the identifiability of these variables.\nThen, by making use of the identified causal relationship, we construct\ncausal-indispensable state representations, enabling the training of policies\nover a more advantageous subset of the agent's state space. We demonstrate the\nefficacy of our approach through extensive experiments, showcasing our method\noutperforms state-of-the-art methods.\n","authors":["Siyu Wang","Xiaocong Chen","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2407.13091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13069v1","updated":"2024-07-18T00:28:04Z","published":"2024-07-18T00:28:04Z","title":"Dynamic Sentiment Analysis with Local Large Language Models using\n  Majority Voting: A Study on Factors Affecting Restaurant Evaluation","summary":"  User-generated contents (UGCs) on online platforms allow marketing\nresearchers to understand consumer preferences for products and services. With\nthe advance of large language models (LLMs), some studies utilized the models\nfor annotation and sentiment analysis. However, the relationship between the\naccuracy and the hyper-parameters of LLMs is yet to be thoroughly examined. In\naddition, the issues of variability and reproducibility of results from each\ntrial of LLMs have rarely been considered in existing literature. Since actual\nhuman annotation uses majority voting to resolve disagreements among\nannotators, this study introduces a majority voting mechanism to a sentiment\nanalysis model using local LLMs. By a series of three analyses of online\nreviews on restaurant evaluations, we demonstrate that majority voting with\nmultiple attempts using a medium-sized model produces more robust results than\nusing a large model with a single attempt. Furthermore, we conducted further\nanalysis to investigate the effect of each aspect on the overall evaluation.\n","authors":["Junichiro Niimi"],"pdf_url":"https://arxiv.org/pdf/2407.13069v1.pdf","comment":"This manuscript is under peer review"},{"id":"http://arxiv.org/abs/2407.09017v2","updated":"2024-07-18T00:18:19Z","published":"2024-07-12T06:10:01Z","title":"AI-Driven Guided Response for Security Operation Centers with Microsoft\n  Copilot for Security","summary":"  Security operation centers contend with a constant stream of security\nincidents, ranging from straightforward to highly complex. To address this, we\ndeveloped Copilot Guided Response (CGR), an industry-scale ML architecture that\nguides security analysts across three key tasks -- (1) investigation, providing\nessential historical context by identifying similar incidents; (2) triaging to\nascertain the nature of the incident -- whether it is a true positive, false\npositive, or benign positive; and (3) remediation, recommending tailored\ncontainment actions. CGR is integrated into the Microsoft Defender XDR product\nand deployed worldwide, generating millions of recommendations across thousands\nof customers. Our extensive evaluation, incorporating internal evaluation,\ncollaboration with security experts, and customer feedback, demonstrates that\nCGR delivers high-quality recommendations across all three tasks. We provide a\ncomprehensive overview of the CGR architecture, setting a precedent as the\nfirst cybersecurity company to openly discuss these capabilities in such depth.\nAdditionally, we GUIDE, the largest public collection of real-world security\nincidents, spanning 13M evidences across 1M annotated incidents. By enabling\nresearchers and practitioners to conduct research on real-world data, GUIDE\nadvances the state of cybersecurity and supports the development of\nnext-generation machine learning systems.\n","authors":["Scott Freitas","Jovan Kalajdjieski","Amir Gharib","Robert McCann"],"pdf_url":"https://arxiv.org/pdf/2407.09017v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2404.01300v3","updated":"2024-07-18T17:59:48Z","published":"2024-04-01T17:59:55Z","title":"NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation\n  Learning for Neural Radiance Fields","summary":"  Neural fields excel in computer vision and robotics due to their ability to\nunderstand the 3D visual world such as inferring semantics, geometry, and\ndynamics. Given the capabilities of neural fields in densely representing a 3D\nscene from 2D images, we ask the question: Can we scale their self-supervised\npretraining, specifically using masked autoencoders, to generate effective 3D\nrepresentations from posed RGB images. Owing to the astounding success of\nextending transformers to novel data modalities, we employ standard 3D Vision\nTransformers to suit the unique formulation of NeRFs. We leverage NeRF's\nvolumetric grid as a dense input to the transformer, contrasting it with other\n3D representations such as pointclouds where the information density can be\nuneven, and the representation is irregular. Due to the difficulty of applying\nmasked autoencoders to an implicit representation, such as NeRF, we opt for\nextracting an explicit representation that canonicalizes scenes across domains\nby employing the camera trajectory for sampling. Our goal is made possible by\nmasking random patches from NeRF's radiance and density grid and employing a\nstandard 3D Swin Transformer to reconstruct the masked patches. In doing so,\nthe model can learn the semantic and spatial structure of complete scenes. We\npretrain this representation at scale on our proposed curated posed-RGB data,\ntotaling over 1.8 million images. Once pretrained, the encoder is used for\neffective 3D transfer learning. Our novel self-supervised pretraining for\nNeRFs, NeRF-MAE, scales remarkably well and improves performance on various\nchallenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,\nNeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF\nscene understanding baselines on Front3D and ScanNet datasets with an absolute\nperformance improvement of over 20% AP50 and 8% AP25 for 3D object detection.\n","authors":["Muhammad Zubair Irshad","Sergey Zakharov","Vitor Guizilini","Adrien Gaidon","Zsolt Kira","Rares Ambrus"],"pdf_url":"https://arxiv.org/pdf/2404.01300v3.pdf","comment":"Accepted to ECCV 2024. Project Page: https://nerf-mae.github.io/"},{"id":"http://arxiv.org/abs/2403.09635v2","updated":"2024-07-18T17:59:35Z","published":"2024-03-14T17:59:14Z","title":"Transformers Get Stable: An End-to-End Signal Propagation Theory for\n  Language Models","summary":"  In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 1000\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across encoder-only, decoder-only and\nencoder-decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for Image Classification.\n","authors":["Akhil Kedia","Mohd Abbas Zaidi","Sushil Khyalia","Jungho Jung","Harshith Goka","Haejun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.09635v2.pdf","comment":"Accepted at ICML 2024. Source code is available at\n  https://github.com/akhilkedia/TranformersGetStable. Akhil Kedia, Mohd Abbas\n  Zaidi, Sushil Khyalia equal contribution"},{"id":"http://arxiv.org/abs/2403.07657v2","updated":"2024-07-18T17:58:42Z","published":"2024-03-12T13:47:50Z","title":"Scalable Spatiotemporal Prediction with Bayesian Neural Fields","summary":"  Spatiotemporal datasets, which consist of spatially-referenced time series,\nare ubiquitous in many scientific and business-intelligence applications, such\nas air pollution monitoring, disease tracking, and cloud-demand forecasting. As\nmodern datasets continue to increase in size and complexity, there is a growing\nneed for new statistical methods that are flexible enough to capture complex\nspatiotemporal dynamics and scalable enough to handle large prediction\nproblems. This work presents the Bayesian Neural Field (BayesNF), a\ndomain-general statistical model for inferring rich probability distributions\nover a spatiotemporal domain, which can be used for data-analysis tasks\nincluding forecasting, interpolation, and variography. BayesNF integrates a\nnovel deep neural network architecture for high-capacity function estimation\nwith hierarchical Bayesian inference for robust uncertainty quantification. By\ndefining the prior through a sequence of smooth differentiable transforms,\nposterior inference is conducted on large-scale data using variationally\nlearned surrogates trained via stochastic gradient descent. We evaluate BayesNF\nagainst prominent statistical and machine-learning baselines, showing\nconsiderable improvements on diverse prediction problems from climate and\npublic health datasets that contain tens to hundreds of thousands of\nmeasurements. The paper is accompanied with an open-source software package\n(https://github.com/google/bayesnf) that is easy-to-use and compatible with\nmodern GPU and TPU accelerators on the JAX machine learning platform.\n","authors":["Feras Saad","Jacob Burnim","Colin Carroll","Brian Patton","Urs Köster","Rif A. Saurous","Matthew Hoffman"],"pdf_url":"https://arxiv.org/pdf/2403.07657v2.pdf","comment":"27 pages, 7 figures, 3 tables, 2 listings"},{"id":"http://arxiv.org/abs/2407.13755v1","updated":"2024-07-18T17:55:22Z","published":"2024-07-18T17:55:22Z","title":"Random Latent Exploration for Deep Reinforcement Learning","summary":"  The ability to efficiently explore high-dimensional state spaces is essential\nfor the practical success of deep Reinforcement Learning (RL). This paper\nintroduces a new exploration technique called Random Latent Exploration (RLE),\nthat combines the strengths of bonus-based and noise-based (two popular\napproaches for effective exploration in deep RL) exploration strategies. RLE\nleverages the idea of perturbing rewards by adding structured random rewards to\nthe original task rewards in certain (random) states of the environment, to\nencourage the agent to explore the environment during training. RLE is\nstraightforward to implement and performs well in practice. To demonstrate the\npractical effectiveness of RLE, we evaluate it on the challenging Atari and\nIsaacGym benchmarks and show that RLE exhibits higher overall scores across all\nthe tasks than other approaches.\n","authors":["Srinath Mahankali","Zhang-Wei Hong","Ayush Sekhari","Alexander Rakhlin","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.13755v1.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2407.13746v1","updated":"2024-07-18T17:51:02Z","published":"2024-07-18T17:51:02Z","title":"Multi-Label Learning with Stronger Consistency Guarantees","summary":"  We present a detailed study of surrogate losses and algorithms for\nmulti-label learning, supported by $H$-consistency bounds. We first show that,\nfor the simplest form of multi-label loss (the popular Hamming loss), the\nwell-known consistent binary relevance surrogate suffers from a sub-optimal\ndependency on the number of labels in terms of $H$-consistency bounds, when\nusing smooth losses such as logistic losses. Furthermore, this loss function\nfails to account for label correlations. To address these drawbacks, we\nintroduce a novel surrogate loss, multi-label logistic loss, that accounts for\nlabel correlations and benefits from label-independent $H$-consistency bounds.\nWe then broaden our analysis to cover a more extensive family of multi-label\nlosses, including all common ones and a new extension defined based on\nlinear-fractional functions with respect to the confusion matrix. We also\nextend our multi-label logistic losses to more comprehensive multi-label\ncomp-sum losses, adapting comp-sum losses from standard classification to the\nmulti-label learning. We prove that this family of surrogate losses benefits\nfrom $H$-consistency bounds, and thus Bayes-consistency, across any general\nmulti-label loss. Our work thus proposes a unified surrogate loss framework\nbenefiting from strong consistency guarantees for any multi-label loss,\nsignificantly expanding upon previous work which only established\nBayes-consistency and for specific loss functions. Additionally, we adapt\nconstrained losses from standard classification to multi-label constrained\nlosses in a similar way, which also benefit from $H$-consistency bounds and\nthus Bayes-consistency for any multi-label loss. We further describe efficient\ngradient computation algorithms for minimizing the multi-label logistic loss.\n","authors":["Anqi Mao","Mehryar Mohri","Yutao Zhong"],"pdf_url":"https://arxiv.org/pdf/2407.13746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13743v1","updated":"2024-07-18T17:49:09Z","published":"2024-07-18T17:49:09Z","title":"Optimistic Q-learning for average reward and episodic reinforcement\n  learning","summary":"  We present an optimistic Q-learning algorithm for regret minimization in\naverage reward reinforcement learning under an additional assumption on the\nunderlying MDP that for all policies, the expected time to visit some frequent\nstate $s_0$ is finite and upper bounded by $H$. Our setting strictly\ngeneralizes the episodic setting and is significantly less restrictive than the\nassumption of bounded hitting time {\\it for all states} made by most previous\nliterature on model-free algorithms in average reward settings. We demonstrate\na regret bound of $\\tilde{O}(H^5 S\\sqrt{AT})$, where $S$ and $A$ are the\nnumbers of states and actions, and $T$ is the horizon. A key technical novelty\nof our work is to introduce an $\\overline{L}$ operator defined as $\\overline{L}\nv = \\frac{1}{H} \\sum_{h=1}^H L^h v$ where $L$ denotes the Bellman operator. We\nshow that under the given assumption, the $\\overline{L}$ operator has a strict\ncontraction (in span) even in the average reward setting. Our algorithm design\nthen uses ideas from episodic Q-learning to estimate and apply this operator\niteratively. Therefore, we provide a unified view of regret minimization in\nepisodic and non-episodic settings that may be of independent interest.\n","authors":["Priyank Agrawal","Shipra Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.13743v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2407.13687v1","updated":"2024-07-18T17:42:37Z","published":"2024-07-18T17:42:37Z","title":"Dynamic Pricing in Securities Lending Market: Application in Revenue\n  Optimization for an Agent Lender Portfolio","summary":"  Securities lending is an important part of the financial market structure,\nwhere agent lenders help long term institutional investors to lend out their\nsecurities to short sellers in exchange for a lending fee. Agent lenders within\nthe market seek to optimize revenue by lending out securities at the highest\nrate possible. Typically, this rate is set by hard-coded business rules or\nstandard supervised machine learning models. These approaches are often\ndifficult to scale and are not adaptive to changing market conditions. Unlike a\ntraditional stock exchange with a centralized limit order book, the securities\nlending market is organized similarly to an e-commerce marketplace, where agent\nlenders and borrowers can transact at any agreed price in a bilateral fashion.\nThis similarity suggests that the use of typical methods for addressing dynamic\npricing problems in e-commerce could be effective in the securities lending\nmarket. We show that existing contextual bandit frameworks can be successfully\nutilized in the securities lending market. Using offline evaluation on real\nhistorical data, we show that the contextual bandit approach can consistently\noutperform typical approaches by at least 15% in terms of total revenue\ngenerated.\n","authors":["Jing Xu","Yung Cheng Hsu","William Biscarri"],"pdf_url":"https://arxiv.org/pdf/2407.13687v1.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.00035v2","updated":"2024-07-18T17:41:28Z","published":"2023-11-21T01:36:35Z","title":"FBChain: A Blockchain-based Federated Learning Model with Efficiency and\n  Secure Communication","summary":"  Privacy and security in the parameter transmission process of federated\nlearning are currently among the most prominent concerns. However, there are\ntwo thorny problems caused by unprotected communication methods:\n\"parameter-leakage\" and \"inefficient-communication\". This article proposes\nBlockchain-based Federated Learning (FBChain) model for federated learning\nparameter communication to overcome the above two problems. First, we utilize\nthe immutability of blockchain to store the global model and hash value of\nlocal model parameters in case of tampering during the communication process,\nprotect data privacy by encrypting parameters, and verify data consistency by\ncomparing the hash values of local parameters, thus addressing the\n\"parameter-leakage\" problem. Second, the Proof of Weighted Link Speed (PoWLS)\nconsensus algorithm comprehensively selects nodes with the higher weighted link\nspeed to aggregate global model and package blocks, thereby solving the\n\"inefficient-communication\" problem. Experimental results demonstrate the\neffectiveness of our proposed FBChain model and its ability to improve model\ncommunication efficiency in federated learning.\n","authors":["Yang Li","Chunhe Xia","Wei Liu","Chen Chen","Tianbo Wang"],"pdf_url":"https://arxiv.org/pdf/2312.00035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09327v2","updated":"2024-07-18T17:37:59Z","published":"2024-02-14T17:17:30Z","title":"Information Complexity of Stochastic Convex Optimization: Applications\n  to Generalization and Memorization","summary":"  In this work, we investigate the interplay between memorization and learning\nin the context of \\emph{stochastic convex optimization} (SCO). We define\nmemorization via the information a learning algorithm reveals about its\ntraining data points. We then quantify this information using the framework of\nconditional mutual information (CMI) proposed by Steinke and Zakynthinou\n(2020). Our main result is a precise characterization of the tradeoff between\nthe accuracy of a learning algorithm and its CMI, answering an open question\nposed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded setting\nand under strong convexity, every learner with an excess error $\\varepsilon$\nhas CMI bounded below by $\\Omega(1/\\varepsilon^2)$ and $\\Omega(1/\\varepsilon)$,\nrespectively. We further demonstrate the essential role of memorization in\nlearning problems in SCO by designing an adversary capable of accurately\nidentifying a significant fraction of the training samples in specific SCO\nproblems. Finally, we enumerate several implications of our results, such as a\nlimitation of generalization bounds based on CMI and the incompressibility of\nsamples in SCO problems.\n","authors":["Idan Attias","Gintare Karolina Dziugaite","Mahdi Haghifam","Roi Livni","Daniel M. Roy"],"pdf_url":"https://arxiv.org/pdf/2402.09327v2.pdf","comment":"41 Pages, To appear in ICML 2024"},{"id":"http://arxiv.org/abs/2407.13734v1","updated":"2024-07-18T17:35:32Z","published":"2024-07-18T17:35:32Z","title":"Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion\n  Models: A Tutorial and Review","summary":"  This tutorial provides a comprehensive survey of methods for fine-tuning\ndiffusion models to optimize downstream reward functions. While diffusion\nmodels are widely known to provide excellent generative modeling capability,\npractical applications in domains such as biology require generating samples\nthat maximize some desired metric (e.g., translation efficiency in RNA, docking\nscore in molecules, stability in protein). In these cases, the diffusion model\ncan be optimized not only to generate realistic samples but also to explicitly\nmaximize the measure of interest. Such methods are based on concepts from\nreinforcement learning (RL). We explain the application of various RL\nalgorithms, including PPO, differentiable optimization, reward-weighted MLE,\nvalue-weighted sampling, and path consistency learning, tailored specifically\nfor fine-tuning diffusion models. We aim to explore fundamental aspects such as\nthe strengths and limitations of different RL-based fine-tuning algorithms\nacross various scenarios, the benefits of RL-based fine-tuning compared to\nnon-RL-based approaches, and the formal objectives of RL-based fine-tuning\n(target distributions). Additionally, we aim to examine their connections with\nrelated topics such as classifier guidance, Gflownets, flow-based diffusion\nmodels, path integral control theory, and sampling from unnormalized\ndistributions such as MCMC. The code of this tutorial is available at\nhttps://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq\n","authors":["Masatoshi Uehara","Yulai Zhao","Tommaso Biancalani","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2407.13734v1.pdf","comment":"We plan to add more content/codes. Please let us know if there are\n  any comments"},{"id":"http://arxiv.org/abs/2407.13732v1","updated":"2024-07-18T17:35:03Z","published":"2024-07-18T17:35:03Z","title":"Realizable $H$-Consistent and Bayes-Consistent Loss Functions for\n  Learning to Defer","summary":"  We present a comprehensive study of surrogate loss functions for learning to\ndefer. We introduce a broad family of surrogate losses, parameterized by a\nnon-increasing function $\\Psi$, and establish their realizable $H$-consistency\nunder mild conditions. For cost functions based on classification error, we\nfurther show that these losses admit $H$-consistency bounds when the hypothesis\nset is symmetric and complete, a property satisfied by common neural network\nand linear function hypothesis sets. Our results also resolve an open question\nraised in previous work (Mozannar et al., 2023) by proving the realizable\n$H$-consistency and Bayes-consistency of a specific surrogate loss.\nFurthermore, we identify choices of $\\Psi$ that lead to $H$-consistent\nsurrogate losses for any general cost function, thus achieving\nBayes-consistency, realizable $H$-consistency, and $H$-consistency bounds\nsimultaneously. We also investigate the relationship between $H$-consistency\nbounds and realizable $H$-consistency in learning to defer, highlighting key\ndifferences from standard classification. Finally, we empirically evaluate our\nproposed surrogate losses and compare them with existing baselines.\n","authors":["Anqi Mao","Mehryar Mohri","Yutao Zhong"],"pdf_url":"https://arxiv.org/pdf/2407.13732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13731v1","updated":"2024-07-18T17:33:14Z","published":"2024-07-18T17:33:14Z","title":"Predictive Low Rank Matrix Learning under Partial Observations:\n  Mixed-Projection ADMM","summary":"  We study the problem of learning a partially observed matrix under the low\nrank assumption in the presence of fully observed side information that depends\nlinearly on the true underlying matrix. This problem consists of an important\ngeneralization of the Matrix Completion problem, a central problem in\nStatistics, Operations Research and Machine Learning, that arises in\napplications such as recommendation systems, signal processing, system\nidentification and image denoising. We formalize this problem as an\noptimization problem with an objective that balances the strength of the fit of\nthe reconstruction to the observed entries with the ability of the\nreconstruction to be predictive of the side information. We derive a\nmixed-projection reformulation of the resulting optimization problem and\npresent a strong semidefinite cone relaxation. We design an efficient, scalable\nalternating direction method of multipliers algorithm that produces high\nquality feasible solutions to the problem of interest. Our numerical results\ndemonstrate that in the small rank regime ($k \\leq 15$), our algorithm outputs\nsolutions that achieve on average $79\\%$ lower objective value and $90.1\\%$\nlower $\\ell_2$ reconstruction error than the solutions returned by the\nexperiment-wise best performing benchmark method. The runtime of our algorithm\nis competitive with and often superior to that of the benchmark methods. Our\nalgorithm is able to solve problems with $n = 10000$ rows and $m = 10000$\ncolumns in less than a minute.\n","authors":["Dimitris Bertsimas","Nicholas A. G. Johnson"],"pdf_url":"https://arxiv.org/pdf/2407.13731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11138v2","updated":"2024-07-18T17:31:57Z","published":"2024-07-15T18:03:42Z","title":"Lessons from a human-in-the-loop machine learning approach for\n  identifying vacant, abandoned, and deteriorated properties in Savannah,\n  Georgia","summary":"  Addressing strategies for managing vacant, abandoned, and deteriorated (VAD)\nproperties is important for maintaining healthy communities. Yet, the process\nof identifying these properties can be difficult. Here, we create a\nhuman-in-the-loop machine learning (HITLML) model called VADecide and apply it\nto a parcel-level case study in Savannah, Georgia. The results show a higher\nprediction accuracy than was achieved when using a machine learning model\nwithout human input in the training. The HITLML approach also reveals\ndifferences between machine vs. human-generated results. Our findings\ncontribute to knowledge about the advantages and challenges of HITLML in urban\nplanning.\n  [Accepted for Publication at a Peer Review Journal]\n","authors":["Xiaofan Liang","Brian Brainerd","Tara Hicks","Clio Andris"],"pdf_url":"https://arxiv.org/pdf/2407.11138v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09648v2","updated":"2024-07-18T17:31:20Z","published":"2024-06-14T00:40:31Z","title":"An Intrinsic Vector Heat Network","summary":"  Vector fields are widely used to represent and model flows for many science\nand engineering applications. This paper introduces a novel neural network\narchitecture for learning tangent vector fields that are intrinsically defined\non manifold surfaces embedded in 3D. Previous approaches to learning vector\nfields on surfaces treat vectors as multi-dimensional scalar fields, using\ntraditional scalar-valued architectures to process channels individually, thus\nfail to preserve fundamental intrinsic properties of the vector field. The core\nidea of this work is to introduce a trainable vector heat diffusion module to\nspatially propagate vector-valued feature data across the surface, which we\nincorporate into our proposed architecture that consists of vector-valued\nneurons. Our architecture is invariant to rigid motion of the input, isometric\ndeformation, and choice of local tangent bases, and is robust to\ndiscretizations of the surface. We evaluate our Vector Heat Network on triangle\nmeshes, and empirically validate its invariant properties. We also demonstrate\nthe effectiveness of our method on the useful industrial application of\nquadrilateral mesh generation.\n","authors":["Alexander Gao","Maurice Chu","Mubbasir Kapadia","Ming C. Lin","Hsueh-Ti Derek Liu"],"pdf_url":"https://arxiv.org/pdf/2406.09648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13726v1","updated":"2024-07-18T17:25:17Z","published":"2024-07-18T17:25:17Z","title":"Compressing Structured Tensor Algebra","summary":"  Tensor algebra is a crucial component for data-intensive workloads such as\nmachine learning and scientific computing. As the complexity of data grows,\nscientists often encounter a dilemma between the highly specialized dense\ntensor algebra and efficient structure-aware algorithms provided by sparse\ntensor algebra. In this paper, we introduce DASTAC, a framework to propagate\nthe tensors's captured high-level structure down to low-level code generation\nby incorporating techniques such as automatic data layout compression,\npolyhedral analysis, and affine code generation. Our methodology reduces memory\nfootprint by automatically detecting the best data layout, heavily benefits\nfrom polyhedral optimizations, leverages further optimizations, and enables\nparallelization through MLIR. Through extensive experimentation, we show that\nDASTAC achieves 1 to 2 orders of magnitude speedup over TACO, a\nstate-of-the-art sparse tensor compiler, and StructTensor, a state-of-the-art\nstructured tensor algebra compiler, with a significantly lower memory\nfootprint.\n","authors":["Mahdi Ghorbani","Emilien Bauer","Tobias Grosser","Amir Shaikhha"],"pdf_url":"https://arxiv.org/pdf/2407.13726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13722v1","updated":"2024-07-18T17:22:40Z","published":"2024-07-18T17:22:40Z","title":"Enhanced $H$-Consistency Bounds","summary":"  Recent research has introduced a key notion of $H$-consistency bounds for\nsurrogate losses. These bounds offer finite-sample guarantees, quantifying the\nrelationship between the zero-one estimation error (or other target loss) and\nthe surrogate loss estimation error for a specific hypothesis set. However,\nprevious bounds were derived under the condition that a lower bound of the\nsurrogate loss conditional regret is given as a convex function of the target\nconditional regret, without non-constant factors depending on the predictor or\ninput instance. Can we derive finer and more favorable $H$-consistency bounds?\nIn this work, we relax this condition and present a general framework for\nestablishing enhanced $H$-consistency bounds based on more general inequalities\nrelating conditional regrets. Our theorems not only subsume existing results as\nspecial cases but also enable the derivation of more favorable bounds in\nvarious scenarios. These include standard multi-class classification, binary\nand multi-class classification under Tsybakov noise conditions, and bipartite\nranking.\n","authors":["Anqi Mao","Mehryar Mohri","Yutao Zhong"],"pdf_url":"https://arxiv.org/pdf/2407.13722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13715v1","updated":"2024-07-18T17:11:29Z","published":"2024-07-18T17:11:29Z","title":"Attention Based Simple Primitives for Open World Compositional Zero-Shot\n  Learning","summary":"  Compositional Zero-Shot Learning (CZSL) aims to predict unknown compositions\nmade up of attribute and object pairs. Predicting compositions unseen during\ntraining is a challenging task. We are exploring Open World Compositional\nZero-Shot Learning (OW-CZSL) in this study, where our test space encompasses\nall potential combinations of attributes and objects. Our approach involves\nutilizing the self-attention mechanism between attributes and objects to\nachieve better generalization from seen to unseen compositions. Utilizing a\nself-attention mechanism facilitates the model's ability to identify\nrelationships between attribute and objects. The similarity between the\nself-attended textual and visual features is subsequently calculated to\ngenerate predictions during the inference phase. The potential test space may\nencompass implausible object-attribute combinations arising from unrestricted\nattribute-object pairings. To mitigate this issue, we leverage external\nknowledge from ConceptNet to restrict the test space to realistic compositions.\nOur proposed model, Attention-based Simple Primitives (ASP), demonstrates\ncompetitive performance, achieving results comparable to the state-of-the-art.\n","authors":["Ans Munir","Faisal Z. Qureshi","Muhammad Haris Khan","Mohsen Ali"],"pdf_url":"https://arxiv.org/pdf/2407.13715v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.13711v1","updated":"2024-07-18T17:08:58Z","published":"2024-07-18T17:08:58Z","title":"FSP-Laplace: Function-Space Priors for the Laplace Approximation in\n  Bayesian Deep Learning","summary":"  Laplace approximations are popular techniques for endowing deep networks with\nepistemic uncertainty estimates as they can be applied without altering the\npredictions of the neural network, and they scale to large models and datasets.\nWhile the choice of prior strongly affects the resulting posterior\ndistribution, computational tractability and lack of interpretability of weight\nspace typically limit the Laplace approximation to isotropic Gaussian priors,\nwhich are known to cause pathological behavior as depth increases. As a remedy,\nwe directly place a prior on function space. More precisely, since Lebesgue\ndensities do not exist on infinite-dimensional function spaces, we have to\nrecast training as finding the so-called weak mode of the posterior measure\nunder a Gaussian process (GP) prior restricted to the space of functions\nrepresentable by the neural network. Through the GP prior, one can express\nstructured and interpretable inductive biases, such as regularity or\nperiodicity, directly in function space, while still exploiting the implicit\ninductive biases that allow deep networks to generalize. After model\nlinearization, the training objective induces a negative log-posterior density\nto which we apply a Laplace approximation, leveraging highly scalable methods\nfrom matrix-free linear algebra. Our method provides improved results where\nprior knowledge is abundant, e.g., in many scientific inference tasks. At the\nsame time, it stays competitive for black-box regression and classification\ntasks where neural networks typically excel.\n","authors":["Tristan Cinquin","Marvin Pförtner","Vincent Fortuin","Philipp Hennig","Robert Bamler"],"pdf_url":"https://arxiv.org/pdf/2407.13711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13709v1","updated":"2024-07-18T17:08:10Z","published":"2024-07-18T17:08:10Z","title":"Understanding Reference Policies in Direct Preference Optimization","summary":"  Direct Preference Optimization (DPO) has become a widely used training method\nfor the instruction fine-tuning of large language models (LLMs). In this work,\nwe explore an under-investigated aspect of DPO - its dependency on the\nreference model or policy. Such reference policies, typically instantiated as\nthe model to be further fine-tuned, are important since they can impose an\nupper limit on DPO's effectiveness. Therefore, we address three related\nresearch questions in this work. First, we explore the optimal strength of the\nKL-divergence constraint in DPO, which penalizes deviations from the reference\npolicy, and find that DPO is sensitive to this strength. Next, we examine the\nnecessity of reference policies for instruction fine-tuning by providing both\ntheoretical and empirical comparisons between DPO and related learning\nobjectives, demonstrating DPO's superiority. Additionally, we investigate\nwhether DPO benefits from stronger reference policies, finding that a stronger\nreference policy can lead to improved performance, but only when it is similar\nto the model being fine-tuned. Our findings highlight the confounding role of\nreference policies in DPO and offer insights for best practices, while also\nidentifying open research questions for future studies.\n","authors":["Yixin Liu","Pengfei Liu","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2407.13709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13708v1","updated":"2024-07-18T17:07:32Z","published":"2024-07-18T17:07:32Z","title":"Are We Ready for Out-of-Distribution Detection in Digital Pathology?","summary":"  The detection of semantic and covariate out-of-distribution (OOD) examples is\na critical yet overlooked challenge in digital pathology (DP). Recently,\nsubstantial insight and methods on OOD detection were presented by the ML\ncommunity, but how do they fare in DP applications? To this end, we establish a\nbenchmark study, our highlights being: 1) the adoption of proper evaluation\nprotocols, 2) the comparison of diverse detectors in both a single and\nmulti-model setting, and 3) the exploration into advanced ML settings like\ntransfer learning (ImageNet vs. DP pre-training) and choice of architecture\n(CNNs vs. transformers). Through our comprehensive experiments, we contribute\nnew insights and guidelines, paving the way for future research and discussion.\n","authors":["Ji-Hun Oh","Kianoush Falahkheirkhah","Rohit Bhargava"],"pdf_url":"https://arxiv.org/pdf/2407.13708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00148v2","updated":"2024-07-18T17:07:17Z","published":"2024-06-28T17:57:12Z","title":"Localizing Anomalies via Multiscale Score Matching Analysis","summary":"  Anomaly detection and localization in medical imaging remain critical\nchallenges in healthcare. This paper introduces Spatial-MSMA (Multiscale Score\nMatching Analysis), a novel unsupervised method for anomaly localization in\nvolumetric brain MRIs. Building upon the MSMA framework, our approach\nincorporates spatial information and conditional likelihoods to enhance anomaly\ndetection capabilities. We employ a flexible normalizing flow model conditioned\non patch positions and global image features to estimate patch-wise anomaly\nscores. The method is evaluated on a dataset of 1,650 T1- and T2-weighted brain\nMRIs from typically developing children, with simulated lesions added to the\ntest set. Spatial-MSMA significantly outperforms existing methods, including\nreconstruction-based, generative-based, and interpretation-based approaches, in\nlesion detection and segmentation tasks. Our model achieves superior\nperformance in both distance-based metrics (99th percentile Hausdorff Distance:\n$7.05 \\pm 0.61$, Mean Surface Distance: $2.10 \\pm 0.43$) and component-wise\nmetrics (True Positive Rate: $0.83 \\pm 0.01$, Positive Predictive Value: $0.96\n\\pm 0.01$). These results demonstrate Spatial-MSMA's potential for accurate and\ninterpretable anomaly localization in medical imaging, with implications for\nimproved diagnosis and treatment planning in clinical settings. Our code is\navailable at~\\url{https://github.com/ahsanMah/sade/}.\n","authors":["Ahsan Mahmood","Junier Oliva","Martin Styner"],"pdf_url":"https://arxiv.org/pdf/2407.00148v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13704v1","updated":"2024-07-18T17:04:14Z","published":"2024-07-18T17:04:14Z","title":"Discovering governing equation in structural dynamics from\n  acceleration-only measurements","summary":"  Over the past few years, equation discovery has gained popularity in\ndifferent fields of science and engineering. However, existing equation\ndiscovery algorithms rely on the availability of noisy measurements of the\nstate variables (i.e., displacement {and velocity}). This is a major bottleneck\nin structural dynamics, where we often only have access to acceleration\nmeasurements. To that end, this paper introduces a novel equation discovery\nalgorithm for discovering governing equations of dynamical systems from\nacceleration-only measurements. The proposed algorithm employs a library-based\napproach for equation discovery. To enable equation discovery from\nacceleration-only measurements, we propose a novel Approximate Bayesian\nComputation (ABC) model that prioritizes parsimonious models. The efficacy of\nthe proposed algorithm is illustrated using {four} structural dynamics examples\nthat include both linear and nonlinear dynamical systems. The case studies\npresented illustrate the possible application of the proposed approach for\nequation discovery of dynamical systems from acceleration-only measurements.\n","authors":["Calvin Alvares","Souvik Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2407.13704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13677v1","updated":"2024-07-18T16:52:45Z","published":"2024-07-18T16:52:45Z","title":"PASTA: Controllable Part-Aware Shape Generation with Autoregressive\n  Transformers","summary":"  The increased demand for tools that automate the 3D content creation process\nled to tremendous progress in deep generative models that can generate diverse\n3D objects of high fidelity. In this paper, we present PASTA, an autoregressive\ntransformer architecture for generating high quality 3D shapes. PASTA comprises\ntwo main components: An autoregressive transformer that generates objects as a\nsequence of cuboidal primitives and a blending network, implemented with a\ntransformer decoder that composes the sequences of cuboids and synthesizes high\nquality meshes for each object. Our model is trained in two stages: First we\ntrain our autoregressive generative model using only annotated cuboidal parts\nas supervision and next, we train our blending network using explicit 3D\nsupervision, in the form of watertight meshes. Evaluations on various ShapeNet\nobjects showcase the ability of our model to perform shape generation from\ndiverse inputs \\eg from scratch, from a partial object, from text and images,\nas well size-guided generation, by explicitly conditioning on a bounding box\nthat defines the object's boundaries. Moreover, as our model considers the\nunderlying part-based structure of a 3D object, we are able to select a\nspecific part and produce shapes with meaningful variations of this part. As\nevidenced by our experiments, our model generates 3D shapes that are both more\nrealistic and diverse than existing part-based and non part-based methods,\nwhile at the same time is simpler to implement and train.\n","authors":["Songlin Li","Despoina Paschalidou","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2407.13677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13666v1","updated":"2024-07-18T16:42:10Z","published":"2024-07-18T16:42:10Z","title":"Non-Asymptotic Uncertainty Quantification in High-Dimensional Learning","summary":"  Uncertainty quantification (UQ) is a crucial but challenging task in many\nhigh-dimensional regression or learning problems to increase the confidence of\na given predictor. We develop a new data-driven approach for UQ in regression\nthat applies both to classical regression approaches such as the LASSO as well\nas to neural networks. One of the most notable UQ techniques is the debiased\nLASSO, which modifies the LASSO to allow for the construction of asymptotic\nconfidence intervals by decomposing the estimation error into a Gaussian and an\nasymptotically vanishing bias component. However, in real-world problems with\nfinite-dimensional data, the bias term is often too significant to be\nneglected, resulting in overly narrow confidence intervals. Our work rigorously\naddresses this issue and derives a data-driven adjustment that corrects the\nconfidence intervals for a large class of predictors by estimating the means\nand variances of the bias terms from training data, exploiting high-dimensional\nconcentration phenomena. This gives rise to non-asymptotic confidence\nintervals, which can help avoid overestimating uncertainty in critical\napplications such as MRI diagnosis. Importantly, our analysis extends beyond\nsparse regression to data-driven predictors like neural networks, enhancing the\nreliability of model-based deep learning. Our findings bridge the gap between\nestablished theory and the practical applicability of such debiased methods.\n","authors":["Frederik Hoppe","Claudio Mayrink Verdun","Hannah Laus","Felix Krahmer","Holger Rauhut"],"pdf_url":"https://arxiv.org/pdf/2407.13666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13664v1","updated":"2024-07-18T16:39:44Z","published":"2024-07-18T16:39:44Z","title":"Decision Focused Causal Learning for Direct Counterfactual Marketing\n  Optimization","summary":"  Marketing optimization plays an important role to enhance user engagement in\nonline Internet platforms. Existing studies usually formulate this problem as a\nbudget allocation problem and solve it by utilizing two fully decoupled stages,\ni.e., machine learning (ML) and operation research (OR). However, the learning\nobjective in ML does not take account of the downstream optimization task in\nOR, which causes that the prediction accuracy in ML may be not positively\nrelated to the decision quality.\n  Decision Focused Learning (DFL) integrates ML and OR into an end-to-end\nframework, which takes the objective of the downstream task as the decision\nloss function and guarantees the consistency of the optimization direction\nbetween ML and OR. However, deploying DFL in marketing is non-trivial due to\nmultiple technological challenges. Firstly, the budget allocation problem in\nmarketing is a 0-1 integer stochastic programming problem and the budget is\nuncertain and fluctuates a lot in real-world settings, which is beyond the\ngeneral problem background in DFL. Secondly, the counterfactual in marketing\ncauses that the decision loss cannot be directly computed and the optimal\nsolution can never be obtained, both of which disable the common\ngradient-estimation approaches in DFL. Thirdly, the OR solver is called\nfrequently to compute the decision loss during model training in DFL, which\nproduces huge computational cost and cannot support large-scale training data.\nIn this paper, we propose a decision focused causal learning framework (DFCL)\nfor direct counterfactual marketing optimization, which overcomes the above\ntechnological challenges. Both offline experiments and online A/B testing\ndemonstrate the effectiveness of DFCL over the state-of-the-art methods.\nCurrently, DFCL has been deployed in several marketing scenarios in Meituan,\none of the largest online food delivery platform in the world.\n","authors":["Hao Zhou","Rongxiao Huang","Shaoming Li","Guibin Jiang","Jiaqi Zheng","Bing Cheng","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2407.13664v1.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2407.13660v1","updated":"2024-07-18T16:38:24Z","published":"2024-07-18T16:38:24Z","title":"CogniVoice: Multimodal and Multilingual Fusion Networks for Mild\n  Cognitive Impairment Assessment from Spontaneous Speech","summary":"  Mild Cognitive Impairment (MCI) is a medical condition characterized by\nnoticeable declines in memory and cognitive abilities, potentially affecting\nindividual's daily activities. In this paper, we introduce CogniVoice, a novel\nmultilingual and multimodal framework to detect MCI and estimate Mini-Mental\nState Examination (MMSE) scores by analyzing speech data and its textual\ntranscriptions. The key component of CogniVoice is an ensemble multimodal and\nmultilingual network based on ``Product of Experts'' that mitigates reliance on\nshortcut solutions. Using a comprehensive dataset containing both English and\nChinese languages from TAUKADIAL challenge, CogniVoice outperforms the best\nperforming baseline model on MCI classification and MMSE regression tasks by\n2.8 and 4.1 points in F1 and RMSE respectively, and can effectively reduce the\nperformance gap across different language groups by 0.7 points in F1.\n","authors":["Jiali Cheng","Mohamed Elgaar","Nidhi Vakil","Hadi Amiri"],"pdf_url":"https://arxiv.org/pdf/2407.13660v1.pdf","comment":"INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.03482v2","updated":"2024-07-18T16:31:29Z","published":"2024-06-05T17:42:05Z","title":"QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead","summary":"  Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.\n","authors":["Amir Zandieh","Majid Daliri","Insu Han"],"pdf_url":"https://arxiv.org/pdf/2406.03482v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2405.20405v2","updated":"2024-07-18T16:22:29Z","published":"2024-05-30T18:20:35Z","title":"Private Mean Estimation with Person-Level Differential Privacy","summary":"  We study person-level differentially private (DP) mean estimation in the case\nwhere each person holds multiple samples. DP here requires the usual notion of\ndistributional stability when $\\textit{all}$ of a person's datapoints can be\nmodified. Informally, if $n$ people each have $m$ samples from an unknown\n$d$-dimensional distribution with bounded $k$-th moments, we show that people\nare necessary and sufficient to estimate the mean up to distance $\\alpha$ in\n$\\ell_2$-norm under $\\varepsilon$-differential privacy (and its common\nrelaxations). In the multivariate setting, we give computationally efficient\nalgorithms under approximate-DP and computationally inefficient algorithms\nunder pure DP, and our nearly matching lower bounds hold for the most\npermissive case of approximate DP. Our computationally efficient estimators are\nbased on the standard clip-and-noise framework, but the analysis for our\nsetting requires both new algorithmic techniques and new analyses. In\nparticular, our new bounds on the tails of sums of independent, vector-valued,\nbounded-moments random variables may be of interest.\n  \\[n = \\tilde \\Theta\\left(\\frac{d}{\\alpha^2 m} + \\frac{d}{\\alpha m^{1/2}\n\\varepsilon} + \\frac{d}{\\alpha^{k/(k-1)} m \\varepsilon} +\n\\frac{d}{\\varepsilon}\\right)\\]\n","authors":["Sushant Agarwal","Gautam Kamath","Mahbod Majid","Argyris Mouzakis","Rose Silver","Jonathan Ullman"],"pdf_url":"https://arxiv.org/pdf/2405.20405v2.pdf","comment":"72 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.10663v2","updated":"2024-07-18T16:10:07Z","published":"2024-03-15T20:12:41Z","title":"Not Just Change the Labels, Learn the Features: Watermarking Deep Neural\n  Networks with Multi-View Data","summary":"  With the increasing prevalence of Machine Learning as a Service (MLaaS)\nplatforms, there is a growing focus on deep neural network (DNN) watermarking\ntechniques. These methods are used to facilitate the verification of ownership\nfor a target DNN model to protect intellectual property. One of the most widely\nemployed watermarking techniques involves embedding a trigger set into the\nsource model. Unfortunately, existing methodologies based on trigger sets are\nstill susceptible to functionality-stealing attacks, potentially enabling\nadversaries to steal the functionality of the source model without a reliable\nmeans of verifying ownership. In this paper, we first introduce a novel\nperspective on trigger set-based watermarking methods from a feature learning\nperspective. Specifically, we demonstrate that by selecting data exhibiting\nmultiple features, also referred to as \\emph{multi-view data}, it becomes\nfeasible to effectively defend functionality stealing attacks. Based on this\nperspective, we introduce a novel watermarking technique based on Multi-view\ndATa, called MAT, for efficiently embedding watermarks within DNNs. This\napproach involves constructing a trigger set with multi-view data and\nincorporating a simple feature-based regularization method for training the\nsource model. We validate our method across various benchmarks and demonstrate\nits efficacy in defending against model extraction attacks, surpassing relevant\nbaselines by a significant margin. The code is available at:\n\\href{https://github.com/liyuxuan-github/MAT}{https://github.com/liyuxuan-github/MAT}.\n","authors":["Yuxuan Li","Sarthak Kumar Maharana","Yunhui Guo"],"pdf_url":"https://arxiv.org/pdf/2403.10663v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13632v1","updated":"2024-07-18T16:03:59Z","published":"2024-07-18T16:03:59Z","title":"Data Alchemy: Mitigating Cross-Site Model Variability Through Test Time\n  Data Calibration","summary":"  Deploying deep learning-based imaging tools across various clinical sites\nposes significant challenges due to inherent domain shifts and regulatory\nhurdles associated with site-specific fine-tuning. For histopathology, stain\nnormalization techniques can mitigate discrepancies, but they often fall short\nof eliminating inter-site variations. Therefore, we present Data Alchemy, an\nexplainable stain normalization method combined with test time data calibration\nvia a template learning framework to overcome barriers in cross-site analysis.\nData Alchemy handles shifts inherent to multi-site data and minimizes them\nwithout needing to change the weights of the normalization or classifier\nnetworks. Our approach extends to unseen sites in various clinical settings\nwhere data domain discrepancies are unknown. Extensive experiments highlight\nthe efficacy of our framework in tumor classification in hematoxylin and\neosin-stained patches. Our explainable normalization method boosts\nclassification tasks' area under the precision-recall curve(AUPR) by 0.165,\n0.545 to 0.710. Additionally, Data Alchemy further reduces the multisite\nclassification domain gap, by improving the 0.710 AUPR an additional 0.142,\nelevating classification performance further to 0.852, from 0.545. Our Data\nAlchemy framework can popularize precision medicine with minimal operational\noverhead by allowing for the seamless integration of pre-trained deep\nlearning-based clinical tools across multiple sites.\n","authors":["Abhijeet Parida","Antonia Alomar","Zhifan Jiang","Pooneh Roshanitabrizi","Austin Tapp","Maria Ledesma-Carbayo","Ziyue Xu","Syed Muhammed Anwar","Marius George Linguraru","Holger R. Roth"],"pdf_url":"https://arxiv.org/pdf/2407.13632v1.pdf","comment":"accepted to Machine Learning in Medical Imaging (MLMI 2024)"},{"id":"http://arxiv.org/abs/2404.17912v2","updated":"2024-07-18T16:03:18Z","published":"2024-04-27T13:46:23Z","title":"SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision\n  Language Models","summary":"  Radiology Report Generation (R2Gen) demonstrates how Multi-modal Large\nLanguage Models (MLLMs) can automate the creation of accurate and coherent\nradiological reports. Existing methods often hallucinate details in text-based\nreports that don't accurately reflect the image content. To mitigate this, we\nintroduce a novel strategy, SERPENT-VLM (SElf Refining Radiology RePort\nGENeraTion using Vision Language Models), which improves the R2Gen task by\nintegrating a self-refining mechanism into the MLLM framework. We employ a\nunique self-supervised loss that leverages similarity between pooled image\nrepresentations and the contextual representations of the generated\nradiological text, alongside the standard Causal Language Modeling objective,\nto refine image-text representations. This allows the model to scrutinize and\nalign the generated text through dynamic interaction between a given image and\nthe generated text, therefore reducing hallucination and continuously enhancing\nnuanced report generation. SERPENT-VLM outperforms existing baselines such as\nLLaVA-Med, BiomedGPT, etc., achieving SoTA performance on the IU X-ray and\nRadiology Objects in COntext (ROCO) datasets, and also proves to be robust\nagainst noisy images. A qualitative case study emphasizes the significant\nadvancements towards more sophisticated MLLM frameworks for R2Gen, opening\npaths for further research into self-supervised refinement in the medical\nimaging domain.\n","authors":["Manav Nitin Kapadnis","Sohan Patnaik","Abhilash Nandy","Sourjyadip Ray","Pawan Goyal","Debdoot Sheet"],"pdf_url":"https://arxiv.org/pdf/2404.17912v2.pdf","comment":"8 pages, 3 figures, 4 tables, Accepted as oral at Clinical NLP\n  workshop at NAACL 2024"},{"id":"http://arxiv.org/abs/2407.13625v1","updated":"2024-07-18T15:59:37Z","published":"2024-07-18T15:59:37Z","title":"Distributionally and Adversarially Robust Logistic Regression via\n  Intersecting Wasserstein Balls","summary":"  Empirical risk minimization often fails to provide robustness against\nadversarial attacks in test data, causing poor out-of-sample performance.\nAdversarially robust optimization (ARO) has thus emerged as the de facto\nstandard for obtaining models that hedge against such attacks. However, while\nthese models are robust against adversarial attacks, they tend to suffer\nseverely from overfitting. To address this issue for logistic regression, we\nstudy the Wasserstein distributionally robust (DR) counterpart of ARO and show\nthat this problem admits a tractable reformulation. Furthermore, we develop a\nframework to reduce the conservatism of this problem by utilizing an auxiliary\ndataset (e.g., synthetic, external, or out-of-domain data), whenever available,\nwith instances independently sampled from a nonidentical but related ground\ntruth. In particular, we intersect the ambiguity set of the DR problem with\nanother Wasserstein ambiguity set that is built using the auxiliary dataset. We\nanalyze the properties of the underlying optimization problem, develop\nefficient solution algorithms, and demonstrate that the proposed method\nconsistently outperforms benchmark approaches on real-world datasets.\n","authors":["Aras Selvi","Eleonora Kreacic","Mohsen Ghassemi","Vamsi Potluru","Tucker Balch","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2407.13625v1.pdf","comment":"34 pages, 3 color figures, under review at a conference"},{"id":"http://arxiv.org/abs/2407.13622v1","updated":"2024-07-18T15:58:04Z","published":"2024-07-18T15:58:04Z","title":"Misspecified $Q$-Learning with Sparse Linear Function Approximation:\n  Tight Bounds on Approximation Error","summary":"  The recent work by Dong & Yang (2023) showed for misspecified sparse linear\nbandits, one can obtain an $O\\left(\\epsilon\\right)$-optimal policy using a\npolynomial number of samples when the sparsity is a constant, where $\\epsilon$\nis the misspecification error. This result is in sharp contrast to misspecified\nlinear bandits without sparsity, which require an exponential number of samples\nto get the same guarantee. In order to study whether the analog result is\npossible in the reinforcement learning setting, we consider the following\nproblem: assuming the optimal $Q$-function is a $d$-dimensional linear function\nwith sparsity $k$ and misspecification error $\\epsilon$, whether we can obtain\nan $O\\left(\\epsilon\\right)$-optimal policy using number of samples polynomially\nin the feature dimension $d$. We first demonstrate why the standard approach\nbased on Bellman backup or the existing optimistic value function elimination\napproach such as OLIVE (Jiang et al., 2017) achieves suboptimal guarantees for\nthis problem. We then design a novel elimination-based algorithm to show one\ncan obtain an $O\\left(H\\epsilon\\right)$-optimal policy with sample complexity\npolynomially in the feature dimension $d$ and planning horizon $H$. Lastly, we\ncomplement our upper bound with an $\\widetilde{\\Omega}\\left(H\\epsilon\\right)$\nsuboptimality lower bound, giving a complete picture of this problem.\n","authors":["Ally Yalei Du","Lin F. Yang","Ruosong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13622v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2407.13621v1","updated":"2024-07-18T15:57:55Z","published":"2024-07-18T15:57:55Z","title":"Differential Privacy Mechanisms in Neural Tangent Kernel Regression","summary":"  Training data privacy is a fundamental problem in modern Artificial\nIntelligence (AI) applications, such as face recognition, recommendation\nsystems, language generation, and many others, as it may contain sensitive user\ninformation related to legal issues. To fundamentally understand how privacy\nmechanisms work in AI applications, we study differential privacy (DP) in the\nNeural Tangent Kernel (NTK) regression setting, where DP is one of the most\npowerful tools for measuring privacy under statistical learning, and NTK is one\nof the most popular analysis frameworks for studying the learning mechanisms of\ndeep neural networks. In our work, we can show provable guarantees for both\ndifferential privacy and test accuracy of our NTK regression. Furthermore, we\nconduct experiments on the basic image classification dataset CIFAR10 to\ndemonstrate that NTK regression can preserve good accuracy under a modest\nprivacy budget, supporting the validity of our analysis. To our knowledge, this\nis the first work to provide a DP guarantee for NTK regression.\n","authors":["Jiuxiang Gu","Yingyu Liang","Zhizhou Sha","Zhenmei Shi","Zhao Song"],"pdf_url":"https://arxiv.org/pdf/2407.13621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13605v1","updated":"2024-07-18T15:44:23Z","published":"2024-07-18T15:44:23Z","title":"Physics-guided Active Sample Reweighting for Urban Flow Prediction","summary":"  Urban flow prediction is a spatio-temporal modeling task that estimates the\nthroughput of transportation services like buses, taxis, and ride-sharing,\nwhere data-driven models have become the most popular solution in the past\ndecade. Meanwhile, the implicitly learned mapping between historical\nobservations to the prediction targets tend to over-simplify the dynamics of\nreal-world urban flows, leading to suboptimal predictions. Some recent\nspatio-temporal prediction solutions bring remedies with the notion of\nphysics-guided machine learning (PGML), which describes spatio-temporal data\nwith nuanced and principled physics laws, thus enhancing both the prediction\naccuracy and interpretability. However, these spatio-temporal PGML methods are\nbuilt upon a strong assumption that the observed data fully conforms to the\ndifferential equations that define the physical system, which can quickly\nbecome ill-posed in urban flow prediction tasks. The observed urban flow data,\nespecially when sliced into time-dependent snapshots to facilitate predictions,\nis typically incomplete and sparse, and prone to inherent noise incurred in the\ncollection process. As a result, such physical inconsistency between the data\nand PGML model significantly limits the predictive power and robustness of the\nsolution. Moreover, due to the interval-based predictions and intermittent\nnature of data filing in many transportation services, the instantaneous\ndynamics of urban flows can hardly be captured, rendering differential\nequation-based continuous modeling a loose fit for this setting. To overcome\nthe challenges, we develop a discretized physics-guided network (PN), and\npropose a data-aware framework Physics-guided Active Sample Reweighting\n(P-GASR) to enhance PN. Experimental results in four real-world datasets\ndemonstrate that our method achieves state-of-the-art performance with a\ndemonstrable improvement in robustness.\n","authors":["Wei Jiang","Tong Chen","Guanhua Ye","Wentao Zhang","Lizhen Cui","Zi Huang","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2407.13605v1.pdf","comment":"This paper is accepted by Proceedings of the 33nd ACM International\n  Conference on Information and Knowledge Management (CIKM '24)"},{"id":"http://arxiv.org/abs/2407.13594v1","updated":"2024-07-18T15:32:44Z","published":"2024-07-18T15:32:44Z","title":"Mechanistically Interpreting a Transformer-based 2-SAT Solver: An\n  Axiomatic Approach","summary":"  Mechanistic interpretability aims to reverse engineer the computation\nperformed by a neural network in terms of its internal components. Although\nthere is a growing body of research on mechanistic interpretation of neural\nnetworks, the notion of a mechanistic interpretation itself is often ad-hoc.\nInspired by the notion of abstract interpretation from the program analysis\nliterature that aims to develop approximate semantics for programs, we give a\nset of axioms that formally characterize a mechanistic interpretation as a\ndescription that approximately captures the semantics of the neural network\nunder analysis in a compositional manner. We use these axioms to guide the\nmechanistic interpretability analysis of a Transformer-based model trained to\nsolve the well-known 2-SAT problem. We are able to reverse engineer the\nalgorithm learned by the model -- the model first parses the input formulas and\nthen evaluates their satisfiability via enumeration of different possible\nvaluations of the Boolean input variables. We also present evidence to support\nthat the mechanistic interpretation of the analyzed model indeed satisfies the\nstated axioms.\n","authors":["Nils Palumbo","Ravi Mangal","Zifan Wang","Saranya Vijayakumar","Corina S. Pasareanu","Somesh Jha"],"pdf_url":"https://arxiv.org/pdf/2407.13594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16619v2","updated":"2024-07-18T15:32:16Z","published":"2024-06-24T13:02:36Z","title":"No More Sliding-Windows: Dynamic Functional Connectivity Based On Random\n  Convolutions Without Learning","summary":"  Compared to static functional connectivity, dynamic functional connectivity\nprovides more detailed temporal information. The traditional sliding window\nmethod constructs functional connectivity matrices by applying a moving time\nwindow across the entire time series to calculate correlations between brain\nregions. However, as a method of feature extraction, it exhibits significant\nlimitations, such as the dependency of feature dimensions on the window length\nand the generation of features lacking information from other time points\nwithin the window. This paper presents RandCon, a novel method for calculating\ndynamic functional connectivity (DFC), which employs randomly generated\nmulti-dimensional convolution kernels. This method performs convolution\noperations directly on the BOLD signal without the need for learning,\nextracting functional connectivity features. Compared to the sliding window\nmethod, RandCon shows notable improvements in performance on simulated data,\nparticularly in terms of temporal accuracy and noise resistance. Results from\nreal data indicate that this method maintains stability within short time\nwindows and better identifies gender differences. Furthermore, we propose a\nmore comprehensive theoretical framework, the multi-dimensional convolution\nmethod, where the sliding window method and its variants are specific cases of\nthis method. The proposed method is straightforward and efficient,\nsignificantly broadening the scope of dynamic functional connectivity research\nand offering substantial theoretical and practical potential.\n","authors":["Yongjie Duan","Zhiying Long"],"pdf_url":"https://arxiv.org/pdf/2406.16619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17029v2","updated":"2024-07-18T15:31:24Z","published":"2024-01-30T14:06:09Z","title":"LADDER: Revisiting the Cosmic Distance Ladder with Deep Learning\n  Approaches and Exploring its Applications","summary":"  We investigate the prospect of reconstructing the ''cosmic distance ladder''\nof the Universe using a novel deep learning framework called LADDER - Learning\nAlgorithm for Deep Distance Estimation and Reconstruction. LADDER is trained on\nthe apparent magnitude data from the Pantheon Type Ia supernovae compilation,\nincorporating the full covariance information among data points, to produce\npredictions along with corresponding errors. After employing several validation\ntests with a number of deep learning models, we pick LADDER as the best\nperforming one. We then demonstrate applications of our method in the\ncosmological context, including serving as a model-independent tool for\nconsistency checks for other datasets like baryon acoustic oscillations,\ncalibration of high-redshift datasets such as gamma ray bursts, and use as a\nmodel-independent mock catalog generator for future probes. Our analysis\nadvocates for careful consideration of machine learning techniques applied to\ncosmological contexts.\n","authors":["Rahul Shah","Soumadeep Saha","Purba Mukherjee","Utpal Garain","Supratik Pal"],"pdf_url":"https://arxiv.org/pdf/2401.17029v2.pdf","comment":"13 pages, 6 sets of figures, 5 tables. To appear in the Astrophys. J.\n  Suppl. Ser. Code available at https://github.com/rahulshah1397/LADDER"},{"id":"http://arxiv.org/abs/2407.13592v1","updated":"2024-07-18T15:29:48Z","published":"2024-07-18T15:29:48Z","title":"MeshFeat: Multi-Resolution Features for Neural Fields on Meshes","summary":"  Parametric feature grid encodings have gained significant attention as an\nencoding approach for neural fields since they allow for much smaller MLPs,\nwhich significantly decreases the inference time of the models. In this work,\nwe propose MeshFeat, a parametric feature encoding tailored to meshes, for\nwhich we adapt the idea of multi-resolution feature grids from Euclidean space.\nWe start from the structure provided by the given vertex topology and use a\nmesh simplification algorithm to construct a multi-resolution feature\nrepresentation directly on the mesh. The approach allows the usage of small\nMLPs for neural fields on meshes, and we show a significant speed-up compared\nto previous representations while maintaining comparable reconstruction quality\nfor texture reconstruction and BRDF representation. Given its intrinsic\ncoupling to the vertices, the method is particularly well-suited for\nrepresentations on deforming meshes, making it a good fit for object animation.\n","authors":["Mihir Mahajan","Florian Hofherr","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2407.13592v1.pdf","comment":"To appear at European Conference on Computer Vision (ECCV), 2024"},{"id":"http://arxiv.org/abs/2407.13575v1","updated":"2024-07-18T15:15:19Z","published":"2024-07-18T15:15:19Z","title":"With or Without Replacement? Improving Confidence in Fourier Imaging","summary":"  Over the last few years, debiased estimators have been proposed in order to\nestablish rigorous confidence intervals for high-dimensional problems in\nmachine learning and data science. The core argument is that the error of these\nestimators with respect to the ground truth can be expressed as a Gaussian\nvariable plus a remainder term that vanishes as long as the dimension of the\nproblem is sufficiently high. Thus, uncertainty quantification (UQ) can be\nperformed exploiting the Gaussian model. Empirically, however, the remainder\nterm cannot be neglected in many realistic situations of moderately-sized\ndimensions, in particular in certain structured measurement scenarios such as\nMagnetic Resonance Imaging (MRI). This, in turn, can downgrade the advantage of\nthe UQ methods as compared to non-UQ approaches such as the standard LASSO. In\nthis paper, we present a method to improve the debiased estimator by sampling\nwithout replacement. Our approach leverages recent results of ours on the\nstructure of the random nature of certain sampling schemes showing how a\ntransition between sampling with and without replacement can lead to a weighted\nreconstruction scheme with improved performance for the standard LASSO. In this\npaper, we illustrate how this reweighted sampling idea can also improve the\ndebiased estimator and, consequently, provide a better method for UQ in Fourier\nimaging.\n","authors":["Frederik Hoppe","Claudio Mayrink Verdun","Felix Krahmer","Marion I. Menzel","Holger Rauhut"],"pdf_url":"https://arxiv.org/pdf/2407.13575v1.pdf","comment":"Accepted at Cosera 2024"},{"id":"http://arxiv.org/abs/2407.12288v2","updated":"2024-07-18T14:35:39Z","published":"2024-07-17T03:18:40Z","title":"Information-Theoretic Foundations for Machine Learning","summary":"  The staggering progress of machine learning in the past decade has been a\nsight to behold. In retrospect, it is both remarkable and unsettling that these\nmilestones were achievable with little to no rigorous theory to guide\nexperimentation. Despite this fact, practitioners have been able to guide their\nfuture experimentation via observations from previous large-scale empirical\ninvestigations. However, alluding to Plato's Allegory of the cave, it is likely\nthat the observations which form the field's notion of reality are but shadows\nrepresenting fragments of that reality. In this work, we propose a theoretical\nframework which attempts to answer what exists outside of the cave. To the\ntheorist, we provide a framework which is mathematically rigorous and leaves\nopen many interesting ideas for future exploration. To the practitioner, we\nprovide a framework whose results are very intuitive, general, and which will\nhelp form principles to guide future investigations. Concretely, we provide a\ntheoretical framework rooted in Bayesian statistics and Shannon's information\ntheory which is general enough to unify the analysis of many phenomena in\nmachine learning. Our framework characterizes the performance of an optimal\nBayesian learner, which considers the fundamental limits of information.\nThroughout this work, we derive very general theoretical results and apply them\nto derive insights specific to settings ranging from data which is\nindependently and identically distributed under an unknown distribution, to\ndata which is sequential, to data which exhibits hierarchical structure\namenable to meta-learning. We conclude with a section dedicated to\ncharacterizing the performance of misspecified algorithms. These results are\nexciting and particularly relevant as we strive to overcome increasingly\ndifficult machine learning challenges in this endlessly complex world.\n","authors":["Hong Jun Jeon","Benjamin Van Roy"],"pdf_url":"https://arxiv.org/pdf/2407.12288v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12206v3","updated":"2024-07-18T14:34:33Z","published":"2023-03-21T21:42:03Z","title":"Policy Optimization for Personalized Interventions in Behavioral Health","summary":"  Behavioral health interventions, delivered through digital platforms, have\nthe potential to significantly improve health outcomes, through education,\nmotivation, reminders, and outreach. We study the problem of optimizing\npersonalized interventions for patients to maximize a long-term outcome, where\ninterventions are costly and capacity-constrained. We assume we have access to\na historical dataset collected from an initial pilot study. We present a new\napproach for this problem that we dub DecompPI, which decomposes the state\nspace for a system of patients to the individual level and then approximates\none step of policy iteration. Implementing DecompPI simply consists of a\nprediction task using the dataset, alleviating the need for online\nexperimentation. DecompPI is a generic model-free algorithm that can be used\nirrespective of the underlying patient behavior model. We derive theoretical\nguarantees on a simple, special case of the model that is representative of our\nproblem setting. When the initial policy used to collect the data is\nrandomized, we establish an approximation guarantee for DecompPI with respect\nto the improvement beyond a null policy that does not allocate interventions.\nWe show that this guarantee is robust to estimation errors. We then conduct a\nrigorous empirical case study using real-world data from a mobile health\nplatform for improving treatment adherence for tuberculosis. Using a validated\nsimulation model, we demonstrate that DecompPI can provide the same efficacy as\nthe status quo approach with approximately half the capacity of interventions.\nDecompPI is simple and easy to implement for an organization aiming to improve\nlong-term behavior through targeted interventions, and this paper demonstrates\nits strong performance both theoretically and empirically, particularly in\nresource-limited settings.\n","authors":["Jackie Baek","Justin J. Boutilier","Vivek F. Farias","Jonas Oddur Jonasson","Erez Yoeli"],"pdf_url":"https://arxiv.org/pdf/2303.12206v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00448v2","updated":"2024-07-18T14:23:29Z","published":"2023-12-31T10:53:58Z","title":"Beyond Chinchilla-Optimal: Accounting for Inference in Language Model\n  Scaling Laws","summary":"  Large language model (LLM) scaling laws are empirical formulas that estimate\nchanges in model quality as a result of increasing parameter count and training\ndata. However, these formulas, including the popular Deepmind Chinchilla\nscaling laws, neglect to include the cost of inference. We modify the\nChinchilla scaling laws to calculate the optimal LLM parameter count and\npre-training data size to train and deploy a model of a given quality and\ninference demand. We conduct our analysis both in terms of a compute budget and\nreal-world costs and find that LLM researchers expecting reasonably large\ninference demand (~1B requests) should train models smaller and longer than\nChinchilla-optimal. Furthermore, we train 47 models of varying sizes and\nparameter counts to validate our formula and find that model quality continues\nto improve as we scale tokens per parameter to extreme ranges (up to 10,000).\nFinally, we ablate the procedure used to fit the Chinchilla scaling law\ncoefficients and find that developing scaling laws only from data collected at\ntypical token/parameter ratios overestimates the impact of additional tokens at\nthese extreme ranges.\n","authors":["Nikhil Sardana","Jacob Portes","Sasha Doubov","Jonathan Frankle"],"pdf_url":"https://arxiv.org/pdf/2401.00448v2.pdf","comment":"16 pages, 7 figures, To appear in the 41st International Conference\n  on Machine Learning, 2024"},{"id":"http://arxiv.org/abs/2406.05227v2","updated":"2024-07-18T14:11:39Z","published":"2024-06-07T19:29:55Z","title":"Mixed-Curvature Decision Trees and Random Forests","summary":"  We extend decision tree and random forest algorithms to product space\nmanifolds: Cartesian products of Euclidean, hyperspherical, and hyperbolic\nmanifolds. Such spaces have extremely expressive geometries capable of\nrepresenting many arrangements of distances with low metric distortion. To\ndate, all classifiers for product spaces fit a single linear decision boundary,\nand no regressor has been described. Our method enables a simple, expressive\nmethod for classification and regression in product manifolds. We demonstrate\nthe superior accuracy of our tool compared to Euclidean methods operating in\nthe ambient space or the tangent plane of the manifold across a range of\nconstant-curvature and product manifolds. Code for our implementation and\nexperiments is available at https://github.com/pchlenski/embedders.\n","authors":["Philippe Chlenski","Quentin Chu","Itsik Pe'er"],"pdf_url":"https://arxiv.org/pdf/2406.05227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13538v1","updated":"2024-07-18T14:10:50Z","published":"2024-07-18T14:10:50Z","title":"EnergyDiff: Universal Time-Series Energy Data Generation using Diffusion\n  Models","summary":"  High-resolution time series data are crucial for operation and planning in\nenergy systems such as electrical power systems and heating systems. However,\ndue to data collection costs and privacy concerns, such data is often\nunavailable or insufficient for downstream tasks. Data synthesis is a potential\nsolution for this data scarcity. With the recent development of generative AI,\nwe propose EnergyDiff, a universal data generation framework for energy time\nseries data. EnergyDiff builds on state-of-the-art denoising diffusion\nprobabilistic models, utilizing a proposed denoising network dedicated to\nhigh-resolution time series data and introducing a novel Marginal Calibration\ntechnique. Our extensive experimental results demonstrate that EnergyDiff\nachieves significant improvement in capturing temporal dependencies and\nmarginal distributions compared to baselines, particularly at the 1-minute\nresolution. Additionally, EnergyDiff consistently generates high-quality time\nseries data across diverse energy domains, time resolutions, and at both\ncustomer and transformer levels with reduced computational need.\n","authors":["Nan Lin","Peter Palensky","Pedro P. Vergara"],"pdf_url":"https://arxiv.org/pdf/2407.13538v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.13531v1","updated":"2024-07-18T14:04:36Z","published":"2024-07-18T14:04:36Z","title":"Evaluating the performance-deviation of itemKNN in RecBole and LensKit","summary":"  This study examines the performance of item-based k-Nearest Neighbors\n(ItemKNN) algorithms in the RecBole and LensKit recommender system libraries.\nUsing four data sets (Anime, Modcloth, ML-100K, and ML-1M), we assess each\nlibrary's efficiency, accuracy, and scalability, focusing primarily on\nnormalized discounted cumulative gain (nDCG). Our results show that RecBole\noutperforms LensKit on two of three metrics on the ML-100K data set: it\nachieved an 18% higher nDCG, 14% higher precision, and 35% lower recall. To\nensure a fair comparison, we adjusted LensKit's nDCG calculation to match\nRecBole's method. This alignment made the performance more comparable, with\nLensKit achieving an nDCG of 0.2540 and RecBole 0.2674. Differences in\nsimilarity matrix calculations were identified as the main cause of performance\ndeviations. After modifying LensKit to retain only the top K similar items,\nboth libraries showed nearly identical nDCG values across all data sets. For\ninstance, both achieved an nDCG of 0.2586 on the ML-1M data set with the same\nrandom seed. Initially, LensKit's original implementation only surpassed\nRecBole in the ModCloth dataset.\n","authors":["Michael Schmidt","Jannik Nitschke","Tim Prinz"],"pdf_url":"https://arxiv.org/pdf/2407.13531v1.pdf","comment":"Pages: 6, Figures: 4, Tables: 4, Subsections: Introduction, Library\n  Introduction, Method (Data Sets, Algorithms, Pre-processing and Data\n  Splitting, Algorithm Training and Evaluation, Hardware Specifications),\n  Results (First Steps, Further Investigations, Discussion)"},{"id":"http://arxiv.org/abs/2407.13526v1","updated":"2024-07-18T13:59:10Z","published":"2024-07-18T13:59:10Z","title":"Discussion: Effective and Interpretable Outcome Prediction by Training\n  Sparse Mixtures of Linear Experts","summary":"  Process Outcome Prediction entails predicting a discrete property of an\nunfinished process instance from its partial trace. High-capacity outcome\npredictors discovered with ensemble and deep learning methods have been shown\nto achieve top accuracy performances, but they suffer from a lack of\ntransparency. Aligning with recent efforts to learn inherently interpretable\noutcome predictors, we propose to train a sparse Mixture-of-Experts where both\nthe ``gate'' and ``expert'' sub-nets are Logistic Regressors. This\nensemble-like model is trained end-to-end while automatically selecting a\nsubset of input features in each sub-net, as an alternative to the common\napproach of performing a global feature selection step prior to model training.\nTest results on benchmark logs confirmed the validity and efficacy of this\napproach.\n","authors":["Francesco Folino","Luigi Pontieri","Pietro Sabatino"],"pdf_url":"https://arxiv.org/pdf/2407.13526v1.pdf","comment":"This paper summarizes results presented at workshop \\emph{ML4PM\n  2023}, associated with conference ICPM 2023, October 23-27, 2023, Rome,\n  Italy. 6 pages, 1 figure"},{"id":"http://arxiv.org/abs/2407.13522v1","updated":"2024-07-18T13:57:16Z","published":"2024-07-18T13:57:16Z","title":"INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question\n  Answering capability of LLMs for Indic Languages","summary":"  Large Language Models (LLMs) have demonstrated remarkable zero-shot and\nfew-shot capabilities in unseen tasks, including context-grounded question\nanswering (QA) in English. However, the evaluation of LLMs' capabilities in\nnon-English languages for context-based QA is limited by the scarcity of\nbenchmarks in non-English languages. To address this gap, we introduce\nIndic-QA, the largest publicly available context-grounded question-answering\ndataset for 11 major Indian languages from two language families. The dataset\ncomprises both extractive and abstractive question-answering tasks and includes\nexisting datasets as well as English QA datasets translated into Indian\nlanguages. Additionally, we generate a synthetic dataset using the Gemini model\nto create question-answer pairs given a passage, which is then manually\nverified for quality assurance. We evaluate various multilingual Large Language\nModels and their instruction-fine-tuned variants on the benchmark and observe\nthat their performance is subpar, particularly for low-resource languages. We\nhope that the release of this dataset will stimulate further research on the\nquestion-answering abilities of LLMs for low-resource languages.\n","authors":["Abhishek Kumar Singh","Rudra Murthy","Vishwajeet kumar","Jaydeep Sen","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2407.13522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13518v1","updated":"2024-07-18T13:49:21Z","published":"2024-07-18T13:49:21Z","title":"Model-based Policy Optimization using Symbolic World Model","summary":"  The application of learning-based control methods in robotics presents\nsignificant challenges. One is that model-free reinforcement learning\nalgorithms use observation data with low sample efficiency. To address this\nchallenge, a prevalent approach is model-based reinforcement learning, which\ninvolves employing an environment dynamics model. We suggest approximating\ntransition dynamics with symbolic expressions, which are generated via symbolic\nregression. Approximation of a mechanical system with a symbolic model has\nfewer parameters than approximation with neural networks, which can potentially\nlead to higher accuracy and quality of extrapolation. We use a symbolic\ndynamics model to generate trajectories in model-based policy optimization to\nimprove the sample efficiency of the learning algorithm. We evaluate our\napproach across various tasks within simulated environments. Our method\ndemonstrates superior sample efficiency in these tasks compared to model-free\nand model-based baseline methods.\n","authors":["Andrey Gorodetskiy","Konstantin Mironov","Aleksandr Panov"],"pdf_url":"https://arxiv.org/pdf/2407.13518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13513v1","updated":"2024-07-18T13:44:43Z","published":"2024-07-18T13:44:43Z","title":"Instance Selection for Dynamic Algorithm Configuration with\n  Reinforcement Learning: Improving Generalization","summary":"  Dynamic Algorithm Configuration (DAC) addresses the challenge of dynamically\nsetting hyperparameters of an algorithm for a diverse set of instances rather\nthan focusing solely on individual tasks. Agents trained with Deep\nReinforcement Learning (RL) offer a pathway to solve such settings. However,\nthe limited generalization performance of these agents has significantly\nhindered the application in DAC. Our hypothesis is that a potential bias in the\ntraining instances limits generalization capabilities. We take a step towards\nmitigating this by selecting a representative subset of training instances to\novercome overrepresentation and then retraining the agent on this subset to\nimprove its generalization performance. For constructing the meta-features for\nthe subset selection, we particularly account for the dynamic nature of the RL\nagent by computing time series features on trajectories of actions and rewards\ngenerated by the agent's interaction with the environment. Through empirical\nevaluations on the Sigmoid and CMA-ES benchmarks from the standard benchmark\nlibrary for DAC, called DACBench, we discuss the potentials of our selection\ntechnique compared to training on the entire instance set. Our results\nhighlight the efficacy of instance selection in refining DAC policies for\ndiverse instance spaces.\n","authors":["Carolin Benjamins","Gjorgjina Cenikj","Ana Nikolikj","Aditya Mohan","Tome Eftimov","Marius Lindauer"],"pdf_url":"https://arxiv.org/pdf/2407.13513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13509v1","updated":"2024-07-18T13:42:38Z","published":"2024-07-18T13:42:38Z","title":"Spontaneous Style Text-to-Speech Synthesis with Controllable Spontaneous\n  Behaviors Based on Language Models","summary":"  Spontaneous style speech synthesis, which aims to generate human-like speech,\noften encounters challenges due to the scarcity of high-quality data and\nlimitations in model capabilities. Recent language model-based TTS systems can\nbe trained on large, diverse, and low-quality speech datasets, resulting in\nhighly natural synthesized speech. However, they are limited by the difficulty\nof simulating various spontaneous behaviors and capturing prosody variations in\nspontaneous speech. In this paper, we propose a novel spontaneous speech\nsynthesis system based on language models. We systematically categorize and\nuniformly model diverse spontaneous behaviors. Moreover, fine-grained prosody\nmodeling is introduced to enhance the model's ability to capture subtle prosody\nvariations in spontaneous speech.Experimental results show that our proposed\nmethod significantly outperforms the baseline methods in terms of prosody\nnaturalness and spontaneous behavior naturalness.\n","authors":["Weiqin Li","Peiji Yang","Yicheng Zhong","Yixuan Zhou","Zhisheng Wang","Zhiyong Wu","Xixin Wu","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2407.13509v1.pdf","comment":"Accepted by INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2405.00625v2","updated":"2024-07-18T13:38:31Z","published":"2024-05-01T16:48:28Z","title":"Queue-based Eco-Driving at Roundabouts with Reinforcement Learning","summary":"  We address eco-driving at roundabouts in mixed traffic to enhance traffic\nflow and traffic efficiency in urban areas. The aim is to proactively optimize\nspeed of automated or non-automated connected vehicles (CVs), ensuring both an\nefficient approach and smooth entry into roundabouts. We incorporate the\ntraffic situation ahead, i.e. preceding vehicles and waiting queues. Further,\nwe develop two approaches: a rule-based and an Reinforcement Learning (RL)\nbased eco-driving system, with both using the approach link and information\nfrom conflicting CVs for speed optimization. A fair comparison of rule-based\nand RL-based approaches is performed to explore RL as a viable alternative to\nclassical optimization. Results show that both approaches outperform the\nbaseline. Improvements significantly increase with growing traffic volumes,\nleading to best results on average being obtained at high volumes. Near\ncapacity, performance deteriorates, indicating limited applicability at\ncapacity limits. Examining different CV penetration rates, a decline in\nperformance is observed, but with substantial results still being achieved at\nlower CV rates. RL agents can discover effective policies for speed\noptimization in dynamic roundabout settings, but they do not offer a\nsubstantial advantage over classical approaches, especially at higher traffic\nvolumes or lower CV penetration rates.\n","authors":["Anna-Lena Schlamp","Werner Huber","Stefanie Schmidtner"],"pdf_url":"https://arxiv.org/pdf/2405.00625v2.pdf","comment":"Reward function of the RL-agent needs to be updated, optimization in\n  progress"},{"id":"http://arxiv.org/abs/2405.10995v2","updated":"2024-07-18T13:29:44Z","published":"2024-05-16T16:35:43Z","title":"Higher-order Spatio-temporal Physics-incorporated Graph Neural Network\n  for Multivariate Time Series Imputation","summary":"  Exploring the missing values is an essential but challenging issue due to the\ncomplex latent spatio-temporal correlation and dynamic nature of time series.\nOwing to the outstanding performance in dealing with structure learning\npotentials, Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs)\nare often used to capture such complex spatio-temporal features in multivariate\ntime series. However, these data-driven models often fail to capture the\nessential spatio-temporal relationships when significant signal corruption\noccurs. Additionally, calculating the high-order neighbor nodes in these models\nis of high computational complexity. To address these problems, we propose a\nnovel higher-order spatio-temporal physics-incorporated GNN (HSPGNN). Firstly,\nthe dynamic Laplacian matrix can be obtained by the spatial attention\nmechanism. Then, the generic inhomogeneous partial differential equation (PDE)\nof physical dynamic systems is used to construct the dynamic higher-order\nspatio-temporal GNN to obtain the missing time series values. Moreover, we\nestimate the missing impact by Normalizing Flows (NF) to evaluate the\nimportance of each node in the graph for better explainability. Experimental\nresults on four benchmark datasets demonstrate the effectiveness of HSPGNN and\nthe superior performance when combining various order neighbor nodes. Also,\ngraph-like optical flow, dynamic graphs, and missing impact can be obtained\nnaturally by HSPGNN, which provides better dynamic analysis and explanation\nthan traditional data-driven models. Our code is available at\nhttps://github.com/gorgen2020/HSPGNN.\n","authors":["Guojun Liang","Prayag Tiwari","Slawomir Nowaczyk","Stefan Byttner"],"pdf_url":"https://arxiv.org/pdf/2405.10995v2.pdf","comment":"18 pages, 7 figures, CIKM 2024"},{"id":"http://arxiv.org/abs/2407.00463v4","updated":"2024-07-18T13:26:57Z","published":"2024-06-29T15:20:11Z","title":"Open-Source Conversational AI with SpeechBrain 1.0","summary":"  SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,\nfocused particularly on speech processing tasks such as speech recognition,\nspeech enhancement, speaker recognition, text-to-speech, and much more. It\npromotes transparency and replicability by releasing both the pre-trained\nmodels and the complete \"recipes\" of code and algorithms required for training\nthem. This paper presents SpeechBrain 1.0, a significant milestone in the\nevolution of the toolkit, which now has over 200 recipes for speech, audio, and\nlanguage processing tasks, and more than 100 models available on Hugging Face.\nSpeechBrain 1.0 introduces new technologies to support diverse learning\nmodalities, Large Language Model (LLM) integration, and advanced decoding\nstrategies, along with novel models, tasks, and modalities. It also includes a\nnew benchmark repository, offering researchers a unified platform for\nevaluating models across diverse tasks.\n","authors":["Mirco Ravanelli","Titouan Parcollet","Adel Moumen","Sylvain de Langen","Cem Subakan","Peter Plantinga","Yingzhi Wang","Pooneh Mousavi","Luca Della Libera","Artem Ploujnikov","Francesco Paissan","Davide Borra","Salah Zaiem","Zeyu Zhao","Shucong Zhang","Georgios Karakasidis","Sung-Lin Yeh","Pierre Champion","Aku Rouhe","Rudolf Braun","Florian Mai","Juan Zuluaga-Gomez","Seyed Mahed Mousavi","Andreas Nautsch","Xuechen Liu","Sangeet Sagar","Jarod Duret","Salima Mdhaffar","Gaelle Laperriere","Mickael Rouvier","Renato De Mori","Yannick Esteve"],"pdf_url":"https://arxiv.org/pdf/2407.00463v4.pdf","comment":"Submitted to JMLR (Machine Learning Open Source Software)"},{"id":"http://arxiv.org/abs/2407.13493v1","updated":"2024-07-18T13:23:16Z","published":"2024-07-18T13:23:16Z","title":"Training Foundation Models as Data Compression: On Information, Model\n  Weights and Copyright Law","summary":"  The training process of foundation models as for other classes of deep\nlearning systems is based on minimizing the reconstruction error over a\ntraining set. For this reason, they are susceptible to the memorization and\nsubsequent reproduction of training samples. In this paper, we introduce a\ntraining-as-compressing perspective, wherein the model's weights embody a\ncompressed representation of the training data. From a copyright standpoint,\nthis point of view implies that the weights could be considered a reproduction\nor a derivative work of a potentially protected set of works. We investigate\nthe technical and legal challenges that emerge from this framing of the\ncopyright of outputs generated by foundation models, including their\nimplications for practitioners and researchers. We demonstrate that adopting an\ninformation-centric approach to the problem presents a promising pathway for\ntackling these emerging complex legal issues.\n","authors":["Giorgio Franceschelli","Claudia Cevenini","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2407.13493v1.pdf","comment":"Accepted for spotlight presentation at GenLaw'24, see\n  https://www.genlaw.org/2024-icml-papers#training-foundation-models-as-data-compression-on-information-model-weights-and-copyright-law"},{"id":"http://arxiv.org/abs/2306.03542v2","updated":"2024-07-18T13:22:21Z","published":"2023-06-06T09:38:57Z","title":"Masked Autoencoders are Efficient Continual Federated Learners","summary":"  Machine learning is typically framed from a perspective of i.i.d., and more\nimportantly, isolated data. In parts, federated learning lifts this assumption,\nas it sets out to solve the real-world challenge of collaboratively learning a\nshared model from data distributed across clients. However, motivated primarily\nby privacy and computational constraints, the fact that data may change,\ndistributions drift, or even tasks advance individually on clients, is seldom\ntaken into account. The field of continual learning addresses this separate\nchallenge and first steps have recently been taken to leverage synergies in\ndistributed supervised settings, in which several clients learn to solve\nchanging classification tasks over time without forgetting previously seen\nones. Motivated by these prior works, we posit that such federated continual\nlearning should be grounded in unsupervised learning of representations that\nare shared across clients; in the loose spirit of how humans can indirectly\nleverage others' experience without exposure to a specific task. For this\npurpose, we demonstrate that masked autoencoders for distribution estimation\nare particularly amenable to this setup. Specifically, their masking strategy\ncan be seamlessly integrated with task attention mechanisms to enable selective\nknowledge transfer between clients. We empirically corroborate the latter\nstatement through several continual federated scenarios on both image and\nbinary datasets.\n","authors":["Subarnaduti Paul","Lars-Joel Frey","Roshni Kamath","Kristian Kersting","Martin Mundt"],"pdf_url":"https://arxiv.org/pdf/2306.03542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.08806v4","updated":"2024-07-18T13:02:56Z","published":"2021-12-16T11:42:45Z","title":"Correlation inference attacks against machine learning models","summary":"  Despite machine learning models being widely used today, the relationship\nbetween a model and its training dataset is not well understood. We explore\ncorrelation inference attacks, whether and when a model leaks information about\nthe correlations between the input variables of its training dataset. We first\npropose a model-less attack, where an adversary exploits the spherical\nparametrization of correlation matrices alone to make an informed guess.\nSecond, we propose a model-based attack, where an adversary exploits black-box\nmodel access to infer the correlations using minimal and realistic assumptions.\nThird, we evaluate our attacks against logistic regression and multilayer\nperceptron models on three tabular datasets and show the models to leak\ncorrelations. We finally show how extracted correlations can be used as\nbuilding blocks for attribute inference attacks and enable weaker adversaries.\nOur results raise fundamental questions on what a model does and should\nremember from its training set.\n","authors":["Ana-Maria Creţu","Florent Guépin","Yves-Alexandre de Montjoye"],"pdf_url":"https://arxiv.org/pdf/2112.08806v4.pdf","comment":"Published in Science Advances. This version contains both the main\n  paper and supplementary material. There are minor editorial differences\n  between this version and the published version. The first two authors\n  contributed equally"},{"id":"http://arxiv.org/abs/2310.07219v2","updated":"2024-07-18T12:55:29Z","published":"2023-10-11T06:09:48Z","title":"Improved Membership Inference Attacks Against Language Classification\n  Models","summary":"  Artificial intelligence systems are prevalent in everyday life, with use\ncases in retail, manufacturing, health, and many other fields. With the rise in\nAI adoption, associated risks have been identified, including privacy risks to\nthe people whose data was used to train models. Assessing the privacy risks of\nmachine learning models is crucial to enabling knowledgeable decisions on\nwhether to use, deploy, or share a model. A common approach to privacy risk\nassessment is to run one or more known attacks against the model and measure\ntheir success rate. We present a novel framework for running membership\ninference attacks against classification models. Our framework takes advantage\nof the ensemble method, generating many specialized attack models for different\nsubsets of the data. We show that this approach achieves higher accuracy than\neither a single attack model or an attack model per class label, both on\nclassical and language classification tasks.\n","authors":["Shlomit Shachor","Natalia Razinkov","Abigail Goldsteen"],"pdf_url":"https://arxiv.org/pdf/2310.07219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13466v1","updated":"2024-07-18T12:40:58Z","published":"2024-07-18T12:40:58Z","title":"LIMT: Language-Informed Multi-Task Visual World Models","summary":"  Most recent successes in robot reinforcement learning involve learning a\nspecialized single-task agent.\n  However, robots capable of performing multiple tasks can be much more\nvaluable in real-world applications.\n  Multi-task reinforcement learning can be very challenging due to the\nincreased sample complexity and the potentially conflicting task objectives.\n  Previous work on this topic is dominated by model-free approaches.\n  The latter can be very sample inefficient even when learning specialized\nsingle-task agents.\n  In this work, we focus on model-based multi-task reinforcement learning.\n  We propose a method for learning multi-task visual world models, leveraging\npre-trained language models to extract semantically meaningful task\nrepresentations.\n  These representations are used by the world model and policy to reason about\ntask similarity in dynamics and behavior.\n  Our results highlight the benefits of using language-driven task\nrepresentations for world models and a clear advantage of model-based\nmulti-task learning over the more common model-free paradigm.\n","authors":["Elie Aljalbout","Nikolaos Sotirakis","Patrick van der Smagt","Maximilian Karl","Nutan Chen"],"pdf_url":"https://arxiv.org/pdf/2407.13466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13460v1","updated":"2024-07-18T12:35:46Z","published":"2024-07-18T12:35:46Z","title":"SA-DVAE: Improving Zero-Shot Skeleton-Based Action Recognition by\n  Disentangled Variational Autoencoders","summary":"  Existing zero-shot skeleton-based action recognition methods utilize\nprojection networks to learn a shared latent space of skeleton features and\nsemantic embeddings. The inherent imbalance in action recognition datasets,\ncharacterized by variable skeleton sequences yet constant class labels,\npresents significant challenges for alignment. To address the imbalance, we\npropose SA-DVAE -- Semantic Alignment via Disentangled Variational\nAutoencoders, a method that first adopts feature disentanglement to separate\nskeleton features into two independent parts -- one is semantic-related and\nanother is irrelevant -- to better align skeleton and semantic features. We\nimplement this idea via a pair of modality-specific variational autoencoders\ncoupled with a total correction penalty. We conduct experiments on three\nbenchmark datasets: NTU RGB+D, NTU RGB+D 120 and PKU-MMD, and our experimental\nresults show that SA-DAVE produces improved performance over existing methods.\nThe code is available at https://github.com/pha123661/SA-DVAE.\n","authors":["Sheng-Wei Li","Zi-Xiang Wei","Wei-Jie Chen","Yi-Hsin Yu","Chih-Yuan Yang","Jane Yung-jen Hsu"],"pdf_url":"https://arxiv.org/pdf/2407.13460v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13449v1","updated":"2024-07-18T12:23:57Z","published":"2024-07-18T12:23:57Z","title":"All Roads Lead to Rome? Exploring Representational Similarities Between\n  Latent Spaces of Generative Image Models","summary":"  Do different generative image models secretly learn similar underlying\nrepresentations? We investigate this by measuring the latent space similarity\nof four different models: VAEs, GANs, Normalizing Flows (NFs), and Diffusion\nModels (DMs). Our methodology involves training linear maps between frozen\nlatent spaces to \"stitch\" arbitrary pairs of encoders and decoders and\nmeasuring output-based and probe-based metrics on the resulting \"stitched''\nmodels. Our main findings are that linear maps between latent spaces of\nperformant models preserve most visual information even when latent sizes\ndiffer; for CelebA models, gender is the most similarly represented probe-able\nattribute. Finally we show on an NF that latent space representations converge\nearly in training.\n","authors":["Charumathi Badrinath","Usha Bhalla","Alex Oesterling","Suraj Srinivas","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2407.13449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13435v1","updated":"2024-07-18T12:03:14Z","published":"2024-07-18T12:03:14Z","title":"Enhancing Out-of-Vocabulary Performance of Indian TTS Systems for\n  Practical Applications through Low-Effort Data Strategies","summary":"  Publicly available TTS datasets for low-resource languages like Hindi and\nTamil typically contain 10-20 hours of data, leading to poor vocabulary\ncoverage. This limitation becomes evident in downstream applications where\ndomain-specific vocabulary coupled with frequent code-mixing with English,\nresults in many OOV words. To highlight this problem, we create a benchmark\ncontaining OOV words from several real-world applications. Indeed,\nstate-of-the-art Hindi and Tamil TTS systems perform poorly on this OOV\nbenchmark, as indicated by intelligibility tests. To improve the model's OOV\nperformance, we propose a low-effort and economically viable strategy to obtain\nmore training data. Specifically, we propose using volunteers as opposed to\nhigh quality voice artists to record words containing character bigrams unseen\nin the training data. We show that using such inexpensive data, the model's\nperformance improves on OOV words, while not affecting voice quality and\nin-domain performance.\n","authors":["Srija Anand","Praveen Srinivasa Varadhan","Ashwin Sankar","Giri Raju","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2407.13435v1.pdf","comment":"Accepted at INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2407.13432v1","updated":"2024-07-18T12:01:09Z","published":"2024-07-18T12:01:09Z","title":"The Art of Imitation: Learning Long-Horizon Manipulation Tasks from Few\n  Demonstrations","summary":"  Task Parametrized Gaussian Mixture Models (TP-GMM) are a sample-efficient\nmethod for learning object-centric robot manipulation tasks. However, there are\nseveral open challenges to applying TP-GMMs in the wild. In this work, we\ntackle three crucial challenges synergistically. First, end-effector velocities\nare non-Euclidean and thus hard to model using standard GMMs. We thus propose\nto factorize the robot's end-effector velocity into its direction and\nmagnitude, and model them using Riemannian GMMs. Second, we leverage the\nfactorized velocities to segment and sequence skills from complex demonstration\ntrajectories. Through the segmentation, we further align skill trajectories and\nhence leverage time as a powerful inductive bias. Third, we present a method to\nautomatically detect relevant task parameters per skill from visual\nobservations. Our approach enables learning complex manipulation tasks from\njust five demonstrations while using only RGB-D observations. Extensive\nexperimental evaluations on RLBench demonstrate that our approach achieves\nstate-of-the-art performance with 20-fold improved sample efficiency. Our\npolicies generalize across different environments, object instances, and object\npositions, while the learned skills are reusable.\n","authors":["Jan Ole von Hartz","Tim Welschehold","Abhinav Valada","Joschka Boedecker"],"pdf_url":"https://arxiv.org/pdf/2407.13432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13431v1","updated":"2024-07-18T12:00:32Z","published":"2024-07-18T12:00:32Z","title":"Improving Out-of-Distribution Generalization of Trajectory Prediction\n  for Autonomous Driving via Polynomial Representations","summary":"  Robustness against Out-of-Distribution (OoD) samples is a key performance\nindicator of a trajectory prediction model. However, the development and\nranking of state-of-the-art (SotA) models are driven by their In-Distribution\n(ID) performance on individual competition datasets. We present an OoD testing\nprotocol that homogenizes datasets and prediction tasks across two large-scale\nmotion datasets. We introduce a novel prediction algorithm based on polynomial\nrepresentations for agent trajectory and road geometry on both the input and\noutput sides of the model. With a much smaller model size, training effort, and\ninference time, we reach near SotA performance for ID testing and significantly\nimprove robustness in OoD testing. Within our OoD testing protocol, we further\nstudy two augmentation strategies of SotA models and their effects on model\ngeneralization. Highlighting the contrast between ID and OoD performance, we\nsuggest adding OoD testing to the evaluation criteria of trajectory prediction\nmodels.\n","authors":["Yue Yao","Shengchao Yan","Daniel Goehring","Wolfram Burgard","Joerg Reichardt"],"pdf_url":"https://arxiv.org/pdf/2407.13431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13429v1","updated":"2024-07-18T11:54:34Z","published":"2024-07-18T11:54:34Z","title":"Towards Dynamic Feature Acquisition on Medical Time Series by Maximizing\n  Conditional Mutual Information","summary":"  Knowing which features of a multivariate time series to measure and when is a\nkey task in medicine, wearables, and robotics. Better acquisition policies can\nreduce costs while maintaining or even improving the performance of downstream\npredictors. Inspired by the maximization of conditional mutual information, we\npropose an approach to train acquirers end-to-end using only the downstream\nloss. We show that our method outperforms random acquisition policy, matches a\nmodel with an unrestrained budget, but does not yet overtake a static\nacquisition strategy. We highlight the assumptions and outline avenues for\nfuture work.\n","authors":["Fedor Sergeev","Paola Malsot","Gunnar Rätsch","Vincent Fortuin"],"pdf_url":"https://arxiv.org/pdf/2407.13429v1.pdf","comment":"Presented at the ICML 2024 Next Generation of Sequence Modeling\n  Architectures (NGSM) Workshop"},{"id":"http://arxiv.org/abs/2404.13235v2","updated":"2024-07-18T11:45:06Z","published":"2024-04-20T02:12:59Z","title":"TrialDura: Hierarchical Attention Transformer for Interpretable Clinical\n  Trial Duration Prediction","summary":"  The clinical trial process, a critical phase in drug development, is\nessential for developing new treatments. The primary goal of interventional\nclinical trials is to evaluate the safety and efficacy of drug-based treatments\nfor specific diseases. However, these trials are often lengthy,\nlabor-intensive, and expensive. The duration of a clinical trial significantly\nimpacts overall costs, making efficient timeline management crucial for\ncontrolling budgets and ensuring the economic feasibility of research. To\naddress this issue, We propose TrialDura, a machine learning-based method that\nestimates the duration of clinical trials using multimodal data, including\ndisease names, drug molecules, trial phases, and eligibility criteria. Then, we\nencode them into Bio-BERT embeddings specifically tuned for biomedical contexts\nto provide a deeper and more relevant semantic understanding of clinical trial\ndata. Finally, the model's hierarchical attention mechanism connects all of the\nembeddings to capture their interactions and predict clinical trial duration.\nOur proposed model demonstrated superior performance with a mean absolute error\n(MAE) of 1.04 years and a root mean square error (RMSE) of 1.39 years compared\nto the other models, indicating more accurate clinical trial duration\nprediction. Publicly available code can be found at:\nhttps://anonymous.4open.science/r/TrialDura-F196.\n","authors":["Ling Yue","Jonathan Li","Sixue Xing","Md Zabirul Islam","Bolun Xia","Tianfan Fu","Jintai Chen"],"pdf_url":"https://arxiv.org/pdf/2404.13235v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13420v1","updated":"2024-07-18T11:42:58Z","published":"2024-07-18T11:42:58Z","title":"Exploring End-to-end Differentiable Neural Charged Particle Tracking --\n  A Loss Landscape Perspective","summary":"  Measurement and analysis of high energetic particles for scientific, medical\nor industrial applications is a complex procedure, requiring the design of\nsophisticated detector and data processing systems. The development of adaptive\nand differentiable software pipelines using a combination of conventional and\nmachine learning algorithms is therefore getting ever more important to\noptimize and operate the system efficiently while maintaining end-to-end (E2E)\ndifferentiability. We propose for the application of charged particle tracking\nan E2E differentiable decision-focused learning scheme using graph neural\nnetworks with combinatorial components solving a linear assignment problem for\neach detector layer. We demonstrate empirically that including differentiable\nvariations of discrete assignment operations allows for efficient network\noptimization, working better or on par with approaches that lack E2E\ndifferentiability. In additional studies, we dive deeper into the optimization\nprocess and provide further insights from a loss landscape perspective. We\ndemonstrate that while both methods converge into similar performing, globally\nwell-connected regions, they suffer under substantial predictive instability\nacross initialization and optimization methods, which can have unpredictable\nconsequences on the performance of downstream tasks such as image\nreconstruction. We also point out a dependency between the interpolation factor\nof the gradient estimator and the prediction stability of the model, suggesting\nthe choice of sufficiently small values. Given the strong global connectivity\nof learned solutions and the excellent training performance, we argue that E2E\ndifferentiability provides, besides the general availability of gradient\ninformation, an important tool for robust particle tracking to mitigate\nprediction instabilities by favoring solutions that perform well on downstream\ntasks.\n","authors":["Tobias Kortus","Ralf Keidel","Nicolas R. Gauger"],"pdf_url":"https://arxiv.org/pdf/2407.13420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13419v1","updated":"2024-07-18T11:42:13Z","published":"2024-07-18T11:42:13Z","title":"From Words to Worlds: Compositionality for Cognitive Architectures","summary":"  Large language models (LLMs) are very performant connectionist systems, but\ndo they exhibit more compositionality? More importantly, is that part of why\nthey perform so well? We present empirical analyses across four LLM families\n(12 models) and three task categories, including a novel task introduced below.\nOur findings reveal a nuanced relationship in learning of compositional\nstrategies by LLMs -- while scaling enhances compositional abilities,\ninstruction tuning often has a reverse effect. Such disparity brings forth some\nopen issues regarding the development and improvement of large language models\nin alignment with human cognitive capacities.\n","authors":["Ruchira Dhar","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2407.13419v1.pdf","comment":"Accepted to ICML 2024 Workshop on LLMs & Cognition"},{"id":"http://arxiv.org/abs/2404.17113v4","updated":"2024-07-18T11:23:25Z","published":"2024-04-26T02:05:20Z","title":"MER 2024: Semi-Supervised Learning, Noise Robustness, and\n  Open-Vocabulary Multimodal Emotion Recognition","summary":"  Multimodal emotion recognition is an important research topic in artificial\nintelligence. Over the past few decades, researchers have made remarkable\nprogress by increasing the dataset size and building more effective algorithms.\nHowever, due to problems such as complex environments and inaccurate\nannotations, current systems are hard to meet the demands of practical\napplications. Therefore, we organize the MER series of competitions to promote\nthe development of this field. Last year, we launched MER2023, focusing on\nthree interesting topics: multi-label learning, noise robustness, and\nsemi-supervised learning. In this year's MER2024, besides expanding the dataset\nsize, we further introduce a new track around open-vocabulary emotion\nrecognition. The main purpose of this track is that existing datasets usually\nfix the label space and use majority voting to enhance the annotator\nconsistency. However, this process may lead to inaccurate annotations, such as\nignoring non-majority or non-candidate labels. In this track, we encourage\nparticipants to generate any number of labels in any category, aiming to\ndescribe emotional states as accurately as possible. Our baseline code relies\non MERTools and is available at:\nhttps://github.com/zeroQiaoba/MERTools/tree/master/MER2024.\n","authors":["Zheng Lian","Haiyang Sun","Licai Sun","Zhuofan Wen","Siyuan Zhang","Shun Chen","Hao Gu","Jinming Zhao","Ziyang Ma","Xie Chen","Jiangyan Yi","Rui Liu","Kele Xu","Bin Liu","Erik Cambria","Guoying Zhao","Björn W. Schuller","Jianhua Tao"],"pdf_url":"https://arxiv.org/pdf/2404.17113v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13399v1","updated":"2024-07-18T11:08:40Z","published":"2024-07-18T11:08:40Z","title":"Correcting the Mythos of KL-Regularization: Direct Alignment without\n  Overparameterization via Chi-squared Preference Optimization","summary":"  Language model alignment methods, such as reinforcement learning from human\nfeedback (RLHF), have led to impressive advances in language model\ncapabilities, but existing techniques are limited by a widely observed\nphenomenon known as overoptimization, where the quality of the language model\nplateaus or degrades over the course of the alignment process. Overoptimization\nis often attributed to overfitting to an inaccurate reward model, and while it\ncan be mitigated through online data collection, this is infeasible in many\nsettings. This raises a fundamental question: Do existing offline alignment\nalgorithms make the most of the data they have, or can their sample-efficiency\nbe improved further?\n  We address this question with a new algorithm for offline alignment,\n$\\chi^2$-Preference Optimization ($\\chi$PO). $\\chi$PO is a one-line change to\nDirect Preference Optimization (DPO; Rafailov et al., 2023), which only\ninvolves modifying the logarithmic link function in the DPO objective. Despite\nthis minimal change, $\\chi$PO implicitly implements the principle of pessimism\nin the face of uncertainty via regularization with the $\\chi^2$-divergence --\nwhich quantifies uncertainty more effectively than KL-regularization -- and\nprovably alleviates overoptimization, achieving sample-complexity guarantees\nbased on single-policy concentrability -- the gold standard in offline\nreinforcement learning. $\\chi$PO's simplicity and strong guarantees make it the\nfirst practical and general-purpose offline alignment algorithm that is\nprovably robust to overoptimization.\n","authors":["Audrey Huang","Wenhao Zhan","Tengyang Xie","Jason D. Lee","Wen Sun","Akshay Krishnamurthy","Dylan J. Foster"],"pdf_url":"https://arxiv.org/pdf/2407.13399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06600v2","updated":"2024-07-18T11:04:24Z","published":"2024-06-06T13:44:57Z","title":"HORAE: A Domain-Agnostic Modeling Language for Automating Multimodal\n  Service Regulation","summary":"  Artificial intelligence is rapidly encroaching on the field of service\nregulation. This work presents the design principles behind HORAE, a unified\nspecification language to model multimodal regulation rules across a diverse\nset of domains. We show how HORAE facilitates an intelligent service regulation\npipeline by further exploiting a fine-tuned large language model named HORAE\nthat automates the HORAE modeling process, thereby yielding an end-to-end\nframework for fully automated intelligent service regulation.\n","authors":["Yutao Sun","Mingshuai Chen","Tiancheng Zhao","Kangjia Zhao","He Li","Jintao Chen","Liqiang Lu","Xinkui Zhao","Shuiguang Deng","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2406.06600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00083v2","updated":"2024-07-18T11:01:46Z","published":"2023-11-30T07:16:11Z","title":"BAM-DETR: Boundary-Aligned Moment Detection Transformer for Temporal\n  Sentence Grounding in Videos","summary":"  Temporal sentence grounding aims to localize moments relevant to a language\ndescription. Recently, DETR-like approaches achieved notable progress by\npredicting the center and length of a target moment. However, they suffer from\nthe issue of center misalignment raised by the inherent ambiguity of moment\ncenters, leading to inaccurate predictions. To remedy this problem, we propose\na novel boundary-oriented moment formulation. In our paradigm, the model no\nlonger needs to find the precise center but instead suffices to predict any\nanchor point within the interval, from which the boundaries are directly\nestimated. Based on this idea, we design a boundary-aligned moment detection\ntransformer, equipped with a dual-pathway decoding process. Specifically, it\nrefines the anchor and boundaries within parallel pathways using global and\nboundary-focused attention, respectively. This separate design allows the model\nto focus on desirable regions, enabling precise refinement of moment\npredictions. Further, we propose a quality-based ranking method, ensuring that\nproposals with high localization qualities are prioritized over incomplete\nones. Experiments on three benchmarks validate the effectiveness of the\nproposed methods. The code is available at\nhttps://github.com/Pilhyeon/BAM-DETR.\n","authors":["Pilhyeon Lee","Hyeran Byun"],"pdf_url":"https://arxiv.org/pdf/2312.00083v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13382v1","updated":"2024-07-18T10:40:22Z","published":"2024-07-18T10:40:22Z","title":"Open-World Visual Reasoning by a Neuro-Symbolic Program of Zero-Shot\n  Symbols","summary":"  We consider the problem of finding spatial configurations of multiple objects\nin images, e.g., a mobile inspection robot is tasked to localize abandoned\ntools on the floor. We define the spatial configuration of objects by\nfirst-order logic in terms of relations and attributes. A neuro-symbolic\nprogram matches the logic formulas to probabilistic object proposals for the\ngiven image, provided by language-vision models by querying them for the\nsymbols. This work is the first to combine neuro-symbolic programming\n(reasoning) and language-vision models (learning) to find spatial\nconfigurations of objects in images in an open world setting. We show the\neffectiveness by finding abandoned tools on floors and leaking pipes. We find\nthat most prediction errors are due to biases in the language-vision model.\n","authors":["Gertjan Burghouts","Fieke Hillerström","Erwin Walraven","Michael van Bekkum","Frank Ruis","Joris Sijs","Jelle van Mil","Judith Dijk"],"pdf_url":"https://arxiv.org/pdf/2407.13382v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2407.08459v2","updated":"2024-07-18T10:33:35Z","published":"2024-07-11T12:58:07Z","title":"Graph Expansions of Deep Neural Networks and their Universal Scaling\n  Limits","summary":"  We present a unified approach to obtain scaling limits of neural networks\nusing the genus expansion technique from random matrix theory. This approach\nbegins with a novel expansion of neural networks which is reminiscent of\nButcher series for ODEs, and is obtained through a generalisation of Fa\\`a di\nBruno's formula to an arbitrary number of compositions. In this expansion, the\nrole of monomials is played by random multilinear maps indexed by directed\ngraphs whose edges correspond to random matrices, which we call operator\ngraphs. This expansion linearises the effect of the activation functions,\nallowing for the direct application of Wick's principle to compute the\nexpectation of each of its terms. We then determine the leading contribution to\neach term by embedding the corresponding graphs onto surfaces, and computing\ntheir Euler characteristic. Furthermore, by developing a correspondence between\nanalytic and graphical operations, we obtain similar graph expansions for the\nneural tangent kernel as well as the input-output Jacobian of the original\nneural network, and derive their infinite-width limits with relative ease.\nNotably, we find explicit formulae for the moments of the limiting singular\nvalue distribution of the Jacobian. We then show that all of these results hold\nfor networks with more general weights, such as general matrices with i.i.d.\nentries satisfying moment assumptions, complex matrices and sparse matrices.\n","authors":["Nicola Muca Cirone","Jad Hamdan","Cristopher Salvi"],"pdf_url":"https://arxiv.org/pdf/2407.08459v2.pdf","comment":"v2: added acknowledgements paragraph"},{"id":"http://arxiv.org/abs/2402.01832v2","updated":"2024-07-18T10:21:29Z","published":"2024-02-02T18:59:58Z","title":"SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?","summary":"  We present SynthCLIP, a CLIP model trained on entirely synthetic text-image\npairs. Leveraging recent text-to-image (TTI) networks and large language models\n(LLM), we generate synthetic datasets of images and corresponding captions at\nscale, with no human intervention. In this work, we provide an analysis on CLIP\nmodels trained on synthetic data. We provide insights on the data generation\nstrategy, number of samples required, scaling trends, and resulting properties.\nWe also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million\ncaptioned images. Our code, trained models, and data, are released as open\nsource at https://github.com/hammoudhasan/SynthCLIP\n","authors":["Hasan Abed Al Kader Hammoud","Hani Itani","Fabio Pizzati","Philip Torr","Adel Bibi","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2402.01832v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2406.14469v3","updated":"2024-07-18T10:19:49Z","published":"2024-06-20T16:32:18Z","title":"Fusion of Movement and Naive Predictions for Point Forecasting in\n  Univariate Random Walks","summary":"  Traditional methods for point forecasting in univariate random walks often\nfail to surpass naive benchmarks due to data unpredictability. This study\nintroduces a novel forecasting method that fuses movement prediction (binary\nclassification) with naive forecasts for accurate one-step-ahead point\nforecasting in univariate random walks. The method's efficacy is demonstrated\nthrough theoretical analysis, simulations, and real-world data experiments. It\nreliably outperforms naive forecasts with moderate movement prediction\naccuracies, such as 0.55, and is superior to baseline models such as the ARIMA,\nlinear regression, MLP, and LSTM networks in forecasting the S&P 500 index and\nBitcoin prices. This method is particularly advantageous when accurate point\npredictions are challenging but accurate movement predictions are attainable,\ntranslating movement predictions into point forecasts in random walk contexts.\n","authors":["Cheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.14469v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13364v1","updated":"2024-07-18T10:15:51Z","published":"2024-07-18T10:15:51Z","title":"Geometric Active Exploration in Markov Decision Processes: the Benefit\n  of Abstraction","summary":"  How can a scientist use a Reinforcement Learning (RL) algorithm to design\nexperiments over a dynamical system's state space? In the case of finite and\nMarkovian systems, an area called Active Exploration (AE) relaxes the\noptimization problem of experiments design into Convex RL, a generalization of\nRL admitting a wider notion of reward. Unfortunately, this framework is\ncurrently not scalable and the potential of AE is hindered by the vastness of\nexperiment spaces typical of scientific discovery applications. However, these\nspaces are often endowed with natural geometries, e.g., permutation invariance\nin molecular design, that an agent could leverage to improve the statistical\nand computational efficiency of AE. To achieve this, we bridge AE and MDP\nhomomorphisms, which offer a way to exploit known geometric structures via\nabstraction. Towards this goal, we make two fundamental contributions: we\nextend MDP homomorphisms formalism to Convex RL, and we present, to the best of\nour knowledge, the first analysis that formally captures the benefit of\nabstraction via homomorphisms on sample efficiency. Ultimately, we propose the\nGeometric Active Exploration (GAE) algorithm, which we analyse theoretically\nand experimentally in environments motivated by problems in scientific\ndiscovery.\n","authors":["Riccardo De Santi","Federico Arangath Joseph","Noah Liniger","Mirco Mutti","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2407.13364v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2403.06870v3","updated":"2024-07-18T10:09:06Z","published":"2024-03-11T16:23:38Z","title":"Semantic Residual Prompts for Continual Learning","summary":"  Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained\nmodel and train a few parameter vectors termed prompts. Most of these methods\norganize these vectors in a pool of key-value pairs and use the input image as\nquery to retrieve the prompts (values). However, as keys are learned while\ntasks progress, the prompting selection strategy is itself subject to\ncatastrophic forgetting, an issue often overlooked by existing approaches. For\ninstance, prompts introduced to accommodate new tasks might end up interfering\nwith previously learned prompts. To make the selection strategy more stable, we\nleverage a foundation model (CLIP) to select our prompts within a two-level\nadaptation mechanism. Specifically, the first level leverages a standard\ntextual prompt pool for the CLIP textual encoder, leading to stable class\nprototypes. The second level, instead, uses these prototypes along with the\nquery image as keys to index a second pool. The retrieved prompts serve to\nadapt a pre-trained ViT, granting plasticity. In doing so, we also propose a\nnovel residual mechanism to transfer CLIP semantics to the ViT layers. Through\nextensive analysis on established CL benchmarks, we show that our method\nsignificantly outperforms both state-of-the-art CL approaches and the zero-shot\nCLIP test. Notably, our findings hold true even for datasets with a substantial\ndomain gap w.r.t. the pre-training knowledge of the backbone model, as\nshowcased by experiments on satellite imagery and medical datasets. The\ncodebase is available at https://github.com/aimagelab/mammoth.\n","authors":["Martin Menabue","Emanuele Frascaroli","Matteo Boschini","Enver Sangineto","Lorenzo Bonicelli","Angelo Porrello","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2403.06870v3.pdf","comment":"25 pages, 5 figures, Accepted at 18th European Conference on Computer\n  Vision (ECCV 2024), Milan, Italy"},{"id":"http://arxiv.org/abs/2407.13358v1","updated":"2024-07-18T10:01:09Z","published":"2024-07-18T10:01:09Z","title":"Capturing Style in Author and Document Representation","summary":"  A wide range of Deep Natural Language Processing (NLP) models integrates\ncontinuous and low dimensional representations of words and documents.\nSurprisingly, very few models study representation learning for authors. These\nrepresentations can be used for many NLP tasks, such as author identification\nand classification, or in recommendation systems. A strong limitation of\nexisting works is that they do not explicitly capture writing style, making\nthem hardly applicable to literary data. We therefore propose a new\narchitecture based on Variational Information Bottleneck (VIB) that learns\nembeddings for both authors and documents with a stylistic constraint. Our\nmodel fine-tunes a pre-trained document encoder. We stimulate the detection of\nwriting style by adding predefined stylistic features making the representation\naxis interpretable with respect to writing style indicators. We evaluate our\nmethod on three datasets: a literary corpus extracted from the Gutenberg\nProject, the Blog Authorship Corpus and IMDb62, for which we show that it\nmatches or outperforms strong/recent baselines in authorship attribution while\ncapturing much more accurately the authors stylistic aspects.\n","authors":["Enzo Terreau","Antoine Gourru","Julien Velcin"],"pdf_url":"https://arxiv.org/pdf/2407.13358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18286v2","updated":"2024-07-18T09:58:03Z","published":"2024-02-28T12:25:01Z","title":"Self-Supervised Learning with Generative Adversarial Networks for\n  Electron Microscopy","summary":"  In this work, we explore the potential of self-supervised learning with\nGenerative Adversarial Networks (GANs) for electron microscopy datasets. We\nshow how self-supervised pretraining facilitates efficient fine-tuning for a\nspectrum of downstream tasks, including semantic segmentation, denoising, noise\n\\& background removal, and super-resolution. Experimentation with varying model\ncomplexities and receptive field sizes reveals the remarkable phenomenon that\nfine-tuned models of lower complexity consistently outperform more complex\nmodels with random weight initialization. We demonstrate the versatility of\nself-supervised pretraining across various downstream tasks in the context of\nelectron microscopy, allowing faster convergence and better performance. We\nconclude that self-supervised pretraining serves as a powerful catalyst, being\nespecially advantageous when limited annotated data are available and efficient\nscaling of computational cost is important.\n","authors":["Bashir Kazimi","Karina Ruzaeva","Stefan Sandfeld"],"pdf_url":"https://arxiv.org/pdf/2402.18286v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18507v3","updated":"2024-07-18T09:55:32Z","published":"2024-05-28T18:24:16Z","title":"Injecting Hierarchical Biological Priors into Graph Neural Networks for\n  Flow Cytometry Prediction","summary":"  In the complex landscape of hematologic samples such as peripheral blood or\nbone marrow derived from flow cytometry (FC) data, cell-level prediction\npresents profound challenges. This work explores injecting hierarchical prior\nknowledge into graph neural networks (GNNs) for single-cell multi-class\nclassification of tabular cellular data. By representing the data as graphs and\nencoding hierarchical relationships between classes, we propose our\nhierarchical plug-in method to be applied to several GNN models, namely,\nFCHC-GNN, and effectively designed to capture neighborhood information crucial\nfor single-cell FC domain. Extensive experiments on our cohort of 19 distinct\npatients, demonstrate that incorporating hierarchical biological constraints\nboosts performance significantly across multiple metrics compared to baseline\nGNNs without such priors. The proposed approach highlights the importance of\nstructured inductive biases for gaining improved generalization in complex\nbiological prediction tasks.\n","authors":["Fatemeh Nassajian Mojarrad","Lorenzo Bini","Thomas Matthes","Stéphane Marchand-Maillet"],"pdf_url":"https://arxiv.org/pdf/2405.18507v3.pdf","comment":"14 pages, ICML Conference Workshop 2024. arXiv admin note: text\n  overlap with arXiv:2402.18610"},{"id":"http://arxiv.org/abs/2302.03391v2","updated":"2024-07-18T09:44:18Z","published":"2023-02-07T10:52:04Z","title":"Sparse and geometry-aware generalisation of the mutual information for\n  joint discriminative clustering and feature selection","summary":"  Feature selection in clustering is a hard task which involves simultaneously\nthe discovery of relevant clusters as well as relevant variables with respect\nto these clusters. While feature selection algorithms are often model-based\nthrough optimised model selection or strong assumptions on the data\ndistribution, we introduce a discriminative clustering model trying to maximise\na geometry-aware generalisation of the mutual information called GEMINI with a\nsimple l1 penalty: the Sparse GEMINI. This algorithm avoids the burden of\ncombinatorial feature subset exploration and is easily scalable to\nhigh-dimensional data and large amounts of samples while only designing a\ndiscriminative clustering model. We demonstrate the performances of Sparse\nGEMINI on synthetic datasets and large-scale datasets. Our results show that\nSparse GEMINI is a competitive algorithm and has the ability to select relevant\nsubsets of variables with respect to the clustering without using relevance\ncriteria or prior hypotheses.\n","authors":["Louis Ohl","Pierre-Alexandre Mattei","Charles Bouveyron","Mickaël Leclercq","Arnaud Droit","Frédéric Precioso"],"pdf_url":"https://arxiv.org/pdf/2302.03391v2.pdf","comment":"Published in Statistics and Computing, Volume 34, article number 155,\n  (2024), https://doi.org/10.1007/s11222-024-10467-9"},{"id":"http://arxiv.org/abs/2407.13331v1","updated":"2024-07-18T09:30:44Z","published":"2024-07-18T09:30:44Z","title":"Reconstruct the Pruned Model without Any Retraining","summary":"  Structured pruning is a promising hardware-friendly compression technique for\nlarge language models (LLMs), which is expected to be retraining-free to avoid\nthe enormous retraining cost. This retraining-free paradigm involves (1)\npruning criteria to define the architecture and (2) distortion reconstruction\nto restore performance. However, existing methods often emphasize pruning\ncriteria while using reconstruction techniques that are specific to certain\nmodules or criteria, resulting in limited generalizability. To address this, we\nintroduce the Linear Interpolation-based Adaptive Reconstruction (LIAR)\nframework, which is both efficient and effective. LIAR does not require\nback-propagation or retraining and is compatible with various pruning criteria\nand modules. By applying linear interpolation to the preserved weights, LIAR\nminimizes reconstruction error and effectively reconstructs the pruned output.\nOur evaluations on benchmarks such as GLUE, SQuAD, WikiText, and common sense\nreasoning show that LIAR enables a BERT model to maintain 98% accuracy even\nafter removing 50% of its parameters and achieves top performance for LLaMA in\njust a few minutes.\n","authors":["Pingjie Wang","Ziqing Fan","Shengchao Hu","Zhe Chen","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13331v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2407.12415v2","updated":"2024-07-18T09:27:25Z","published":"2024-07-17T08:54:41Z","title":"Not All Frequencies Are Created Equal:Towards a Dynamic Fusion of\n  Frequencies in Time-Series Forecasting","summary":"  Long-term time series forecasting is a long-standing challenge in various\napplications. A central issue in time series forecasting is that methods should\nexpressively capture long-term dependency. Furthermore, time series forecasting\nmethods should be flexible when applied to different scenarios. Although\nFourier analysis offers an alternative to effectively capture reusable and\nperiodic patterns to achieve long-term forecasting in different scenarios,\nexisting methods often assume high-frequency components represent noise and\nshould be discarded in time series forecasting. However, we conduct a series of\nmotivation experiments and discover that the role of certain frequencies varies\ndepending on the scenarios. In some scenarios, removing high-frequency\ncomponents from the original time series can improve the forecasting\nperformance, while in others scenarios, removing them is harmful to forecasting\nperformance. Therefore, it is necessary to treat the frequencies differently\naccording to specific scenarios. To achieve this, we first reformulate the time\nseries forecasting problem as learning a transfer function of each frequency in\nthe Fourier domain. Further, we design Frequency Dynamic Fusion (FreDF), which\nindividually predicts each Fourier component, and dynamically fuses the output\nof different frequencies. Moreover, we provide a novel insight into the\ngeneralization ability of time series forecasting and propose the\ngeneralization bound of time series forecasting. Then we prove FreDF has a\nlower bound, indicating that FreDF has better generalization ability. Extensive\nexperiments conducted on multiple benchmark datasets and ablation studies\ndemonstrate the effectiveness of FreDF.\n","authors":["Xingyu Zhang","Siyu Zhao","Zeen Song","Huijie Guo","Jianqi Zhang","Changwen Zheng","Wenwen Qiang"],"pdf_url":"https://arxiv.org/pdf/2407.12415v2.pdf","comment":"Accpeted by ACMMM2024"},{"id":"http://arxiv.org/abs/2402.15113v2","updated":"2024-07-18T09:26:40Z","published":"2024-02-23T05:57:22Z","title":"MSPipe: Efficient Temporal GNN Training via Staleness-Aware Pipeline","summary":"  Memory-based Temporal Graph Neural Networks (MTGNNs) are a class of temporal\ngraph neural networks that utilize a node memory module to capture and retain\nlong-term temporal dependencies, leading to superior performance compared to\nmemory-less counterparts. However, the iterative reading and updating process\nof the memory module in MTGNNs to obtain up-to-date information needs to follow\nthe temporal dependencies. This introduces significant overhead and limits\ntraining throughput. Existing optimizations for static GNNs are not directly\napplicable to MTGNNs due to differences in training paradigm, model\narchitecture, and the absence of a memory module. Moreover, they do not\neffectively address the challenges posed by temporal dependencies, making them\nineffective for MTGNN training. In this paper, we propose MSPipe, a general and\nefficient framework for MTGNNs that maximizes training throughput while\nmaintaining model accuracy. Our design addresses the unique challenges\nassociated with fetching and updating node memory states in MTGNNs by\nintegrating staleness into the memory module. However, simply introducing a\npredefined staleness bound in the memory module to break temporal dependencies\nmay lead to suboptimal performance and lack of generalizability across\ndifferent models and datasets. To solve this, we introduce an online pipeline\nscheduling algorithm in MSPipe that strategically breaks temporal dependencies\nwith minimal staleness and delays memory fetching to obtain fresher memory\nstates. Moreover, we design a staleness mitigation mechanism to enhance\ntraining convergence and model accuracy. We provide convergence analysis and\nprove that MSPipe maintains the same convergence rate as vanilla sample-based\nGNN training. Experimental results show that MSPipe achieves up to 2.45x\nspeed-up without sacrificing accuracy, making it a promising solution for\nefficient MTGNN training.\n","authors":["Guangming Sheng","Junwei Su","Chao Huang","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2402.15113v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13326v1","updated":"2024-07-18T09:26:07Z","published":"2024-07-18T09:26:07Z","title":"RISC-V RVV efficiency for ANN algorithms","summary":"  Handling vast amounts of data is crucial in today's world. The growth of\nhigh-performance computing has created a need for parallelization, particularly\nin the area of machine learning algorithms such as ANN (Approximate Nearest\nNeighbors). To improve the speed of these algorithms, it is important to\noptimize them for specific processor architectures. RISC-V (Reduced Instruction\nSet Computer Five) is one of the modern processor architectures, which features\na vector instruction set called RVV (RISC-V Vector Extension). In machine\nlearning algorithms, vector extensions are widely utilized to improve the\nprocessing of voluminous data. This study examines the effectiveness of\napplying RVV to commonly used ANN algorithms. The algorithms were adapted for\nRISC-V and optimized using RVV after identifying the primary bottlenecks.\nAdditionally, we developed a theoretical model of a parameterized vector block\nand identified the best on average configuration that demonstrates the highest\ntheoretical performance of the studied ANN algorithms when the other CPU\nparameters are fixed.\n","authors":["Konstantin Rumyantsev","Pavel Yakovlev","Andrey Gorshkov","Andrey P. Sokolov"],"pdf_url":"https://arxiv.org/pdf/2407.13326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13316v1","updated":"2024-07-18T09:17:47Z","published":"2024-07-18T09:17:47Z","title":"Deterministic Trajectory Optimization through Probabilistic Optimal\n  Control","summary":"  This article proposes two new algorithms tailored to discrete-time\ndeterministic finite-horizon nonlinear optimal control problems or so-called\ntrajectory optimization problems. Both algorithms are inspired by a novel\ntheoretical paradigm known as probabilistic optimal control, that reformulates\noptimal control as an equivalent probabilistic inference problem. This\nperspective allows to address the problem using the Expectation-Maximization\nalgorithm. We show that the application of this algorithm results in a fixed\npoint iteration of probabilistic policies that converge to the deterministic\noptimal policy. Two strategies for policy evaluation are discussed, using\nstate-of-the-art uncertainty quantification methods resulting into two distinct\nalgorithms. The algorithms are structurally closest related to the differential\ndynamic programming algorithm and related methods that use sigma-point methods\nto avoid direct gradient evaluations. The main advantage of our work is an\nimproved balance between exploration and exploitation over the iterations,\nleading to improved numerical stability and accelerated convergence. These\nproperties are demonstrated on different nonlinear systems.\n","authors":["Mohammad Mahmoudi Filabadi","Tom Lefebvre","Guillaume Crevecoeur"],"pdf_url":"https://arxiv.org/pdf/2407.13316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13808v3","updated":"2024-07-18T09:15:00Z","published":"2024-03-20T17:59:58Z","title":"On Pretraining Data Diversity for Self-Supervised Learning","summary":"  We explore the impact of training with more diverse datasets, characterized\nby the number of unique samples, on the performance of self-supervised learning\n(SSL) under a fixed computational budget. Our findings consistently demonstrate\nthat increasing pretraining data diversity enhances SSL performance, albeit\nonly when the distribution distance to the downstream data is minimal. Notably,\neven with an exceptionally large pretraining data diversity achieved through\nmethods like web crawling or diffusion-generated data, among other ways, the\ndistribution shift remains a challenge. Our experiments are comprehensive with\nseven SSL methods using large-scale datasets such as ImageNet and YFCC100M\namounting to over 200 GPU days. Code and trained models are available at\nhttps://github.com/hammoudhasan/DiversitySSL\n","authors":["Hasan Abed Al Kader Hammoud","Tuhin Das","Fabio Pizzati","Philip Torr","Adel Bibi","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2403.13808v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2308.12919v2","updated":"2024-07-18T09:14:13Z","published":"2023-08-24T16:47:17Z","title":"Realistic Unsupervised CLIP Fine-tuning with Universal Entropy\n  Optimization","summary":"  The emergence of vision-language models, such as CLIP, has spurred a\nsignificant research effort towards their application for downstream supervised\nlearning tasks. Although some previous studies have explored the unsupervised\nfine-tuning of CLIP, they often rely on prior knowledge in the form of class\nnames associated with ground truth labels. This paper explores a realistic\nunsupervised fine-tuning scenario, considering the presence of\nout-of-distribution samples from unknown classes within the unlabeled data. In\nparticular, we focus on simultaneously enhancing out-of-distribution detection\nand the recognition of instances associated with known classes. To tackle this\nproblem, we present a simple, efficient, and effective approach called\nUniversal Entropy Optimization (UEO). UEO leverages sample-level confidence to\napproximately minimize the conditional entropy of confident instances and\nmaximize the marginal entropy of less confident instances. Apart from\noptimizing the textual prompt, UEO incorporates optimization of channel-wise\naffine transformations within the visual branch of CLIP. Extensive experiments\nacross 15 domains and 4 different types of prior knowledge validate the\neffectiveness of UEO compared to baseline methods. The code is publicly\navailable at \\url{https://github.com/tim-learn/UEO}.\n","authors":["Jian Liang","Lijun Sheng","Zhengbo Wang","Ran He","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2308.12919v2.pdf","comment":"ICML 2024 Highlight"},{"id":"http://arxiv.org/abs/2407.13310v1","updated":"2024-07-18T09:13:22Z","published":"2024-07-18T09:13:22Z","title":"A deep latent variable model for semi-supervised multi-unit soft sensing\n  in industrial processes","summary":"  In many industrial processes, an apparent lack of data limits the development\nof data-driven soft sensors. There are, however, often opportunities to learn\nstronger models by being more data-efficient. To achieve this, one can leverage\nknowledge about the data from which the soft sensor is learned. Taking\nadvantage of properties frequently possessed by industrial data, we introduce a\ndeep latent variable model for semi-supervised multi-unit soft sensing. This\nhierarchical, generative model is able to jointly model different units, as\nwell as learning from both labeled and unlabeled data.\n  An empirical study of multi-unit soft sensing is conducted using two\ndatasets: a synthetic dataset of single-phase fluid flow, and a large, real\ndataset of multi-phase flow in oil and gas wells. We show that by combining\nsemi-supervised and multi-task learning, the proposed model achieves superior\nresults, outperforming current leading methods for this soft sensing problem.\nWe also show that when a model has been trained on a multi-unit dataset, it may\nbe finetuned to previously unseen units using only a handful of data points. In\nthis finetuning procedure, unlabeled data improve soft sensor performance;\nremarkably, this is true even when no labeled data are available.\n","authors":["Bjarne Grimstad","Kristian Løvland","Lars S. Imsland","Vidar Gunnerud"],"pdf_url":"https://arxiv.org/pdf/2407.13310v1.pdf","comment":"30 pages, 11 figures"},{"id":"http://arxiv.org/abs/2306.14088v2","updated":"2024-07-18T09:12:20Z","published":"2023-06-25T01:31:54Z","title":"Private Aggregation in Hierarchical Wireless Federated Learning with\n  Partial and Full Collusion","summary":"  In federated learning, a federator coordinates the training of a model, e.g.,\na neural network, on privately owned data held by several participating\nclients. The gradient descent algorithm, a well-known and popular iterative\noptimization procedure, is run to train the model. Every client computes\npartial gradients based on their local data and sends them to the federator,\nwhich aggregates the results and updates the model. Privacy of the clients'\ndata is a major concern. In fact, it is shown that observing the partial\ngradients can be enough to reveal the clients' data. Existing literature\nfocuses on private aggregation schemes that tackle the privacy problem in\nfederated learning in settings where all users are connected to each other and\nto the federator. In this paper, we consider a hierarchical wireless system\narchitecture in which the clients are connected to base stations; the base\nstations are connected to the federator either directly or through relays. We\nexamine settings with and without relays, and derive fundamental limits on the\ncommunication cost under information-theoretic privacy with different collusion\nassumptions. We introduce suitable private aggregation schemes tailored for\nthese settings whose communication costs are multiplicative factors away from\nthe derived bounds.\n","authors":["Maximilian Egger","Christoph Hofmeister","Antonia Wachter-Zeh","Rawad Bitar"],"pdf_url":"https://arxiv.org/pdf/2306.14088v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13082v2","updated":"2024-07-18T09:12:08Z","published":"2024-05-21T06:44:40Z","title":"A Survey of Artificial Intelligence in Gait-Based Neurodegenerative\n  Disease Diagnosis","summary":"  Recent years have witnessed an increasing global population affected by\nneurodegenerative diseases (NDs), which traditionally require extensive\nhealthcare resources and human effort for medical diagnosis and monitoring. As\na crucial disease-related motor symptom, human gait can be exploited to\ncharacterize different NDs. The current advances in artificial intelligence\n(AI) models enable automatic gait analysis for NDs identification and\nclassification, opening a new avenue to facilitate faster and more\ncost-effective diagnosis of NDs. In this paper, we provide a comprehensive\nsurvey on recent progress of machine learning and deep learning based AI\ntechniques applied to diagnosis of five typical NDs through gait. We provide an\noverview of the process of AI-assisted NDs diagnosis, and present a systematic\ntaxonomy of existing gait data and AI models. Meanwhile, a novel quality\nevaluation criterion is proposed to quantitatively assess the quality of\nexisting studies. Through an extensive review and analysis of 164 studies, we\nidentify and discuss the challenges, potential solutions, and future directions\nin this field. Finally, we envision the prospective utilization of 3D skeleton\ndata for human gait representation and the development of more efficient AI\nmodels for NDs diagnosis. We provide a public resource repository to track and\nfacilitate developments in this emerging field:\nhttps://github.com/Kali-Hac/AI4NDD-Survey.\n","authors":["Haocong Rao","Minlin Zeng","Xuejiao Zhao","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2405.13082v2.pdf","comment":"Article: 46 pages, 9 figures, 7 tables, citing 274 papers. Appendix:\n  29 pages, 1 figure, 5 tables. A up-to-date resource (papers, data, etc.) of\n  this survey (AI4NDD) is provided at https://github.com/Kali-Hac/AI4NDD-Survey"},{"id":"http://arxiv.org/abs/2208.11010v6","updated":"2024-07-18T09:10:11Z","published":"2022-08-23T14:46:54Z","title":"Convex mixed-integer optimization with Frank-Wolfe methods","summary":"  Mixed-integer nonlinear optimization encompasses a broad class of problems\nthat present both theoretical and computational challenges. We propose a new\ntype of method to solve these problems based on a branch-and-bound algorithm\nwith convex node relaxations. These relaxations are solved with a Frank-Wolfe\nalgorithm over the convex hull of mixed-integer feasible points instead of the\ncontinuous relaxation via calls to a mixed-integer linear solver as the linear\nminimization oracle. The proposed method computes feasible solutions while\nworking on a single representation of the polyhedral constraints, leveraging\nthe full extent of mixed-integer linear solvers without an outer approximation\nscheme and can exploit inexact solutions of node subproblems.\n","authors":["Deborah Hendrych","Hannah Troppens","Mathieu Besançon","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2208.11010v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13303v1","updated":"2024-07-18T09:07:20Z","published":"2024-07-18T09:07:20Z","title":"Mean Teacher based SSL Framework for Indoor Localization Using Wi-Fi\n  RSSI Fingerprinting","summary":"  Wi-Fi fingerprinting is widely applied for indoor localization due to the\nwidespread availability of Wi-Fi devices. However, traditional methods are not\nideal for multi-building and multi-floor environments due to the scalability\nissues. Therefore, more and more researchers have employed deep learning\ntechniques to enable scalable indoor localization. This paper introduces a\nnovel semi-supervised learning framework for neural networks based on wireless\naccess point selection, noise injection, and Mean Teacher model, which\nleverages unlabeled fingerprints to enhance localization performance. The\nproposed framework can manage hybrid in/outsourcing and voluntarily contributed\ndatabases and continually expand the fingerprint database with newly submitted\nunlabeled fingerprints during service. The viability of the proposed framework\nwas examined using two established deep-learning models with the UJIIndoorLoc\ndatabase. The experimental results suggest that the proposed framework\nsignificantly improves localization performance compared to the supervised\nlearning-based approach in terms of floor-level coordinate estimation using\nEvAAL metric. It shows enhancements up to 10.99% and 8.98% in the former\nscenario and 4.25% and 9.35% in the latter, respectively with additional\nstudies highlight the importance of the essential components of the proposed\nframework.\n","authors":["Sihao Li","Zhe Tang","Kyeong Soo Kim","Jeremy S. Smith"],"pdf_url":"https://arxiv.org/pdf/2407.13303v1.pdf","comment":"12 pages, 10 figures, under preparation for a journal publication"},{"id":"http://arxiv.org/abs/2407.13301v1","updated":"2024-07-18T09:06:27Z","published":"2024-07-18T09:06:27Z","title":"CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis","summary":"  The field of medical diagnosis has undergone a significant transformation\nwith the advent of large language models (LLMs), yet the challenges of\ninterpretability within these models remain largely unaddressed. This study\nintroduces Chain-of-Diagnosis (CoD) to enhance the interpretability of\nLLM-based medical diagnostics. CoD transforms the diagnostic process into a\ndiagnostic chain that mirrors a physician's thought process, providing a\ntransparent reasoning pathway. Additionally, CoD outputs the disease confidence\ndistribution to ensure transparency in decision-making. This interpretability\nmakes model diagnostics controllable and aids in identifying critical symptoms\nfor inquiry through the entropy reduction of confidences. With CoD, we\ndeveloped DiagnosisGPT, capable of diagnosing 9604 diseases. Experimental\nresults demonstrate that DiagnosisGPT outperforms other LLMs on diagnostic\nbenchmarks. Moreover, DiagnosisGPT provides interpretability while ensuring\ncontrollability in diagnostic rigor.\n","authors":["Junying Chen","Chi Gui","Anningzhe Gao","Ke Ji","Xidong Wang","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13542v3","updated":"2024-07-18T09:00:23Z","published":"2024-06-19T13:29:53Z","title":"Self-play with Execution Feedback: Improving Instruction-following\n  Capabilities of Large Language Models","summary":"  One core capability of large language models (LLMs) is to follow natural\nlanguage instructions. However, the issue of automatically constructing\nhigh-quality training data to enhance the complex instruction-following\nabilities of LLMs without manual annotation remains unresolved. In this paper,\nwe introduce AutoIF, the first scalable and reliable method for automatically\ngenerating instruction-following training data. AutoIF transforms the\nvalidation of instruction-following data quality into code verification,\nrequiring LLMs to generate instructions, the corresponding code to check the\ncorrectness of the instruction responses, and unit test samples to verify the\ncode's correctness. Then, execution feedback-based rejection sampling can\ngenerate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from\nHuman Feedback (RLHF) training. AutoIF achieves significant improvements across\nthree training algorithms, SFT, Offline DPO, and Online DPO, when applied to\nthe top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and\nstrong-to-weak distillation settings. Our code is publicly available at\nhttps://github.com/QwenLM/AutoIF.\n","authors":["Guanting Dong","Keming Lu","Chengpeng Li","Tingyu Xia","Bowen Yu","Chang Zhou","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.13542v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2401.09274v3","updated":"2024-07-18T08:59:47Z","published":"2024-01-17T15:25:50Z","title":"Avoiding strict saddle points of nonconvex regularized problems","summary":"  In this paper, we consider a class of non-convex and non-smooth sparse\noptimization problems, which encompass most existing nonconvex\nsparsity-inducing terms. We show the second-order optimality conditions only\ndepend on the nonzeros of the stationary points. We propose two damped\niterative reweighted algorithms including the iteratively reweighted $\\ell_1$\nalgorithm (DIRL$_1$) and the iteratively reweighted $\\ell_2$ (DIRL$_2$)\nalgorithm, to solve these problems. For DIRL$_1$, we show the reweighted\n$\\ell_1$ subproblem has support identification property so that DIRL$_1$\nlocally reverts to a gradient descent algorithm around a stationary point. For\nDIRL$_2$, we show the solution map of the reweighted $\\ell_2$ subproblem is\ndifferentiable and Lipschitz continuous everywhere. Therefore, the map of\nDIRL$_1$ and DIRL$_2$ and their inverse are Lipschitz continuous, and the\nstrict saddle points are their unstable fixed points. By applying the stable\nmanifold theorem, these algorithms are shown to converge only to local\nminimizers with randomly initialization when the strictly saddle point property\nis assumed.\n","authors":["Luwei Bai","Yaohua Hu","Hao Wang","Xiaoqi Yang"],"pdf_url":"https://arxiv.org/pdf/2401.09274v3.pdf","comment":"34 pages,4 figures"},{"id":"http://arxiv.org/abs/2308.02121v3","updated":"2024-07-18T08:53:10Z","published":"2023-08-04T03:46:41Z","title":"Model Provenance via Model DNA","summary":"  Understanding the life cycle of the machine learning (ML) model is an\nintriguing area of research (e.g., understanding where the model comes from,\nhow it is trained, and how it is used). This paper focuses on a novel problem\nwithin this field, namely Model Provenance (MP), which concerns the\nrelationship between a target model and its pre-training model and aims to\ndetermine whether a source model serves as the provenance for a target model.\nThis is an important problem that has significant implications for ensuring the\nsecurity and intellectual property of machine learning models but has not\nreceived much attention in the literature. To fill in this gap, we introduce a\nnovel concept of Model DNA which represents the unique characteristics of a\nmachine learning model. We utilize a data-driven and model-driven\nrepresentation learning method to encode the model's training data and\ninput-output information as a compact and comprehensive representation (i.e.,\nDNA) of the model. Using this model DNA, we develop an efficient framework for\nmodel provenance identification, which enables us to identify whether a source\nmodel is a pre-training model of a target model. We conduct evaluations on both\ncomputer vision and natural language processing tasks using various models,\ndatasets, and scenarios to demonstrate the effectiveness of our approach in\naccurately identifying model provenance.\n","authors":["Xin Mu","Yu Wang","Yehong Zhang","Jiaqi Zhang","Hui Wang","Yang Xiang","Yue Yu"],"pdf_url":"https://arxiv.org/pdf/2308.02121v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16883v4","updated":"2024-07-18T08:50:18Z","published":"2024-03-25T15:53:32Z","title":"GLAD: Improving Latent Graph Generative Modeling with Simple\n  Quantization","summary":"  Exploring the graph latent structures has not garnered much attention in the\ngraph generative research field. Yet, exploiting the latent space is as crucial\nas working on the data space for discrete data such as graphs. However,\nprevious methods either failed to preserve the permutation symmetry of graphs\nor lacked an effective approaches to model appropriately within the latent\nspace. To mitigate those issues, we propose a simple, yet effective discrete\nlatent graph diffusion generative model. Our model, namely GLAD, not only\novercomes the drawbacks of existing latent approaches, but also alleviates\ninherent issues present in diffusion methods applied on the graph space. We\nvalidate our generative model on the molecular benchmark datasets, on which it\ndemonstrates competitive performance compared with the state-of-the-art\nbaselines.\n","authors":["Van Khoa Nguyen","Yoann Boget","Frantzeska Lavda","Alexandros Kalousis"],"pdf_url":"https://arxiv.org/pdf/2403.16883v4.pdf","comment":"Accepted in the 2nd Structured Probabilistic Inference & Generative\n  Modeling workshop of ICML 2024"},{"id":"http://arxiv.org/abs/2407.13291v1","updated":"2024-07-18T08:45:14Z","published":"2024-07-18T08:45:14Z","title":"Scikit-fingerprints: easy and efficient computation of molecular\n  fingerprints in Python","summary":"  In this work, we present \\textit{scikit-fingerprints}, a Python package for\ncomputation of molecular fingerprints for applications in chemoinformatics. Our\nlibrary offers an industry-standard scikit-learn interface, allowing intuitive\nusage and easy integration with machine learning pipelines. It is also highly\noptimized, featuring parallel computation that enables efficient processing of\nlarge molecular datasets. Currently, \\textit{scikit-fingerprints} stands as the\nmost feature-rich library in the Python ecosystem, offering over 30 molecular\nfingerprints. Our library simplifies chemoinformatics tasks based on molecular\nfingerprints, including molecular property prediction and virtual screening. It\nis also flexible, highly efficient, and fully open source.\n","authors":["Jakub Adamczyk","Piotr Ludynia"],"pdf_url":"https://arxiv.org/pdf/2407.13291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11962v2","updated":"2024-07-18T08:44:16Z","published":"2024-07-16T17:59:01Z","title":"Motion-Oriented Compositional Neural Radiance Fields for Monocular\n  Dynamic Human Modeling","summary":"  This paper introduces Motion-oriented Compositional Neural Radiance Fields\n(MoCo-NeRF), a framework designed to perform free-viewpoint rendering of\nmonocular human videos via novel non-rigid motion modeling approach. In the\ncontext of dynamic clothed humans, complex cloth dynamics generate non-rigid\nmotions that are intrinsically distinct from skeletal articulations and\ncritically important for the rendering quality. The conventional approach\nmodels non-rigid motions as spatial (3D) deviations in addition to skeletal\ntransformations. However, it is either time-consuming or challenging to achieve\noptimal quality due to its high learning complexity without a direct\nsupervision. To target this problem, we propose a novel approach of modeling\nnon-rigid motions as radiance residual fields to benefit from more direct color\nsupervision in the rendering and utilize the rigid radiance fields as a prior\nto reduce the complexity of the learning process. Our approach utilizes a\nsingle multiresolution hash encoding (MHE) to concurrently learn the canonical\nT-pose representation from rigid skeletal motions and the radiance residual\nfield for non-rigid motions. Additionally, to further improve both training\nefficiency and usability, we extend MoCo-NeRF to support simultaneous training\nof multiple subjects within a single framework, thanks to our effective design\nfor modeling non-rigid motions. This scalability is achieved through the\nintegration of a global MHE and learnable identity codes in addition to\nmultiple local MHEs. We present extensive results on ZJU-MoCap and MonoCap,\nclearly demonstrating state-of-the-art performance in both single- and\nmulti-subject settings. The code and model will be made publicly available at\nthe project page: https://stevejaehyeok.github.io/publications/moco-nerf.\n","authors":["Jaehyeok Kim","Dongyoon Wee","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2407.11962v2.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.13288v1","updated":"2024-07-18T08:43:26Z","published":"2024-07-18T08:43:26Z","title":"Hierarchical Stage-Wise Training of Linked Deep Neural Networks for\n  Multi-Building and Multi-Floor Indoor Localization Based on Wi-Fi RSSI\n  Fingerprinting","summary":"  In this paper, we present a new solution to the problem of large-scale\nmulti-building and multi-floor indoor localization based on linked neural\nnetworks, where each neural network is dedicated to a sub-problem and trained\nunder a hierarchical stage-wise training framework. When the measured data from\nsensors have a hierarchical representation as in multi-building and multi-floor\nindoor localization, it is important to exploit the hierarchical nature in data\nprocessing to provide a scalable solution. In this regard, the hierarchical\nstage-wise training framework extends the original stage-wise training\nframework to the case of multiple linked networks by training a lower-hierarchy\nnetwork based on the prior knowledge gained from the training of\nhigher-hierarchy networks. The experimental results with the publicly-available\nUJIIndoorLoc multi-building and multi-floor Wi-Fi RSSI fingerprint database\ndemonstrate that the linked neural networks trained under the proposed\nhierarchical stage-wise training framework can achieve a three-dimensional\nlocalization error of 8.19 m, which, to the best of the authors' knowledge, is\nthe most accurate result ever obtained for neural network-based models trained\nand evaluated with the full datasets of the UJIIndoorLoc database, and that,\nwhen applied to a model based on hierarchical convolutional neural networks,\nthe proposed training framework can also significantly reduce the\nthree-dimensional localization error from 11.78 m to 8.71 m.\n","authors":["Sihao Li","Kyeong Soo Kim","Zhe Tang"," Graduate","Jeremy S. Smith"],"pdf_url":"https://arxiv.org/pdf/2407.13288v1.pdf","comment":"9 pages, 5 figures, under review for journal publication"},{"id":"http://arxiv.org/abs/2407.11626v2","updated":"2024-07-18T08:41:40Z","published":"2024-07-16T11:41:35Z","title":"Dynamic Dimension Wrapping (DDW) Algorithm: A Novel Approach for\n  Efficient Cross-Dimensional Search in Dynamic Multidimensional Spaces","summary":"  In the real world, as the complexity of optimization problems continues to\nincrease, there is an urgent need to research more efficient optimization\nmethods. Current optimization algorithms excel in solving problems with a fixed\nnumber of dimensions. However, their efficiency in searching dynamic\nmulti-dimensional spaces is unsatisfactory. In response to the challenge of\ncross-dimensional search in multi-dimensional spaces with varying numbers of\ndimensions, this study proposes a new optimization algorithm-Dynamic Dimension\nWrapping (DDW) algorithm. Firstly, by utilizing the Dynamic Time Warping (DTW)\nalgorithm and Euclidean distance, a mapping relationship between different time\nseries across dimensions is established, thus creating a fitness function\nsuitable for dimensionally dynamic multi-dimensional space. Additionally, DDW\nintroduces a novel, more efficient cross-dimensional search mechanism for\ndynamic multidimensional spaces. Finally, through comparative tests with 31\noptimization algorithms in dynamic multidimensional space search, the results\ndemonstrate that DDW exhibits outstanding search efficiency and provides search\nresults closest to the actual optimal solution.\n","authors":["Dongnan Jin","Yali Liu","Qiuzhi Song","Xunju Ma","Yue Liu","Dehao Wu"],"pdf_url":"https://arxiv.org/pdf/2407.11626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13281v1","updated":"2024-07-18T08:34:05Z","published":"2024-07-18T08:34:05Z","title":"Auditing Local Explanations is Hard","summary":"  In sensitive contexts, providers of machine learning algorithms are\nincreasingly required to give explanations for their algorithms' decisions.\nHowever, explanation receivers might not trust the provider, who potentially\ncould output misleading or manipulated explanations. In this work, we\ninvestigate an auditing framework in which a third-party auditor or a\ncollective of users attempts to sanity-check explanations: they can query model\ndecisions and the corresponding local explanations, pool all the information\nreceived, and then check for basic consistency properties. We prove upper and\nlower bounds on the amount of queries that are needed for an auditor to succeed\nwithin this framework. Our results show that successful auditing requires a\npotentially exorbitant number of queries -- particularly in high dimensional\ncases. Our analysis also reveals that a key property is the ``locality'' of the\nprovided explanations -- a quantity that so far has not been paid much\nattention to in the explainability literature. Looking forward, our results\nsuggest that for complex high-dimensional settings, merely providing a\npointwise prediction and explanation could be insufficient, as there is no way\nfor the users to verify that the provided explanations are not completely\nmade-up.\n","authors":["Robi Bhattacharjee","Ulrike von Luxburg"],"pdf_url":"https://arxiv.org/pdf/2407.13281v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2407.13279v1","updated":"2024-07-18T08:33:10Z","published":"2024-07-18T08:33:10Z","title":"Analyzing and Bridging the Gap between Maximizing Total Reward and\n  Discounted Reward in Deep Reinforcement Learning","summary":"  In deep reinforcement learning applications, maximizing discounted reward is\noften employed instead of maximizing total reward to ensure the convergence and\nstability of algorithms, even though the performance metric for evaluating the\npolicy remains the total reward. However, the optimal policies corresponding to\nthese two objectives may not always be consistent. To address this issue, we\nanalyzed the suboptimality of the policy obtained through maximizing discounted\nreward in relation to the policy that maximizes total reward and identified the\ninfluence of hyperparameters. Additionally, we proposed sufficient conditions\nfor aligning the optimal policies of these two objectives under various\nsettings. The primary contributions are as follows: We theoretically analyzed\nthe factors influencing performance when using discounted reward as a proxy for\ntotal reward, thereby enhancing the theoretical understanding of this scenario.\nFurthermore, we developed methods to align the optimal policies of the two\nobjectives in certain situations, which can improve the performance of\nreinforcement learning algorithms.\n","authors":["Shuyu Yin","Fei Wen","Peilin Liu","Tao Luo"],"pdf_url":"https://arxiv.org/pdf/2407.13279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11743v2","updated":"2024-07-18T08:32:51Z","published":"2024-03-18T12:55:40Z","title":"PARMESAN: Parameter-Free Memory Search and Transduction for Dense\n  Prediction Tasks","summary":"  This work addresses flexibility in deep learning by means of transductive\nreasoning. For adaptation to new data and tasks, e.g., in continual learning,\nexisting methods typically involve tuning learnable parameters or complete\nre-training from scratch, rendering such approaches unflexible in practice. We\nargue that the notion of separating computation from memory by the means of\ntransduction can act as a stepping stone for solving these issues. We therefore\npropose PARMESAN (parameter-free memory search and transduction), a scalable\nmethod which leverages a memory module for solving dense prediction tasks. At\ninference, hidden representations in memory are being searched to find\ncorresponding patterns. In contrast to other methods that rely on continuous\ntraining of learnable parameters, PARMESAN learns via memory consolidation\nsimply by modifying stored contents. Our method is compatible with commonly\nused architectures and canonically transfers to 1D, 2D, and 3D grid-based data.\nThe capabilities of our approach are demonstrated at the complex task of\ncontinual learning. PARMESAN learns by 3-4 orders of magnitude faster than\nestablished baselines while being on par in terms of predictive performance,\nhardware-efficiency, and knowledge retention.\n","authors":["Philip Matthias Winter","Maria Wimmer","David Major","Dimitrios Lenis","Astrid Berg","Theresa Neubauer","Gaia Romana De Paolis","Johannes Novotny","Sophia Ulonska","Katja Bühler"],"pdf_url":"https://arxiv.org/pdf/2403.11743v2.pdf","comment":"preprint, 25 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.13278v1","updated":"2024-07-18T08:31:55Z","published":"2024-07-18T08:31:55Z","title":"Deep Time Series Models: A Comprehensive Survey and Benchmark","summary":"  Time series, characterized by a sequence of data points arranged in a\ndiscrete-time order, are ubiquitous in real-world applications. Different from\nother modalities, time series present unique challenges due to their complex\nand dynamic nature, including the entanglement of nonlinear patterns and\ntime-variant trends. Analyzing time series data is of great significance in\nreal-world scenarios and has been widely studied over centuries. Recent years\nhave witnessed remarkable breakthroughs in the time series community, with\ntechniques shifting from traditional statistical methods to advanced deep\nlearning models. In this paper, we delve into the design of deep time series\nmodels across various analysis tasks and review the existing literature from\ntwo perspectives: basic modules and model architectures. Further, we develop\nand release Time Series Library (TSLib) as a fair benchmark of deep time series\nmodels for diverse analysis tasks, which implements 24 mainstream models,\ncovers 30 datasets from different domains, and supports five prevalent analysis\ntasks. Based on TSLib, we thoroughly evaluate 12 advanced deep time series\nmodels on different tasks. Empirical results indicate that models with specific\nstructures are well-suited for distinct analytical tasks, which offers insights\nfor research and adoption of deep time series models. Code is available at\nhttps://github.com/thuml/Time-Series-Library.\n","authors":["Yuxuan Wang","Haixu Wu","Jiaxiang Dong","Yong Liu","Mingsheng Long","Jianmin Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13278v1.pdf","comment":"\\"},{"id":"http://arxiv.org/abs/2406.18676v2","updated":"2024-07-18T08:28:09Z","published":"2024-06-26T18:26:53Z","title":"Understand What LLM Needs: Dual Preference Alignment for\n  Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) has demonstrated effectiveness in\nmitigating the hallucination problem of large language models (LLMs). However,\nthe difficulty of aligning the retriever with the diverse LLMs' knowledge\npreferences inevitably poses an inevitable challenge in developing a reliable\nRAG system. To address this issue, we propose DPA-RAG, a universal framework\ndesigned to align diverse knowledge preferences within RAG systems.\nSpecifically, we initially introduce a preference knowledge construction\npipline and incorporate five novel query augmentation strategies to alleviate\npreference data scarcity. Based on preference data, DPA-RAG accomplishes both\nexternal and internal preference alignment: 1) It jointly integrate pair-wise,\npoint-wise, and contrastive preference alignment abilities into the reranker,\nachieving external preference alignment among RAG components. 2) It further\nintroduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT),\nenabling LLMs to implicitly capture knowledge aligned with their reasoning\npreferences, achieving LLMs' internal alignment. Experimental results across\nfour knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all\nbaselines and seamlessly integrates both black-box and open-sourced LLM\nreaders. Further qualitative analysis and discussions also provide empirical\nguidance for achieving reliable RAG systems. Our code is publicly available at\nhttps://github.com/dongguanting/DPA-RAG.\n","authors":["Guanting Dong","Yutao Zhu","Chenghao Zhang","Zechen Wang","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2406.18676v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2402.16359v3","updated":"2024-07-18T08:21:54Z","published":"2024-02-26T07:24:32Z","title":"Feedback Efficient Online Fine-Tuning of Diffusion Models","summary":"  Diffusion models excel at modeling complex data distributions, including\nthose of images, proteins, and small molecules. However, in many cases, our\ngoal is to model parts of the distribution that maximize certain properties:\nfor example, we may want to generate images with high aesthetic quality, or\nmolecules with high bioactivity. It is natural to frame this as a reinforcement\nlearning (RL) problem, in which the objective is to fine-tune a diffusion model\nto maximize a reward function that corresponds to some property. Even with\naccess to online queries of the ground-truth reward function, efficiently\ndiscovering high-reward samples can be challenging: they might have a low\nprobability in the initial distribution, and there might be many infeasible\nsamples that do not even have a well-defined reward (e.g., unnatural images or\nphysically impossible molecules). In this work, we propose a novel\nreinforcement learning procedure that efficiently explores on the manifold of\nfeasible samples. We present a theoretical analysis providing a regret\nguarantee, as well as empirical validation across three domains: images,\nbiological sequences, and molecules.\n","authors":["Masatoshi Uehara","Yulai Zhao","Kevin Black","Ehsan Hajiramezanali","Gabriele Scalia","Nathaniel Lee Diamant","Alex M Tseng","Sergey Levine","Tommaso Biancalani"],"pdf_url":"https://arxiv.org/pdf/2402.16359v3.pdf","comment":"Accepted at ICML 2024"},{"id":"http://arxiv.org/abs/2406.03145v3","updated":"2024-07-18T08:21:34Z","published":"2024-06-05T11:00:27Z","title":"E(n) Equivariant Message Passing Cellular Networks","summary":"  This paper introduces E(n) Equivariant Message Passing Cellular Networks\n(EMPCNs), an extension of E(n) Equivariant Graph Neural Networks to\nCW-complexes. Our approach addresses two aspects of geometric message passing\nnetworks: 1) enhancing their expressiveness by incorporating arbitrary cells,\nand 2) achieving this in a computationally efficient way with a decoupled\nEMPCNs technique. We demonstrate that EMPCNs achieve close to state-of-the-art\nperformance on multiple tasks without the need for steerability, including\nmany-body predictions and motion capture. Moreover, ablation studies confirm\nthat decoupled EMPCNs exhibit stronger generalization capabilities than their\nnon-topologically informed counterparts. These findings show that EMPCNs can be\nused as a scalable and expressive framework for higher-order message passing in\ngeometric and topological graphs\n","authors":["Veljko Kovač","Erik J. Bekkers","Pietro Liò","Floor Eijkelboom"],"pdf_url":"https://arxiv.org/pdf/2406.03145v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13268v1","updated":"2024-07-18T08:21:31Z","published":"2024-07-18T08:21:31Z","title":"Mixture of Experts based Multi-task Supervise Learning from Crowds","summary":"  Existing truth inference methods in crowdsourcing aim to map redundant labels\nand items to the ground truth. They treat the ground truth as hidden variables\nand use statistical or deep learning-based worker behavior models to infer the\nground truth. However, worker behavior models that rely on ground truth hidden\nvariables overlook workers' behavior at the item feature level, leading to\nimprecise characterizations and negatively impacting the quality of truth\ninference. This paper proposes a new paradigm of multi-task supervised learning\nfrom crowds, which eliminates the need for modeling of items's ground truth in\nworker behavior models. Within this paradigm, we propose a worker behavior\nmodel at the item feature level called Mixture of Experts based Multi-task\nSupervised Learning from Crowds (MMLC). Two truth inference strategies are\nproposed within MMLC. The first strategy, named MMLC-owf, utilizes clustering\nmethods in the worker spectral space to identify the projection vector of the\noracle worker. Subsequently, the labels generated based on this vector are\nconsidered as the inferred truth. The second strategy, called MMLC-df, employs\nthe MMLC model to fill the crowdsourced data, which can enhance the\neffectiveness of existing truth inference methods. Experimental results\ndemonstrate that MMLC-owf outperforms state-of-the-art methods and MMLC-df\nenhances the quality of existing truth inference methods.\n","authors":["Tao Han","Huaixuan Shi","Xinyi Ding","Xiao Ma","Huamao Gu","Yili Fang"],"pdf_url":"https://arxiv.org/pdf/2407.13268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05610v2","updated":"2024-07-18T08:12:46Z","published":"2023-09-11T16:49:05Z","title":"Privacy Side Channels in Machine Learning Systems","summary":"  Most current approaches for protecting privacy in machine learning (ML)\nassume that models exist in a vacuum. Yet, in reality, these models are part of\nlarger systems that include components for training data filtering, output\nmonitoring, and more. In this work, we introduce privacy side channels: attacks\nthat exploit these system-level components to extract private information at\nfar higher rates than is otherwise possible for standalone models. We propose\nfour categories of side channels that span the entire ML lifecycle (training\ndata filtering, input preprocessing, output post-processing, and query\nfiltering) and allow for enhanced membership inference, data extraction, and\neven novel threats such as extraction of users' test queries. For example, we\nshow that deduplicating training data before applying differentially-private\ntraining creates a side-channel that completely invalidates any provable\nprivacy guarantees. We further show that systems which block language models\nfrom regenerating training data can be exploited to exfiltrate private keys\ncontained in the training set--even if the model did not memorize these keys.\nTaken together, our results demonstrate the need for a holistic, end-to-end\nprivacy analysis of machine learning systems.\n","authors":["Edoardo Debenedetti","Giorgio Severi","Nicholas Carlini","Christopher A. Choquette-Choo","Matthew Jagielski","Milad Nasr","Eric Wallace","Florian Tramèr"],"pdf_url":"https://arxiv.org/pdf/2309.05610v2.pdf","comment":"USENIX Security 2024"},{"id":"http://arxiv.org/abs/2406.05666v5","updated":"2024-07-18T08:11:28Z","published":"2024-06-09T06:49:22Z","title":"General Distribution Learning: A theoretical framework for Deep Learning","summary":"  This paper introduces General Distribution Learning (GD learning), a novel\ntheoretical learning framework designed to address a comprehensive range of\nmachine learning and statistical tasks, including classification, regression,\nand parameter estimation. GD learning focuses on estimating the true underlying\nprobability distribution of dataset and using models to fit the estimated\nparameters of the distribution. The learning error in GD learning is thus\ndecomposed into two distinct categories: estimation error and fitting error.\nThe estimation error, which stems from the constraints of finite sampling,\nlimited prior knowledge, and the estimation algorithm's inherent limitations,\nquantifies the discrepancy between the true distribution and its estimate. The\nfitting error can be attributed to model's capacity limitation and the\nperformance limitation of the optimization algorithm, which evaluates the\ndeviation of the model output from the fitted objective. To address the\nchallenge of non-convexity in the optimization of learning error, we introduce\nthe standard loss function and demonstrate that, when employing this function,\nglobal optimal solutions in non-convex optimization can be approached by\nminimizing the gradient norm and the structural error. Moreover, we demonstrate\nthat the estimation error is determined by the uncertainty of the estimate $q$,\nand propose the minimum uncertainty principle to obtain an optimal estimate of\nthe true distribution. We further provide upper bounds for the estimation\nerror, fitting error, and learning error within the GD learning framework.\nUltimately, our findings are applied to offer theoretical explanations for\nseveral unanswered questions on deep learning, including overparameterization,\nnon-convex optimization, flat minima, dynamic isometry condition and other\ntechniques in deep learning.\n","authors":["Binchuan Qi"],"pdf_url":"https://arxiv.org/pdf/2406.05666v5.pdf","comment":"arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors"},{"id":"http://arxiv.org/abs/2407.09105v2","updated":"2024-07-18T08:09:32Z","published":"2024-07-12T09:10:37Z","title":"Enhancing Training Efficiency Using Packing with Flash Attention","summary":"  Padding is often used in tuning LLM models by adding special tokens to\nshorter training examples to match the length of the longest sequence in each\nbatch. While this ensures uniformity for batch processing, it introduces\ninefficiencies by including irrelevant padding tokens in the computation and\nwastes GPU resources. On the other hand, the Hugging Face SFT trainer offers\nthe option to use packing to combine multiple training examples up to the\nmaximum sequence length. This allows for maximal utilization of GPU resources.\nHowever, without proper masking of each packed training example, attention will\nnot be computed correctly when using SFT trainer. We enable and then analyse\npacking and Flash Attention with proper attention masking of each example and\nshow the benefits of this training paradigm.\n","authors":["Achintya Kundu","Rhui Dih Lee","Laura Wynter","Raghu Kiran Ganti","Mayank Mishra"],"pdf_url":"https://arxiv.org/pdf/2407.09105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13251v1","updated":"2024-07-18T08:04:57Z","published":"2024-07-18T08:04:57Z","title":"Motif-Consistent Counterfactuals with Adversarial Refinement for\n  Graph-Level Anomaly Detection","summary":"  Graph-level anomaly detection is significant in diverse domains. To improve\ndetection performance, counterfactual graphs have been exploited to benefit the\ngeneralization capacity by learning causal relations. Most existing studies\ndirectly introduce perturbations (e.g., flipping edges) to generate\ncounterfactual graphs, which are prone to alter the semantics of generated\nexamples and make them off the data manifold, resulting in sub-optimal\nperformance. To address these issues, we propose a novel approach,\nMotif-consistent Counterfactuals with Adversarial Refinement (MotifCAR), for\ngraph-level anomaly detection. The model combines the motif of one graph, the\ncore subgraph containing the identification (category) information, and the\ncontextual subgraph (non-motif) of another graph to produce a raw\ncounterfactual graph. However, the produced raw graph might be distorted and\ncannot satisfy the important counterfactual properties: Realism, Validity,\nProximity and Sparsity. Towards that, we present a Generative Adversarial\nNetwork (GAN)-based graph optimizer to refine the raw counterfactual graphs. It\nadopts the discriminator to guide the generator to generate graphs close to\nrealistic data, i.e., meet the property Realism. Further, we design the motif\nconsistency to force the motif of the generated graphs to be consistent with\nthe realistic graphs, meeting the property Validity. Also, we devise the\ncontextual loss and connection loss to control the contextual subgraph and the\nnewly added links to meet the properties Proximity and Sparsity. As a result,\nthe model can generate high-quality counterfactual graphs. Experiments\ndemonstrate the superiority of MotifCAR.\n","authors":["Chunjing Xiao","Shikang Pang","Wenxin Tai","Yanlong Huang","Goce Trajcevski","Fan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.13251v1.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2305.00510v4","updated":"2024-07-18T07:49:58Z","published":"2023-04-30T15:38:36Z","title":"Towards AI-Architecture Liberty: A Comprehensive Survey on Design and\n  Generation of Virtual Architecture by Deep Learning","summary":"  3D shape generation techniques leveraging deep learning have garnered\nsignificant interest from both the computer vision and architectural design\ncommunities, promising to enrich the content in the virtual environment.\nHowever, research on virtual architectural design remains limited, particularly\nregarding designer-AI collaboration and deep learning-assisted design. In our\nsurvey, we reviewed 149 related articles (81.2% of articles published between\n2019 and 2023) covering architectural design, 3D shape techniques, and virtual\nenvironments. Through scrutinizing the literature, we first identify the\nprinciples of virtual architecture and illuminate its current production\nchallenges, including datasets, multimodality, design intuition, and generative\nframeworks. We then introduce the latest approaches to designing and generating\nvirtual buildings leveraging 3D shape generation and summarize four\ncharacteristics of various approaches to virtual architecture. Based on our\nanalysis, we expound on four research agendas, including agency, communication,\nuser consideration, and integrating tools. Additionally, we highlight four\nimportant enablers of ubiquitous interaction with immersive systems in deep\nlearning-assisted architectural generation. Our work contributes to fostering\nunderstanding between designers and deep learning techniques, broadening access\nto designer-AI collaboration. We advocate for interdisciplinary efforts to\naddress this timely research topic, facilitating content designing and\ngeneration in the virtual environment.\n","authors":["Anqi Wang","Jiahua Dong","Lik-Hang Lee","Jiachuan Shen","Pan Hui"],"pdf_url":"https://arxiv.org/pdf/2305.00510v4.pdf","comment":"36 pages, 9 figures, and 5 tables"},{"id":"http://arxiv.org/abs/2407.13238v1","updated":"2024-07-18T07:48:48Z","published":"2024-07-18T07:48:48Z","title":"Transformers with Stochastic Competition for Tabular Data Modelling","summary":"  Despite the prevalence and significance of tabular data across numerous\nindustries and fields, it has been relatively underexplored in the realm of\ndeep learning. Even today, neural networks are often overshadowed by techniques\nsuch as gradient boosted decision trees (GBDT). However, recent models are\nbeginning to close this gap, outperforming GBDT in various setups and garnering\nincreased attention in the field. Inspired by this development, we introduce a\nnovel stochastic deep learning model specifically designed for tabular data.\nThe foundation of this model is a Transformer-based architecture, carefully\nadapted to cater to the unique properties of tabular data through strategic\narchitectural modifications and leveraging two forms of stochastic competition.\nFirst, we employ stochastic \"Local Winner Takes All\" units to promote\ngeneralization capacity through stochasticity and sparsity. Second, we\nintroduce a novel embedding layer that selects among alternative linear\nembedding layers through a mechanism of stochastic competition. The\neffectiveness of the model is validated on a variety of widely-used, publicly\navailable datasets. We demonstrate that, through the incorporation of these\nelements, our model yields high performance and marks a significant advancement\nin the application of deep learning to tabular data.\n","authors":["Andreas Voskou","Charalambos Christoforou","Sotirios Chatzis"],"pdf_url":"https://arxiv.org/pdf/2407.13238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06886v4","updated":"2024-07-18T07:41:12Z","published":"2024-07-09T14:14:47Z","title":"Aligning Cyber Space with Physical World: A Comprehensive Survey on\n  Embodied AI","summary":"  Embodied Artificial Intelligence (Embodied AI) is crucial for achieving\nArtificial General Intelligence (AGI) and serves as a foundation for various\napplications that bridge cyberspace and the physical world. Recently, the\nemergence of Multi-modal Large Models (MLMs) and World Models (WMs) have\nattracted significant attention due to their remarkable perception,\ninteraction, and reasoning capabilities, making them a promising architecture\nfor the brain of embodied agents. However, there is no comprehensive survey for\nEmbodied AI in the era of MLMs. In this survey, we give a comprehensive\nexploration of the latest advancements in Embodied AI. Our analysis firstly\nnavigates through the forefront of representative works of embodied robots and\nsimulators, to fully understand the research focuses and their limitations.\nThen, we analyze four main research targets: 1) embodied perception, 2)\nembodied interaction, 3) embodied agent, and 4) sim-to-real adaptation,\ncovering the state-of-the-art methods, essential paradigms, and comprehensive\ndatasets. Additionally, we explore the complexities of MLMs in virtual and real\nembodied agents, highlighting their significance in facilitating interactions\nin dynamic digital and physical environments. Finally, we summarize the\nchallenges and limitations of embodied AI and discuss their potential future\ndirections. We hope this survey will serve as a foundational reference for the\nresearch community and inspire continued innovation. The associated project can\nbe found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.\n","authors":["Yang Liu","Weixing Chen","Yongjie Bai","Jingzhou Luo","Xinshuai Song","Kaixuan Jiang","Zhida Li","Ganlong Zhao","Junyi Lin","Guanbin Li","Wen Gao","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2407.06886v4.pdf","comment":"The first comprehensive review of Embodied AI in the era of MLMs, 37\n  pages. We also provide the paper list for Embodied AI:\n  https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List"},{"id":"http://arxiv.org/abs/2406.13352v2","updated":"2024-07-18T07:37:28Z","published":"2024-06-19T08:55:56Z","title":"AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for\n  LLM Agents","summary":"  AI agents aim to solve complex tasks by combining text-based reasoning with\nexternal tool calls. Unfortunately, AI agents are vulnerable to prompt\ninjection attacks where data returned by external tools hijacks the agent to\nexecute malicious tasks. To measure the adversarial robustness of AI agents, we\nintroduce AgentDojo, an evaluation framework for agents that execute tools over\nuntrusted data. To capture the evolving nature of attacks and defenses,\nAgentDojo is not a static test suite, but rather an extensible environment for\ndesigning and evaluating new agent tasks, defenses, and adaptive attacks. We\npopulate the environment with 97 realistic tasks (e.g., managing an email\nclient, navigating an e-banking website, or making travel bookings), 629\nsecurity test cases, and various attack and defense paradigms from the\nliterature. We find that AgentDojo poses a challenge for both attacks and\ndefenses: state-of-the-art LLMs fail at many tasks (even in the absence of\nattacks), and existing prompt injection attacks break some security properties\nbut not all. We hope that AgentDojo can foster research on new design\nprinciples for AI agents that solve common tasks in a reliable and robust\nmanner. We release the code for AgentDojo at\nhttps://github.com/ethz-spylab/agentdojo.\n","authors":["Edoardo Debenedetti","Jie Zhang","Mislav Balunović","Luca Beurer-Kellner","Marc Fischer","Florian Tramèr"],"pdf_url":"https://arxiv.org/pdf/2406.13352v2.pdf","comment":"Updated version after fixing a bug in the Llama implementation and\n  updating the travel suite"},{"id":"http://arxiv.org/abs/2302.10913v2","updated":"2024-07-18T07:33:45Z","published":"2023-02-14T16:58:32Z","title":"From paintbrush to pixel: A review of deep neural networks in\n  AI-generated art","summary":"  This paper delves into the fascinating field of AI-generated art and explores\nthe various deep neural network architectures and models that have been\nutilized to create it. From the classic convolutional networks to the\ncutting-edge diffusion models, we examine the key players in the field. We\nexplain the general structures and working principles of these neural networks.\nThen, we showcase examples of milestones, starting with the dreamy landscapes\nof DeepDream and moving on to the most recent developments, including Stable\nDiffusion and DALL-E 3, which produce mesmerizing images. We provide a detailed\ncomparison of these models, highlighting their strengths and limitations, and\nexamining the remarkable progress that deep neural networks have made so far in\na short period of time. With a unique blend of technical explanations and\ninsights into the current state of AI-generated art, this paper exemplifies how\nart and computer science interact.\n","authors":["Anne-Sofie Maerten","Derya Soydaner"],"pdf_url":"https://arxiv.org/pdf/2302.10913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05649v2","updated":"2024-07-18T07:30:43Z","published":"2024-07-08T06:21:56Z","title":"Graph Attention with Random Rewiring","summary":"  Graph Neural Networks (GNNs) have become fundamental in graph-structured deep\nlearning. Key paradigms of modern GNNs include message passing, graph rewiring,\nand Graph Transformers. This paper introduces Graph-Rewiring Attention with\nStochastic Structures (GRASS), a novel GNN architecture that combines the\nadvantages of these three paradigms. GRASS rewires the input graph by\nsuperimposing a random regular graph, enhancing long-range information\npropagation while preserving structural features of the input graph. It also\nemploys a unique additive attention mechanism tailored for graph-structured\ndata, providing a graph inductive bias while remaining computationally\nefficient. Our empirical evaluations demonstrate that GRASS achieves\nstate-of-the-art performance on multiple benchmark datasets, confirming its\npractical efficacy.\n","authors":["Tongzhou Liao","Barnabás Póczos"],"pdf_url":"https://arxiv.org/pdf/2407.05649v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13228v1","updated":"2024-07-18T07:26:09Z","published":"2024-07-18T07:26:09Z","title":"Evaluating Large Language Models for Anxiety and Depression\n  Classification using Counseling and Psychotherapy Transcripts","summary":"  We aim to evaluate the efficacy of traditional machine learning and large\nlanguage models (LLMs) in classifying anxiety and depression from long\nconversational transcripts. We fine-tune both established transformer models\n(BERT, RoBERTa, Longformer) and more recent large models (Mistral-7B), trained\na Support Vector Machine with feature engineering, and assessed GPT models\nthrough prompting. We observe that state-of-the-art models fail to enhance\nclassification outcomes compared to traditional machine learning methods.\n","authors":["Junwei Sun","Siqi Ma","Yiran Fan","Peter Washington"],"pdf_url":"https://arxiv.org/pdf/2407.13228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17717v3","updated":"2024-07-18T07:23:03Z","published":"2023-11-29T15:19:49Z","title":"Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via\n  Lightweight Erasers","summary":"  Concept erasure in text-to-image diffusion models aims to disable pre-trained\ndiffusion models from generating images related to a target concept. To perform\nreliable concept erasure, the properties of robustness and locality are\ndesirable. The former refrains the model from producing images associated with\nthe target concept for any paraphrased or learned prompts, while the latter\npreserves its ability in generating images with non-target concepts. In this\npaper, we propose Reliable Concept Erasing via Lightweight Erasers (Receler).\nIt learns a lightweight Eraser to perform concept erasing while satisfying the\nabove desirable properties through the proposed concept-localized\nregularization and adversarial prompt learning scheme. Experiments with various\nconcepts verify the superiority of Receler over previous methods.\n","authors":["Chi-Pin Huang","Kai-Po Chang","Chung-Ting Tsai","Yung-Hsuan Lai","Fu-En Yang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2311.17717v3.pdf","comment":"ECCV 2024. Project page:\n  https://jasper0314-huang.github.io/receler-concept-erasing/"},{"id":"http://arxiv.org/abs/2403.09918v2","updated":"2024-07-18T07:12:05Z","published":"2024-03-14T23:31:41Z","title":"Attention-based Class-Conditioned Alignment for Multi-Source Domain\n  Adaptation of Object Detectors","summary":"  Domain adaptation methods for object detection (OD) strive to mitigate the\nimpact of distribution shifts by promoting feature alignment across source and\ntarget domains. Multi-source domain adaptation (MSDA) allows leveraging\nmultiple annotated source datasets and unlabeled target data to improve the\naccuracy and robustness of the detection model. Most state-of-the-art MSDA\nmethods for OD perform feature alignment in a class-agnostic manner. This is\nchallenging since the objects have unique modal information due to variations\nin object appearance across domains. A recent prototype-based approach proposed\na class-wise alignment, yet it suffers from error accumulation due to noisy\npseudo-labels that can negatively affect adaptation with imbalanced data. To\novercome these limitations, we propose an attention-based class-conditioned\nalignment method for MSDA that aligns instances of each object category across\ndomains. In particular, an attention module coupled with an adversarial domain\nclassifier allows learning domain-invariant and class-specific instance\nrepresentations. Experimental results on multiple benchmarking MSDA datasets\nindicate that our method outperforms the state-of-the-art methods and is robust\nto class imbalance using a conceptually simple class-conditioning method. Our\ncode is available at https://github.com/imatif17/ACIA.\n","authors":["Atif Belal","Akhil Meethal","Francisco Perdigon Romero","Marco Pedersoli","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2403.09918v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13222v1","updated":"2024-07-18T07:08:47Z","published":"2024-07-18T07:08:47Z","title":"Non-Contact Breath Rate Classification Using SVM Model and mmWave Radar\n  Sensor Data","summary":"  This work presents the use of frequency modulated continuous wave (FMCW)\nradar technology combined with a machine learning model to differentiate\nbetween normal and abnormal breath rates. The proposed system non-contactly\ncollects data using FMCW radar, which depends on breath rates. Various support\nvector machine kernels are used to classify the observed data into normal and\nabnormal states. Prolonged experiments show good accuracy in breath rate\nclassification, confirming the model's efficacy. The best accuracy is 95\npercent with the smallest number of support vectors in the case of the\nquadratic polynomial kernel.\n","authors":["Mohammad Wassaf Ali","Ayushi Gupta","Mujeev Khan","Mohd Wajid"],"pdf_url":"https://arxiv.org/pdf/2407.13222v1.pdf","comment":"6 Pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.19961v3","updated":"2024-07-18T07:04:46Z","published":"2024-05-30T11:32:42Z","title":"Collective Variable Free Transition Path Sampling with Generative Flow\n  Network","summary":"  Understanding transition paths between meta-stable states in molecular\nsystems is fundamental for material design and drug discovery. However,\nsampling these paths via unbiased molecular dynamics simulations is\ncomputationally prohibitive due to the high energy barriers between the\nmeta-stable states. Recent machine learning approaches are often restricted to\nsimple systems or rely on collective variables (CVs) extracted from expensive\ndomain knowledge. In this work, we propose to leverage generative flow networks\n(GFlowNets) to sample transition paths without relying on CVs. We reformulate\nthe problem as amortized energy-based sampling over transition paths and train\na neural bias potential by minimizing the squared log-ratio between the target\ndistribution and the generator, derived from the flow matching objective of\nGFlowNets. Our evaluation on three proteins (Alanine Dipeptide, Polyproline\nHelix, and Chignolin) demonstrates that our approach, called TPS-GFN, generates\nmore realistic and diverse transition paths than the previous CV-free machine\nlearning approach.\n","authors":["Kiyoung Seong","Seonghyun Park","Seonghwan Kim","Woo Youn Kim","Sungsoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2405.19961v3.pdf","comment":"8 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.13218v1","updated":"2024-07-18T07:04:33Z","published":"2024-07-18T07:04:33Z","title":"LiNR: Model Based Neural Retrieval on GPUs at LinkedIn","summary":"  This paper introduces LiNR, LinkedIn's large-scale, GPU-based retrieval\nsystem. LiNR supports a billion-sized index on GPU models. We discuss our\nexperiences and challenges in creating scalable, differentiable search indexes\nusing TensorFlow and PyTorch at production scale. In LiNR, both items and model\nweights are integrated into the model binary. Viewing index construction as a\nform of model training, we describe scaling our system for large indexes,\nincorporating full scans and efficient filtering. A key focus is on enabling\nattribute-based pre-filtering for exhaustive GPU searches, addressing the\ncommon challenge of post-filtering in KNN searches that often reduces system\nquality. We further provide multi-embedding retrieval algorithms and strategies\nfor tackling cold start issues in retrieval. Our advancements in supporting\nlarger indexes through quantization are also discussed. We believe LiNR\nrepresents one of the industry's first Live-updated model-based retrieval\nindexes. Applied to out-of-network post recommendations on LinkedIn Feed, LiNR\nhas contributed to a 3% relative increase in professional daily active users.\nWe envisage LiNR as a step towards integrating retrieval and ranking into a\nsingle GPU model, simplifying complex infrastructures and enabling end-to-end\noptimization of the entire differentiable infrastructure through gradient\ndescent.\n","authors":["Fedor Borisyuk","Qingquan Song","Mingzhou Zhou","Ganesh Parameswaran","Madhu Arun","Siva Popuri","Tugrul Bingol","Zhuotao Pei","Kuang-Hsuan Lee","Lu Zheng","Qizhan Shao","Ali Naqvi","Sen Zhou","Aman Gupta"],"pdf_url":"https://arxiv.org/pdf/2407.13218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13631v2","updated":"2024-07-18T06:55:46Z","published":"2024-04-21T12:11:03Z","title":"Fermi-Bose Machine achieves both generalization and adversarial\n  robustness","summary":"  Distinct from human cognitive processing, deep neural networks trained by\nbackpropagation can be easily fooled by adversarial examples. To design a\nsemantically meaningful representation learning, we discard backpropagation,\nand instead, propose a local contrastive learning, where the representation for\nthe inputs bearing the same label shrink (akin to boson) in hidden layers,\nwhile those of different labels repel (akin to fermion). This layer-wise\nlearning is local in nature, being biological plausible. A statistical\nmechanics analysis shows that the target fermion-pair-distance is a key\nparameter. Moreover, the application of this local contrastive learning to\nMNIST benchmark dataset demonstrates that the adversarial vulnerability of\nstandard perceptron can be greatly mitigated by tuning the target distance,\ni.e., controlling the geometric separation of prototype manifolds.\n","authors":["Mingshan Xie","Yuchen Wang","Haiping Huang"],"pdf_url":"https://arxiv.org/pdf/2404.13631v2.pdf","comment":"32 pages, 6 figures, a physics inspired machine without\n  backpropagation yet with enhanced adversarial robustness"},{"id":"http://arxiv.org/abs/2405.17458v2","updated":"2024-07-18T06:54:04Z","published":"2024-05-23T01:34:59Z","title":"Blood Glucose Control Via Pre-trained Counterfactual Invertible Neural\n  Networks","summary":"  Type 1 diabetes mellitus (T1D) is characterized by insulin deficiency and\nblood glucose (BG) control issues. The state-of-the-art solution for continuous\nBG control is reinforcement learning (RL), where an agent can dynamically\nadjust exogenous insulin doses in time to maintain BG levels within the target\nrange. However, due to the lack of action guidance, the agent often needs to\nlearn from randomized trials to understand misleading correlations between\nexogenous insulin doses and BG levels, which can lead to instability and\nunsafety. To address these challenges, we propose an introspective RL based on\nCounterfactual Invertible Neural Networks (CINN). We use the pre-trained CINN\nas a frozen introspective block of the RL agent, which integrates forward\nprediction and counterfactual inference to guide the policy updates, promoting\nmore stable and safer BG control. Constructed based on interpretable causal\norder, CINN employs bidirectional encoders with affine coupling layers to\nensure invertibility while using orthogonal weight normalization to enhance the\ntrainability, thereby ensuring the bidirectional differentiability of network\nparameters. We experimentally validate the accuracy and generalization ability\nof the pre-trained CINN in BG prediction and counterfactual inference for\naction. Furthermore, our experimental results highlight the effectiveness of\npre-trained CINN in guiding RL policy updates for more accurate and safer BG\ncontrol.\n","authors":["Jingchi Jiang","Rujia Shen","Boran Wang","Yi Guan"],"pdf_url":"https://arxiv.org/pdf/2405.17458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15492v2","updated":"2024-07-18T06:29:37Z","published":"2024-02-23T18:31:02Z","title":"Mechanics-Informed Autoencoder Enables Automated Detection and\n  Localization of Unforeseen Structural Damage","summary":"  Structural health monitoring (SHM) ensures the safety and longevity of\nstructures like buildings and bridges. As the volume and scale of structures\nand the impact of their failure continue to grow, there is a dire need for SHM\ntechniques that are scalable, inexpensive, can operate passively without human\nintervention, and are customized for each mechanical structure without the need\nfor complex baseline models. We present MIDAS, a novel \"deploy-and-forget\"\napproach for automated detection and localization of damage in structures. It\nis a synergistic integration of entirely passive measurements from inexpensive\nsensors, data compression, and a mechanics-informed autoencoder. Once deployed,\nMIDAS continuously learns and adapts a bespoke baseline model for each\nstructure, learning from its undamaged state's response characteristics. After\nlearning from just 3 hours of data, it can autonomously detect and localize\ndifferent types of unforeseen damage. Results from numerical simulations and\nexperiments indicate that incorporating the mechanical characteristics into the\nautoencoder allows for up to a 35% improvement in the detection and\nlocalization of minor damage over a standard autoencoder. Our approach holds\nsignificant promise for reducing human intervention and inspection costs while\nenabling proactive and preventive maintenance strategies. This will extend the\nlifespan, reliability, and sustainability of civil infrastructures.\n","authors":["Xuyang Li","Hamed Bolandi","Mahdi Masmoudi","Talal Salem","Nizar Lajnef","Vishnu Naresh Boddeti"],"pdf_url":"https://arxiv.org/pdf/2402.15492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13195v1","updated":"2024-07-18T06:16:09Z","published":"2024-07-18T06:16:09Z","title":"Adaptive Foundation Models for Online Decisions: HyperAgent with Fast\n  Incremental Uncertainty Estimation","summary":"  Foundation models often struggle with uncertainty when faced with new\nsituations in online decision-making, necessitating scalable and efficient\nexploration to resolve this uncertainty. We introduce GPT-HyperAgent, an\naugmentation of GPT with HyperAgent for uncertainty-aware, scalable exploration\nin contextual bandits, a fundamental online decision problem involving natural\nlanguage input. We prove that HyperAgent achieves fast incremental uncertainty\nestimation with $\\tilde{O}(\\log T)$ per-step computational complexity over $T$\nperiods under the linear realizable assumption. Our analysis demonstrates that\nHyperAgent's regret order matches that of exact Thompson sampling in linear\ncontextual bandits, closing a significant theoretical gap in scalable\nexploration. Empirical results in real-world contextual bandit tasks, such as\nautomated content moderation with human feedback, validate the practical\neffectiveness of GPT-HyperAgent for safety-critical decisions. Our code is\nopen-sourced at \\url{https://github.com/szrlee/GPT-HyperAgent/}.\n","authors":["Yingru Li","Jiawei Xu","Zhi-Quan Luo"],"pdf_url":"https://arxiv.org/pdf/2407.13195v1.pdf","comment":"41 pages"},{"id":"http://arxiv.org/abs/2407.13194v1","updated":"2024-07-18T06:16:03Z","published":"2024-07-18T06:16:03Z","title":"Robust Multivariate Time Series Forecasting against Intra- and\n  Inter-Series Transitional Shift","summary":"  The non-stationary nature of real-world Multivariate Time Series (MTS) data\npresents forecasting models with a formidable challenge of the time-variant\ndistribution of time series, referred to as distribution shift. Existing\nstudies on the distribution shift mostly adhere to adaptive normalization\ntechniques for alleviating temporal mean and covariance shifts or time-variant\nmodeling for capturing temporal shifts. Despite improving model generalization,\nthese normalization-based methods often assume a time-invariant transition\nbetween outputs and inputs but disregard specific intra-/inter-series\ncorrelations, while time-variant models overlook the intrinsic causes of the\ndistribution shift. This limits model expressiveness and interpretability of\ntackling the distribution shift for MTS forecasting. To mitigate such a\ndilemma, we present a unified Probabilistic Graphical Model to Jointly\ncapturing intra-/inter-series correlations and modeling the time-variant\ntransitional distribution, and instantiate a neural framework called JointPGM\nfor non-stationary MTS forecasting. Specifically, JointPGM first employs\nmultiple Fourier basis functions to learn dynamic time factors and designs two\ndistinct learners: intra-series and inter-series learners. The intra-series\nlearner effectively captures temporal dynamics by utilizing temporal gates,\nwhile the inter-series learner explicitly models spatial dynamics through\nmulti-hop propagation, incorporating Gumbel-softmax sampling. These two types\nof series dynamics are subsequently fused into a latent variable, which is\ninversely employed to infer time factors, generate final prediction, and\nperform reconstruction. We validate the effectiveness and efficiency of\nJointPGM through extensive experiments on six highly non-stationary MTS\ndatasets, achieving state-of-the-art forecasting performance of MTS\nforecasting.\n","authors":["Hui He","Qi Zhang","Kun Yi","Xiaojun Xue","Shoujin Wang","Liang Hu","Longbing Cao"],"pdf_url":"https://arxiv.org/pdf/2407.13194v1.pdf","comment":"19 pages, 11 figures"},{"id":"http://arxiv.org/abs/2310.10237v2","updated":"2024-07-18T06:12:41Z","published":"2023-10-16T09:51:24Z","title":"SGOOD: Substructure-enhanced Graph-Level Out-of-Distribution Detection","summary":"  Graph-level representation learning is important in a wide range of\napplications. Existing graph-level models are generally built on i.i.d.\nassumption for both training and testing graphs. However, in an open world,\nmodels can encounter out-of-distribution (OOD) testing graphs that are from\ndifferent distributions unknown during training. A trustworthy model should be\nable to detect OOD graphs to avoid unreliable predictions, while producing\naccurate in-distribution (ID) predictions. To achieve this, we present SGOOD, a\nnovel graph-level OOD detection framework. We find that substructure\ndifferences commonly exist between ID and OOD graphs, and design SGOOD with a\nseries of techniques to encode task-agnostic substructures for effective OOD\ndetection. Specifically, we build a super graph of substructures for every\ngraph, and develop a two-level graph encoding pipeline that works on both\noriginal graphs and super graphs to obtain substructure-enhanced graph\nrepresentations. We then devise substructure-preserving graph augmentation\ntechniques to further capture more substructure semantics of ID graphs.\nExtensive experiments against 11 competitors on numerous graph datasets\ndemonstrate the superiority of SGOOD, often surpassing existing methods by a\nsignificant margin. The code is available at https://github.com/TommyDzh/SGOOD.\n","authors":["Zhihao Ding","Jieming Shi","Shiqi Shen","Xuequn Shang","Jiannong Cao","Zhipeng Wang","Zhi Gong"],"pdf_url":"https://arxiv.org/pdf/2310.10237v2.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2403.01071v2","updated":"2024-07-18T06:05:58Z","published":"2024-03-02T02:28:20Z","title":"GraphRCG: Self-Conditioned Graph Generation","summary":"  Graph generation generally aims to create new graphs that closely align with\na specific graph distribution. Existing works often implicitly capture this\ndistribution through the optimization of generators, potentially overlooking\nthe intricacies of the distribution itself. Furthermore, these approaches\ngenerally neglect the insights offered by the learned distribution for graph\ngeneration. In contrast, in this work, we propose a novel self-conditioned\ngraph generation framework designed to explicitly model graph distributions and\nemploy these distributions to guide the generation process. We first perform\nself-conditioned modeling to capture the graph distributions by transforming\neach graph sample into a low-dimensional representation and optimizing a\nrepresentation generator to create new representations reflective of the\nlearned distribution. Subsequently, we leverage these bootstrapped\nrepresentations as self-conditioned guidance for the generation process,\nthereby facilitating the generation of graphs that more accurately reflect the\nlearned distributions. We conduct extensive experiments on generic and\nmolecular graph datasets across various fields. Our framework demonstrates\nsuperior performance over existing state-of-the-art graph generation methods in\nterms of graph quality and fidelity to training data.\n","authors":["Song Wang","Zhen Tan","Xinyu Zhao","Tianlong Chen","Huan Liu","Jundong Li"],"pdf_url":"https://arxiv.org/pdf/2403.01071v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02460v3","updated":"2024-07-18T05:59:52Z","published":"2023-09-04T09:01:56Z","title":"Effective Illicit Account Detection on Large Cryptocurrency MultiGraphs","summary":"  Cryptocurrencies are rapidly expanding and becoming vital in digital\nfinancial markets. However, the rise in cryptocurrency-related illicit\nactivities has led to significant losses for users. To protect the security of\nthese platforms, it is critical to identify illicit accounts effectively.\nCurrent detection methods mainly depend on feature engineering or are\ninadequate to leverage the complex information within cryptocurrency\ntransaction networks, resulting in suboptimal performance. In this paper, we\npresent DIAM, an effective method for detecting illicit accounts in\ncryptocurrency transaction networks modeled by directed multi-graphs with\nattributed edges. DIAM first features an Edge2Seq module that captures\nintrinsic transaction patterns from parallel edges by considering edge\nattributes and their directed sequences, to generate effective node\nrepresentations. Then in DIAM, we design a multigraph Discrepancy (MGD) module\nwith a tailored message passing mechanism to capture the discrepant features\nbetween normal and illicit nodes over the multigraph topology, assisted by an\nattention mechanism. DIAM integrates these techniques for end-to-end training\nto detect illicit accounts from legitimate ones. Extensive experiments,\ncomparing against 15 existing solutions on 4 large cryptocurrency datasets of\nBitcoin and Ethereum, demonstrate that DIAM consistently outperforms others in\naccurately identifying illicit accounts. For example, on a Bitcoin dataset with\n20 million nodes and 203 million edges, DIAM attains an F1 score of 96.55%,\nmarkedly surpassing the runner-up's score of 83.92%. The code is available at\nhttps://github.com/TommyDzh/DIAM.\n","authors":["Zhihao Ding","Jieming Shi","Qing Li","Jiannong Cao"],"pdf_url":"https://arxiv.org/pdf/2309.02460v3.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2407.13189v1","updated":"2024-07-18T05:57:30Z","published":"2024-07-18T05:57:30Z","title":"Data-Driven Estimation of Conditional Expectations, Application to\n  Optimal Stopping and Reinforcement Learning","summary":"  When the underlying conditional density is known, conditional expectations\ncan be computed analytically or numerically. When, however, such knowledge is\nnot available and instead we are given a collection of training data, the goal\nof this work is to propose simple and purely data-driven means for estimating\ndirectly the desired conditional expectation. Because conditional expectations\nappear in the description of a number of stochastic optimization problems with\nthe corresponding optimal solution satisfying a system of nonlinear equations,\nwe extend our data-driven method to cover such cases as well. We test our\nmethodology by applying it to Optimal Stopping and Optimal Action Policy in\nReinforcement Learning.\n","authors":["George V. Moustakides"],"pdf_url":"https://arxiv.org/pdf/2407.13189v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.13102v2","updated":"2024-07-18T05:45:45Z","published":"2023-11-22T02:04:35Z","title":"Detecting out-of-distribution text using topological features of\n  transformer-based language models","summary":"  To safeguard machine learning systems that operate on textual data against\nout-of-distribution (OOD) inputs that could cause unpredictable behaviour, we\nexplore the use of topological features of self-attention maps from\ntransformer-based language models to detect when input text is out of\ndistribution. Self-attention forms the core of transformer-based language\nmodels, dynamically assigning vectors to words based on context, thus in theory\nour methodology is applicable to any transformer-based language model with\nmultihead self-attention. We evaluate our approach on BERT and compare it to a\ntraditional OOD approach using CLS embeddings. Our results show that our\napproach outperforms CLS embeddings in distinguishing in-distribution samples\nfrom far-out-of-domain samples, but struggles with near or same-domain\ndatasets.\n","authors":["Andres Pollano","Anupam Chaudhuri","Anj Simmons"],"pdf_url":"https://arxiv.org/pdf/2311.13102v2.pdf","comment":"8 pages, 6 figures, 3 tables, to be published in proceedings of the\n  IJCAI-2024 AISafety Workshop"},{"id":"http://arxiv.org/abs/2312.00024v4","updated":"2024-07-18T05:42:57Z","published":"2023-11-13T08:54:37Z","title":"Can LLMs Patch Security Issues?","summary":"  Large Language Models (LLMs) have shown impressive proficiency in code\ngeneration. Unfortunately, these models share a weakness with their human\ncounterparts: producing code that inadvertently has security vulnerabilities.\nThese vulnerabilities could allow unauthorized attackers to access sensitive\ndata or systems, which is unacceptable for safety-critical applications. In\nthis work, we propose Feedback-Driven Security Patching (FDSP), where LLMs\nautomatically refine generated, vulnerable code. Our approach leverages\nautomatic static code analysis to empower the LLM to generate and implement\npotential solutions to address vulnerabilities. We address the research\ncommunitys needs for safe code generation by introducing a large-scale dataset,\nPythonSecurityEval, covering the diversity of real-world applications,\nincluding databases, websites and operating systems. We empirically validate\nthat FDSP outperforms prior work that uses self-feedback from LLMs by up to\n17.6% through our procedure that injects targeted, external feedback. Code and\ndata are available at \\url{https://github.com/Kamel773/LLM-code-refine}\n","authors":["Kamel Alrashedy","Abdullah Aljasser","Pradyumna Tambwekar","Matthew Gombolay"],"pdf_url":"https://arxiv.org/pdf/2312.00024v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13182v1","updated":"2024-07-18T05:40:50Z","published":"2024-07-18T05:40:50Z","title":"SpaDiT: Diffusion Transformer for Spatial Gene Expression Prediction\n  using scRNA-seq","summary":"  The rapid development of spatial transcriptomics (ST) technologies is\nrevolutionizing our understanding of the spatial organization of biological\ntissues. Current ST methods, categorized into next-generation sequencing-based\n(seq-based) and fluorescence in situ hybridization-based (image-based) methods,\noffer innovative insights into the functional dynamics of biological tissues.\nHowever, these methods are limited by their cellular resolution and the\nquantity of genes they can detect. To address these limitations, we propose\nSpaDiT, a deep learning method that utilizes a diffusion generative model to\nintegrate scRNA-seq and ST data for the prediction of undetected genes. By\nemploying a Transformer-based diffusion model, SpaDiT not only accurately\npredicts unknown genes but also effectively generates the spatial structure of\nST genes. We have demonstrated the effectiveness of SpaDiT through extensive\nexperiments on both seq-based and image-based ST data. SpaDiT significantly\ncontributes to ST gene prediction methods with its innovative approach.\nCompared to eight leading baseline methods, SpaDiT achieved state-of-the-art\nperformance across multiple metrics, highlighting its substantial\nbioinformatics contribution.\n","authors":["Xiaoyu Li","Fangfang Zhu","Wenwen Min"],"pdf_url":"https://arxiv.org/pdf/2407.13182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05005v2","updated":"2024-07-18T05:37:49Z","published":"2024-07-06T08:57:22Z","title":"Personalized Federated Domain-Incremental Learning based on Adaptive\n  Knowledge Matching","summary":"  This paper focuses on Federated Domain-Incremental Learning (FDIL) where each\nclient continues to learn incremental tasks where their domain shifts from each\nother. We propose a novel adaptive knowledge matching-based personalized FDIL\napproach (pFedDIL) which allows each client to alternatively utilize\nappropriate incremental task learning strategy on the correlation with the\nknowledge from previous tasks. More specifically, when a new task arrives, each\nclient first calculates its local correlations with previous tasks. Then, the\nclient can choose to adopt a new initial model or a previous model with similar\nknowledge to train the new task and simultaneously migrate knowledge from\nprevious tasks based on these correlations. Furthermore, to identify the\ncorrelations between the new task and previous tasks for each client, we\nseparately employ an auxiliary classifier to each target classification model\nand propose sharing partial parameters between the target classification model\nand the auxiliary classifier to condense model parameters. We conduct extensive\nexperiments on several datasets of which results demonstrate that pFedDIL\noutperforms state-of-the-art methods by up to 14.35\\% in terms of average\naccuracy of all tasks.\n","authors":["Yichen Li","Wenchao Xu","Haozhao Wang","Ruixuan Li","Yining Qi","Jingcai Guo"],"pdf_url":"https://arxiv.org/pdf/2407.05005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12484v2","updated":"2024-07-18T05:35:01Z","published":"2023-12-19T16:54:03Z","title":"SkyMask: Attack-agnostic Robust Federated Learning with Fine-grained\n  Learnable Masks","summary":"  Federated Learning (FL) is becoming a popular paradigm for leveraging\ndistributed data and preserving data privacy. However, due to the distributed\ncharacteristic, FL systems are vulnerable to Byzantine attacks that compromised\nclients attack the global model by uploading malicious model updates. With the\ndevelopment of layer-level and parameter-level fine-grained attacks, the\nattacks' stealthiness and effectiveness have been significantly improved. The\nexisting defense mechanisms solely analyze the model-level statistics of\nindividual model updates uploaded by clients to mitigate Byzantine attacks,\nwhich are ineffective against fine-grained attacks due to unawareness or\noverreaction. To address this problem, we propose SkyMask, a new\nattack-agnostic robust FL system that firstly leverages fine-grained learnable\nmasks to identify malicious model updates at the parameter level. Specifically,\nthe FL server freezes and multiplies the model updates uploaded by clients with\nthe parameter-level masks, and trains the masks over a small clean dataset\n(i.e., root dataset) to learn the subtle difference between benign and\nmalicious model updates in a high-dimension space. Our extensive experiments\ninvolve different models on three public datasets under state-of-the-art (SOTA)\nattacks, where the results show that SkyMask achieves up to 14% higher testing\naccuracy compared with SOTA defense strategies under the same attacks and\nsuccessfully defends against attacks with malicious clients of a high fraction\nup to 80%. Code is available at https://github.com/KoalaYan/SkyMask.\n","authors":["Peishen Yan","Hao Wang","Tao Song","Yang Hua","Ruhui Ma","Ningxin Hu","Mohammad R. Haghighat","Haibing Guan"],"pdf_url":"https://arxiv.org/pdf/2312.12484v2.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.13174v1","updated":"2024-07-18T05:28:50Z","published":"2024-07-18T05:28:50Z","title":"Compressed models are NOT miniature versions of large models","summary":"  Large neural models are often compressed before deployment. Model compression\nis necessary for many practical reasons, such as inference latency, memory\nfootprint, and energy consumption. Compressed models are assumed to be\nminiature versions of corresponding large neural models. However, we question\nthis belief in our work. We compare compressed models with corresponding large\nneural models using four model characteristics: prediction errors, data\nrepresentation, data distribution, and vulnerability to adversarial attack. We\nperform experiments using the BERT-large model and its five compressed\nversions. For all four model characteristics, compressed models significantly\ndiffer from the BERT-large model. Even among compressed models, they differ\nfrom each other on all four model characteristics. Apart from the expected loss\nin model performance, there are major side effects of using compressed models\nto replace large neural models.\n","authors":["Rohit Raj Rai","Rishant Pal","Amit Awekar"],"pdf_url":"https://arxiv.org/pdf/2407.13174v1.pdf","comment":"Accepted at the 33rd ACM International Conference on Information and\n  Knowledge Management (CIKM 2024) for the Short Research Paper track, 5 pages"},{"id":"http://arxiv.org/abs/2407.13158v1","updated":"2024-07-18T04:58:27Z","published":"2024-07-18T04:58:27Z","title":"HHGT: Hierarchical Heterogeneous Graph Transformer for Heterogeneous\n  Graph Representation Learning","summary":"  Despite the success of Heterogeneous Graph Neural Networks (HGNNs) in\nmodeling real-world Heterogeneous Information Networks (HINs), challenges such\nas expressiveness limitations and over-smoothing have prompted researchers to\nexplore Graph Transformers (GTs) for enhanced HIN representation learning.\nHowever, research on GT in HINs remains limited, with two key shortcomings in\nexisting work: (1) A node's neighbors at different distances in HINs convey\ndiverse semantics. Unfortunately, existing methods ignore such differences and\nuniformly treat neighbors within a given distance in a coarse manner, which\nresults in semantic confusion. (2) Nodes in HINs have various types, each with\nunique semantics. Nevertheless, existing methods mix nodes of different types\nduring neighbor aggregation, hindering the capture of proper correlations\nbetween nodes of diverse types. To bridge these gaps, we design an innovative\nstructure named (k,t)-ring neighborhood, where nodes are initially organized by\ntheir distance, forming different non-overlapping k-ring neighborhoods for each\ndistance. Within each k-ring structure, nodes are further categorized into\ndifferent groups according to their types, thus emphasizing the heterogeneity\nof both distances and types in HINs naturally. Based on this structure, we\npropose a novel Hierarchical Heterogeneous Graph Transformer (HHGT) model,\nwhich seamlessly integrates a Type-level Transformer for aggregating nodes of\ndifferent types within each k-ring neighborhood, followed by a Ring-level\nTransformer for aggregating different k-ring neighborhoods in a hierarchical\nmanner. Extensive experiments are conducted on downstream tasks to verify\nHHGT's superiority over 14 baselines, with a notable improvement of up to\n24.75% in NMI and 29.25% in ARI for node clustering task on the ACM dataset\ncompared to the best baseline.\n","authors":["Qiuyu Zhu","Liang Zhang","Qianxiong Xu","Kaijun Liu","Cheng Long","Xiaoyang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13153v1","updated":"2024-07-18T04:42:01Z","published":"2024-07-18T04:42:01Z","title":"Preset-Voice Matching for Privacy Regulated Speech-to-Speech Translation\n  Systems","summary":"  In recent years, there has been increased demand for speech-to-speech\ntranslation (S2ST) systems in industry settings. Although successfully\ncommercialized, cloning-based S2ST systems expose their distributors to\nliabilities when misused by individuals and can infringe on personality rights\nwhen exploited by media organizations. This work proposes a regulated S2ST\nframework called Preset-Voice Matching (PVM). PVM removes cross-lingual voice\ncloning in S2ST by first matching the input voice to a similar prior consenting\nspeaker voice in the target-language. With this separation, PVM avoids cloning\nthe input speaker, ensuring PVM systems comply with regulations and reduce risk\nof misuse. Our results demonstrate PVM can significantly improve S2ST system\nrun-time in multi-speaker settings and the naturalness of S2ST synthesized\nspeech. To our knowledge, PVM is the first explicitly regulated S2ST framework\nleveraging similarly-matched preset-voices for dynamic S2ST tasks.\n","authors":["Daniel Platnick","Bishoy Abdelnour","Eamon Earl","Rahul Kumar","Zahra Rezaei","Thomas Tsangaris","Faraj Lagum"],"pdf_url":"https://arxiv.org/pdf/2407.13153v1.pdf","comment":"Accepted to the ACL PrivateNLP 2024 Workshop, 7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.07693v3","updated":"2024-07-18T04:24:57Z","published":"2024-06-11T20:14:22Z","title":"A Labelled Dataset for Sentiment Analysis of Videos on YouTube, TikTok,\n  and Other Sources about the 2024 Outbreak of Measles","summary":"  The work of this paper presents a dataset that contains the data of 4011\nvideos about the ongoing outbreak of measles published on 264 websites on the\ninternet between January 1, 2024, and May 31, 2024. The dataset is available at\nhttps://dx.doi.org/10.21227/40s8-xf63. These websites primarily include YouTube\nand TikTok, which account for 48.6% and 15.2% of the videos, respectively. The\nremainder of the websites include Instagram and Facebook as well as the\nwebsites of various global and local news organizations. For each of these\nvideos, the URL of the video, title of the post, description of the post, and\nthe date of publication of the video are presented as separate attributes in\nthe dataset. After developing this dataset, sentiment analysis (using VADER),\nsubjectivity analysis (using TextBlob), and fine-grain sentiment analysis\n(using DistilRoBERTa-base) of the video titles and video descriptions were\nperformed. This included classifying each video title and video description\ninto (i) one of the sentiment classes i.e. positive, negative, or neutral, (ii)\none of the subjectivity classes i.e. highly opinionated, neutral opinionated,\nor least opinionated, and (iii) one of the fine-grain sentiment classes i.e.\nfear, surprise, joy, sadness, anger, disgust, or neutral. These results are\npresented as separate attributes in the dataset for the training and testing of\nmachine learning algorithms for performing sentiment analysis or subjectivity\nanalysis in this field as well as for other applications. Finally, this paper\nalso presents a list of open research questions that may be investigated using\nthis dataset.\n","authors":["Nirmalya Thakur","Vanessa Su","Mingchen Shao","Kesha A. Patel","Hongseok Jeong","Victoria Knieling","Andrew Bian"],"pdf_url":"https://arxiv.org/pdf/2406.07693v3.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2407.13146v1","updated":"2024-07-18T04:18:52Z","published":"2024-07-18T04:18:52Z","title":"PG-Rainbow: Using Distributional Reinforcement Learning in Policy\n  Gradient Methods","summary":"  This paper introduces PG-Rainbow, a novel algorithm that incorporates a\ndistributional reinforcement learning framework with a policy gradient\nalgorithm. Existing policy gradient methods are sample inefficient and rely on\nthe mean of returns when calculating the state-action value function,\nneglecting the distributional nature of returns in reinforcement learning\ntasks. To address this issue, we use an Implicit Quantile Network that provides\nthe quantile information of the distribution of rewards to the critic network\nof the Proximal Policy Optimization algorithm. We show empirical results that\nthrough the integration of reward distribution information into the policy\nnetwork, the policy agent acquires enhanced capabilities to comprehensively\nevaluate the consequences of potential actions in a given state, facilitating\nmore sophisticated and informed decision-making processes. We evaluate the\nperformance of the proposed algorithm in the Atari-2600 game suite, simulated\nvia the Arcade Learning Environment (ALE).\n","authors":["WooJae Jeon","KanJun Lee","Jeewoo Lee"],"pdf_url":"https://arxiv.org/pdf/2407.13146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13143v1","updated":"2024-07-18T04:02:35Z","published":"2024-07-18T04:02:35Z","title":"Integrated Hardware Architecture and Device Placement Search","summary":"  Distributed execution of deep learning training involves a dynamic interplay\nbetween hardware accelerator architecture and device placement strategy. This\nis the first work to explore the co-optimization of determining the optimal\narchitecture and device placement strategy through novel algorithms, improving\nthe balance of computational resources, memory usage, and data distribution.\nOur architecture search leverages tensor and vector units, determining their\nquantity and dimensionality, and on-chip and off-chip memory configurations. It\nalso determines the microbatch size and decides whether to recompute or stash\nactivations, balancing the memory footprint of training and storage size. For\neach explored architecture configuration, we use an Integer Linear Program\n(ILP) to find the optimal schedule for executing operators on the accelerator.\nThe ILP results then integrate with a dynamic programming solution to identify\nthe most effective device placement strategy, combining data, pipeline, and\ntensor model parallelism across multiple accelerators. Our approach achieves\nhigher throughput on large language models compared to the state-of-the-art\nTPUv4 and the Spotlight accelerator search framework. The entire source code of\nPHAZE is available at https://github.com/msr-fiddle/phaze.\n","authors":["Irene Wang","Jakub Tarnawski","Amar Phanishayee","Divya Mahajan"],"pdf_url":"https://arxiv.org/pdf/2407.13143v1.pdf","comment":"Accepted at the 41st International Conference on Machine Learning\n  (ICML), 2024"},{"id":"http://arxiv.org/abs/2407.13142v1","updated":"2024-07-18T04:01:12Z","published":"2024-07-18T04:01:12Z","title":"A light-weight and efficient punctuation and word casing prediction\n  model for on-device streaming ASR","summary":"  Punctuation and word casing prediction are necessary for automatic speech\nrecognition (ASR). With the popularity of on-device end-to-end streaming ASR\nsystems, the on-device punctuation and word casing prediction become a\nnecessity while we found little discussion on this. With the emergence of\nTransformer, Transformer based models have been explored for this scenario.\nHowever, Transformer based models are too large for on-device ASR systems. In\nthis paper, we propose a light-weight and efficient model that jointly predicts\npunctuation and word casing in real time. The model is based on Convolutional\nNeural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM).\nExperimental results on the IWSLT2011 test set show that the proposed model\nobtains 9% relative improvement compared to the best of non-Transformer models\non overall F1-score. Compared to the representative of Transformer based\nmodels, the proposed model achieves comparable results to the representative\nmodel while being only one-fortieth its size and 2.5 times faster in terms of\ninference time. It is suitable for on-device streaming ASR systems. Our code is\npublicly available.\n","authors":["Jian You","Xiangfeng Li"],"pdf_url":"https://arxiv.org/pdf/2407.13142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13141v1","updated":"2024-07-18T03:57:08Z","published":"2024-07-18T03:57:08Z","title":"Out-of-Distribution Detection through Soft Clustering with Non-Negative\n  Kernel Regression","summary":"  As language models become more general purpose, increased attention needs to\nbe paid to detecting out-of-distribution (OOD) instances, i.e., those not\nbelonging to any of the distributions seen during training. Existing methods\nfor detecting OOD data are computationally complex and storage-intensive. We\npropose a novel soft clustering approach for OOD detection based on\nnon-negative kernel regression. Our approach greatly reduces computational and\nspace complexities (up to 11x improvement in inference time and 87% reduction\nin storage requirements) and outperforms existing approaches by up to 4 AUROC\npoints on four different benchmarks. We also introduce an entropy-constrained\nversion of our algorithm, which leads to further reductions in storage\nrequirements (up to 97% lower than comparable approaches) while retaining\ncompetitive performance. Our soft clustering approach for OOD detection\nhighlights its potential for detecting tail-end phenomena in extreme-scale data\nsettings.\n","authors":["Aryan Gulati","Xingjian Dong","Carlos Hurtado","Sarath Shekkizhar","Swabha Swayamdipta","Antonio Ortega"],"pdf_url":"https://arxiv.org/pdf/2407.13141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14951v2","updated":"2024-07-18T03:34:50Z","published":"2024-03-22T05:04:48Z","title":"Simple Graph Condensation","summary":"  The burdensome training costs on large-scale graphs have aroused significant\ninterest in graph condensation, which involves tuning Graph Neural Networks\n(GNNs) on a small condensed graph for use on the large-scale original graph.\nExisting methods primarily focus on aligning key metrics between the condensed\nand original graphs, such as gradients, output distribution and trajectories of\nGNNs, yielding satisfactory performance on downstream tasks. However, these\ncomplex metrics necessitate intricate external parameters and can potentially\ndisrupt the optimization process of the condensation graph, making the\ncondensation process highly demanding and unstable. Motivated by the recent\nsuccess of simplified models across various domains, we propose a simplified\napproach to metric alignment in graph condensation, aiming to reduce\nunnecessary complexity inherited from intricate metrics. We introduce the\nSimple Graph Condensation (SimGC) framework, which aligns the condensed graph\nwith the original graph from the input layer to the prediction layer, guided by\na pre-trained Simple Graph Convolution (SGC) model on the original graph.\nImportantly, SimGC eliminates external parameters and exclusively retains the\ntarget condensed graph during the condensation process. This straightforward\nyet effective strategy achieves a significant speedup of up to 10 times\ncompared to existing graph condensation methods while performing on par with\nstate-of-the-art baselines. Comprehensive experiments conducted on seven\nbenchmark datasets demonstrate the effectiveness of SimGC in prediction\naccuracy, condensation time, and generalization capability. Our code is\navailable at https://github.com/BangHonor/SimGC.\n","authors":["Zhenbang Xiao","Yu Wang","Shunyu Liu","Huiqiong Wang","Mingli Song","Tongya Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.14951v2.pdf","comment":"ECML-PKDD 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.13676v1","updated":"2024-07-18T16:51:15Z","published":"2024-07-18T16:51:15Z","title":"Aligning Sight and Sound: Advanced Sound Source Localization Through\n  Audio-Visual Alignment","summary":"  Recent studies on learning-based sound source localization have mainly\nfocused on the localization performance perspective. However, prior work and\nexisting benchmarks overlook a crucial aspect: cross-modal interaction, which\nis essential for interactive sound source localization. Cross-modal interaction\nis vital for understanding semantically matched or mismatched audio-visual\nevents, such as silent objects or off-screen sounds. In this paper, we first\ncomprehensively examine the cross-modal interaction of existing methods,\nbenchmarks, evaluation metrics, and cross-modal understanding tasks. Then, we\nidentify the limitations of previous studies and make several contributions to\novercome the limitations. First, we introduce a new synthetic benchmark for\ninteractive sound source localization. Second, we introduce new evaluation\nmetrics to rigorously assess sound source localization methods, focusing on\naccurately evaluating both localization performance and cross-modal interaction\nability. Third, we propose a learning framework with a cross-modal alignment\nstrategy to enhance cross-modal interaction. Lastly, we evaluate both\ninteractive sound source localization and auxiliary cross-modal retrieval tasks\ntogether to thoroughly assess cross-modal interaction capabilities and\nbenchmark competing methods. Our new benchmarks and evaluation metrics reveal\npreviously overlooked issues in sound source localization studies. Our proposed\nnovel method, with enhanced cross-modal alignment, shows superior sound source\nlocalization performance. This work provides the most comprehensive analysis of\nsound source localization to date, with extensive validation of competing\nmethods on both existing and new benchmarks using new and standard evaluation\nmetrics.\n","authors":["Arda Senocak","Hyeonggon Ryu","Junsik Kim","Tae-Hyun Oh","Hanspeter Pfister","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2407.13676v1.pdf","comment":"Journal Extension of ICCV 2023 paper (arXiV:2309.10724). Code is\n  available at https://github.com/kaistmm/SSLalignment"},{"id":"http://arxiv.org/abs/2407.13488v1","updated":"2024-07-18T13:08:55Z","published":"2024-07-18T13:08:55Z","title":"Similarity over Factuality: Are we making progress on multimodal\n  out-of-context misinformation detection?","summary":"  Out-of-context (OOC) misinformation poses a significant challenge in\nmultimodal fact-checking, where images are paired with texts that misrepresent\ntheir original context to support false narratives. Recent research in\nevidence-based OOC detection has seen a trend towards increasingly complex\narchitectures, incorporating Transformers, foundation models, and large\nlanguage models. In this study, we introduce a simple yet robust baseline,\nwhich assesses MUltimodal SimilaritiEs (MUSE), specifically the similarity\nbetween image-text pairs and external image and text evidence. Our results\ndemonstrate that MUSE, when used with conventional classifiers like Decision\nTree, Random Forest, and Multilayer Perceptron, can compete with and even\nsurpass the state-of-the-art on the NewsCLIPpings and VERITE datasets.\nFurthermore, integrating MUSE in our proposed \"Attentive Intermediate\nTransformer Representations\" (AITR) significantly improved performance, by 3.3%\nand 7.5% on NewsCLIPpings and VERITE, respectively. Nevertheless, the success\nof MUSE, relying on surface-level patterns and shortcuts, without examining\nfactuality and logical inconsistencies, raises critical questions about how we\ndefine the task, construct datasets, collect external evidence and overall, how\nwe assess progress in the field. We release our code at:\nhttps://github.com/stevejpapad/outcontext-misinfo-progress\n","authors":["Stefanos-Iordanis Papadopoulos","Christos Koutlis","Symeon Papadopoulos","Panagiotis C. Petrantonakis"],"pdf_url":"https://arxiv.org/pdf/2407.13488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13309v1","updated":"2024-07-18T09:13:08Z","published":"2024-07-18T09:13:08Z","title":"Exposure Completing for Temporally Consistent Neural High Dynamic Range\n  Video Rendering","summary":"  High dynamic range (HDR) video rendering from low dynamic range (LDR) videos\nwhere frames are of alternate exposure encounters significant challenges, due\nto the exposure change and absence at each time stamp. The exposure change and\nabsence make existing methods generate flickering HDR results. In this paper,\nwe propose a novel paradigm to render HDR frames via completing the absent\nexposure information, hence the exposure information is complete and\nconsistent. Our approach involves interpolating neighbor LDR frames in the time\ndimension to reconstruct LDR frames for the absent exposures. Combining the\ninterpolated and given LDR frames, the complete set of exposure information is\navailable at each time stamp. This benefits the fusing process for HDR results,\nreducing noise and ghosting artifacts therefore improving temporal consistency.\nExtensive experimental evaluations on standard benchmarks demonstrate that our\nmethod achieves state-of-the-art performance, highlighting the importance of\nabsent exposure completing in HDR video rendering. The code is available at\nhttps://github.com/cuijiahao666/NECHDR.\n","authors":["Jiahao Cui","Wei Jiang","Zhan Peng","Zhiyu Pan","Zhiguo Cao"],"pdf_url":"https://arxiv.org/pdf/2407.13309v1.pdf","comment":"9 pages, 6 figures, accepted by ACM-MM 2024"},{"id":"http://arxiv.org/abs/2405.16640v2","updated":"2024-07-18T09:01:52Z","published":"2024-05-26T17:31:21Z","title":"A Survey of Multimodal Large Language Model from A Data-centric\n  Perspective","summary":"  Multimodal large language models (MLLMs) enhance the capabilities of standard\nlarge language models by integrating and processing data from multiple\nmodalities, including text, vision, audio, video, and 3D environments. Data\nplays a pivotal role in the development and refinement of these models. In this\nsurvey, we comprehensively review the literature on MLLMs from a data-centric\nperspective. Specifically, we explore methods for preparing multimodal data\nduring the pretraining and adaptation phases of MLLMs. Additionally, we analyze\nthe evaluation methods for the datasets and review the benchmarks for\nevaluating MLLMs. Our survey also outlines potential future research\ndirections. This work aims to provide researchers with a detailed understanding\nof the data-driven aspects of MLLMs, fostering further exploration and\ninnovation in this field.\n","authors":["Tianyi Bai","Hao Liang","Binwang Wan","Yanran Xu","Xi Li","Shiyu Li","Ling Yang","Bozhou Li","Yifan Wang","Bin Cui","Ping Huang","Jiulong Shan","Conghui He","Binhang Yuan","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.16640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05628v2","updated":"2024-07-18T08:00:38Z","published":"2024-03-08T19:02:21Z","title":"AMUSE: Adaptive Multi-Segment Encoding for Dataset Watermarking","summary":"  Curating high quality datasets that play a key role in the emergence of new\nAI applications requires considerable time, money, and computational resources.\nSo, effective ownership protection of datasets is becoming critical. Recently,\nto protect the ownership of an image dataset, imperceptible watermarking\ntechniques are used to store ownership information (i.e., watermark) into the\nindividual image samples. Embedding the entire watermark into all samples leads\nto significant redundancy in the embedded information which damages the\nwatermarked dataset quality and extraction accuracy. In this paper, a\nmulti-segment encoding-decoding method for dataset watermarking (called AMUSE)\nis proposed to adaptively map the original watermark into a set of shorter\nsub-messages and vice versa. Our message encoder is an adaptive method that\nadjusts the length of the sub-messages according to the protection requirements\nfor the target dataset. Existing image watermarking methods are then employed\nto embed the sub-messages into the original images in the dataset and also to\nextract them from the watermarked images. Our decoder is then used to\nreconstruct the original message from the extracted sub-messages. The proposed\nencoder and decoder are plug-and-play modules that can easily be added to any\nwatermarking method. To this end, extensive experiments are preformed with\nmultiple watermarking solutions which show that applying AMUSE improves the\noverall message extraction accuracy upto 28% for the same given dataset\nquality. Furthermore, the image dataset quality is enhanced by a PSNR of\n$\\approx$2 dB on average, while improving the extraction accuracy for one of\nthe tested image watermarking methods.\n","authors":["Saeed Ranjbar Alvar","Mohammad Akbari","David Ming Xuan Yue","Yong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05628v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13117v1","updated":"2024-07-18T02:55:52Z","published":"2024-07-18T02:55:52Z","title":"SOMONITOR: Explainable Marketing Data Processing and Analysis with Large\n  Language Models","summary":"  Online marketing faces formidable challenges in managing and interpreting\nimmense volumes of data necessary for competitor analysis, content research,\nand strategic branding. It is impossible to review hundreds to thousands of\ntransient online content items by hand, and partial analysis often leads to\nsuboptimal outcomes and poorly performing campaigns. We introduce an\nexplainable AI framework SoMonitor that aims to synergize human intuition with\nAI-based efficiency, helping marketers across all stages of the marketing\nfunnel, from strategic planning to content creation and campaign execution.\nSoMonitor incorporates a CTR prediction and ranking model for advertising\ncontent and uses large language models (LLMs) to process high-performing\ncompetitor content, identifying core content pillars such as target audiences,\ncustomer needs, and product features. These pillars are then organized into\nbroader categories, including communication themes and targeted customer\npersonas. By integrating these insights with data from the brand's own\nadvertising campaigns, SoMonitor constructs a narrative for addressing new\ncustomer personas and simultaneously generates detailed content briefs in the\nform of user stories that can be directly applied by marketing teams to\nstreamline content production and campaign execution. The adoption of SoMonitor\nin daily operations allows digital marketers to quickly parse through extensive\ndatasets, offering actionable insights that significantly enhance campaign\neffectiveness and overall job satisfaction\n","authors":["Qi Yang","Sergey Nikolenko","Marlo Ongpin","Ilia Gossoudarev","Yu-Yi Chu-Farseeva","Aleksandr Farseev"],"pdf_url":"https://arxiv.org/pdf/2407.13117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13111v1","updated":"2024-07-18T02:39:31Z","published":"2024-07-18T02:39:31Z","title":"PG-Attack: A Precision-Guided Adversarial Attack Framework Against\n  Vision Foundation Models for Autonomous Driving","summary":"  Vision foundation models are increasingly employed in autonomous driving\nsystems due to their advanced capabilities. However, these models are\nsusceptible to adversarial attacks, posing significant risks to the reliability\nand safety of autonomous vehicles. Adversaries can exploit these\nvulnerabilities to manipulate the vehicle's perception of its surroundings,\nleading to erroneous decisions and potentially catastrophic consequences. To\naddress this challenge, we propose a novel Precision-Guided Adversarial Attack\n(PG-Attack) framework that combines two techniques: Precision Mask Perturbation\nAttack (PMP-Attack) and Deceptive Text Patch Attack (DTP-Attack). PMP-Attack\nprecisely targets the attack region to minimize the overall perturbation while\nmaximizing its impact on the target object's representation in the model's\nfeature space. DTP-Attack introduces deceptive text patches that disrupt the\nmodel's understanding of the scene, further enhancing the attack's\neffectiveness. Our experiments demonstrate that PG-Attack successfully deceives\na variety of advanced multi-modal large models, including GPT-4V, Qwen-VL, and\nimp-V1. Additionally, we won First-Place in the CVPR 2024 Workshop Challenge:\nBlack-box Adversarial Attacks on Vision Foundation Models and codes are\navailable at https://github.com/fuhaha824/PG-Attack.\n","authors":["Jiyuan Fu","Zhaoyu Chen","Kaixun Jiang","Haijing Guo","Shuyong Gao","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.13111v1.pdf","comment":"First-Place in the CVPR 2024 Workshop Challenge: Black-box\n  Adversarial Attacks on Vision Foundation Models"},{"id":"http://arxiv.org/abs/2407.13095v1","updated":"2024-07-18T01:57:16Z","published":"2024-07-18T01:57:16Z","title":"Audio-visual Generalized Zero-shot Learning the Easy Way","summary":"  Audio-visual generalized zero-shot learning is a rapidly advancing domain\nthat seeks to understand the intricate relations between audio and visual cues\nwithin videos. The overarching goal is to leverage insights from seen classes\nto identify instances from previously unseen ones. Prior approaches primarily\nutilized synchronized auto-encoders to reconstruct audio-visual attributes,\nwhich were informed by cross-attention transformers and projected text\nembeddings. However, these methods fell short of effectively capturing the\nintricate relationship between cross-modal features and class-label embeddings\ninherent in pre-trained language-aligned embeddings. To circumvent these\nbottlenecks, we introduce a simple yet effective framework for Easy\nAudio-Visual Generalized Zero-shot Learning, named EZ-AVGZL, that aligns\naudio-visual embeddings with transformed text representations. It utilizes a\nsingle supervised text audio-visual contrastive loss to learn an alignment\nbetween audio-visual and textual modalities, moving away from the conventional\napproach of reconstructing cross-modal features and text embeddings. Our key\ninsight is that while class name embeddings are well aligned with\nlanguage-based audio-visual features, they don't provide sufficient class\nseparation to be useful for zero-shot learning. To address this, our method\nleverages differential optimization to transform class embeddings into a more\ndiscriminative space while preserving the semantic structure of language\nrepresentations. We conduct extensive experiments on VGGSound-GZSL, UCF-GZSL,\nand ActivityNet-GZSL benchmarks. Our results demonstrate that our EZ-AVGZL\nachieves state-of-the-art performance in audio-visual generalized zero-shot\nlearning.\n","authors":["Shentong Mo","Pedro Morgado"],"pdf_url":"https://arxiv.org/pdf/2407.13095v1.pdf","comment":null}]},"2024-07-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.13048v1","updated":"2024-07-17T23:16:11Z","published":"2024-07-17T23:16:11Z","title":"Establishing Knowledge Preference in Language Models","summary":"  Language models are known to encode a great amount of factual knowledge\nthrough pretraining. However, such knowledge might be insufficient to cater to\nuser requests, requiring the model to integrate external knowledge sources and\nadhere to user-provided specifications. When answering questions about ongoing\nevents, the model should use recent news articles to update its response; when\nasked to provide recommendations, the model should prioritize user\nspecifications over retrieved product reviews; when some facts are edited in\nthe model, the updated facts should override all prior knowledge learned by the\nmodel even if they are conflicting. In all of the cases above, the model faces\na decision between its own parametric knowledge, (retrieved) contextual\nknowledge, and user instruction knowledge. In this paper, we (1) unify such\nsettings into the problem of knowledge preference and define a three-level\npreference hierarchy over these knowledge sources; (2) compile a collection of\nexisting datasets IfQA, MQuAKE, and MRQA covering a combination of settings\n(with/without user specifications, with/without context documents) to\nsystematically evaluate how well models obey the intended knowledge preference;\nand (3) propose a dataset synthesis method that composes diverse\nquestion-answer pairs with user assumptions and related context to directly\nfine-tune LMs for instilling the hierarchy of knowledge. We demonstrate that a\n7B model, fine-tuned on only a few thousand examples automatically generated by\nour proposed method, effectively achieves superior performance (more than 18%\nimprovement across all evaluation benchmarks) in adhering to the desired\nknowledge preference hierarchy.\n","authors":["Sizhe Zhou","Sha Li","Yu Meng","Yizhu Jiao","Heng Ji","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2407.13048v1.pdf","comment":"27 pages, 8 figures, 23 tables, working in progress"},{"id":"http://arxiv.org/abs/2311.03658v2","updated":"2024-07-17T22:24:27Z","published":"2023-11-07T01:59:11Z","title":"The Linear Representation Hypothesis and the Geometry of Large Language\n  Models","summary":"  Informally, the 'linear representation hypothesis' is the idea that\nhigh-level concepts are represented linearly as directions in some\nrepresentation space. In this paper, we address two closely related questions:\nWhat does \"linear representation\" actually mean? And, how do we make sense of\ngeometric notions (e.g., cosine similarity or projection) in the representation\nspace? To answer these, we use the language of counterfactuals to give two\nformalizations of \"linear representation\", one in the output (word)\nrepresentation space, and one in the input (sentence) space. We then prove\nthese connect to linear probing and model steering, respectively. To make sense\nof geometric notions, we use the formalization to identify a particular\n(non-Euclidean) inner product that respects language structure in a sense we\nmake precise. Using this causal inner product, we show how to unify all notions\nof linear representation. In particular, this allows the construction of probes\nand steering vectors using counterfactual pairs. Experiments with LLaMA-2\ndemonstrate the existence of linear representations of concepts, the connection\nto interpretation and control, and the fundamental role of the choice of inner\nproduct.\n","authors":["Kiho Park","Yo Joong Choe","Victor Veitch"],"pdf_url":"https://arxiv.org/pdf/2311.03658v2.pdf","comment":"Accepted for a presentation at ICML 2024 and an oral presentation at\n  NeurIPS 2023 Workshop on Causal Representation Learning. Code is available at\n  https://github.com/KihoPark/linear_rep_geometry"},{"id":"http://arxiv.org/abs/2407.11211v2","updated":"2024-07-17T22:23:42Z","published":"2024-07-15T19:53:02Z","title":"Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer\n  from Text to Image via CLIP Inversion","summary":"  We introduce NOVIC, an innovative uNconstrained Open Vocabulary Image\nClassifier that uses an autoregressive transformer to generatively output\nclassification labels as language. Leveraging the extensive knowledge of CLIP\nmodels, NOVIC harnesses the embedding space to enable zero-shot transfer from\npure text to images. Traditional CLIP models, despite their ability for open\nvocabulary classification, require an exhaustive prompt of potential class\nlabels, restricting their application to images of known content or context. To\naddress this, we propose an \"object decoder\" model that is trained on a\nlarge-scale 92M-target dataset of templated object noun sets and LLM-generated\ncaptions to always output the object noun in question. This effectively inverts\nthe CLIP text encoder and allows textual object labels to be generated directly\nfrom image-derived embedding vectors, without requiring any a priori knowledge\nof the potential content of an image. The trained decoders are tested on a mix\nof manually and web-curated datasets, as well as standard image classification\nbenchmarks, and achieve fine-grained prompt-free prediction scores of up to\n87.5%, a strong result considering the model must work for any conceivable\nimage and without any contextual clues.\n","authors":["Philipp Allgeuer","Kyra Ahrens","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2407.11211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13040v1","updated":"2024-07-17T22:13:42Z","published":"2024-07-17T22:13:42Z","title":"Turkish Delights: a Dataset on Turkish Euphemisms","summary":"  Euphemisms are a form of figurative language relatively understudied in\nnatural language processing. This research extends the current computational\nwork on potentially euphemistic terms (PETs) to Turkish. We introduce the\nTurkish PET dataset, the first available of its kind in the field. By creating\na list of euphemisms in Turkish, collecting example contexts, and annotating\nthem, we provide both euphemistic and non-euphemistic examples of PETs in\nTurkish. We describe the dataset and methodologies, and also experiment with\ntransformer-based models on Turkish euphemism detection by using our dataset\nfor binary classification. We compare performances across models using F1,\naccuracy, and precision as evaluation metrics.\n","authors":["Hasan Can Biyik","Patrick Lee","Anna Feldman"],"pdf_url":"https://arxiv.org/pdf/2407.13040v1.pdf","comment":"In Proceedings of The First SIGTURK workshop co-located with ACL\n  2024: https://sigturk.github.io/workshop/"},{"id":"http://arxiv.org/abs/2207.07051v4","updated":"2024-07-17T22:01:29Z","published":"2022-07-14T16:51:09Z","title":"Language models show human-like content effects on reasoning tasks","summary":"  Reasoning is a key ability for an intelligent system. Large language models\n(LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit\nmany imperfections. However, human abstract reasoning is also imperfect. For\nexample, human reasoning is affected by our real-world knowledge and beliefs,\nand shows notable \"content effects\"; humans reason more reliably when the\nsemantic content of a problem supports the correct logical inferences. These\ncontent-entangled reasoning patterns play a central role in debates about the\nfundamental nature of human intelligence. Here, we investigate whether language\nmodels $\\unicode{x2014}$ whose prior expectations capture some aspects of human\nknowledge $\\unicode{x2014}$ similarly mix content into their answers to logical\nproblems. We explored this question across three logical reasoning tasks:\nnatural language inference, judging the logical validity of syllogisms, and the\nWason selection task. We evaluate state of the art large language models, as\nwell as humans, and find that the language models reflect many of the same\npatterns observed in humans across these tasks $\\unicode{x2014}$ like humans,\nmodels answer more accurately when the semantic content of a task supports the\nlogical inferences. These parallels are reflected both in answer patterns, and\nin lower-level features like the relationship between model answer\ndistributions and human response times. Our findings have implications for\nunderstanding both these cognitive effects in humans, and the factors that\ncontribute to language model performance.\n","authors":["Ishita Dasgupta","Andrew K. Lampinen","Stephanie C. Y. Chan","Hannah R. Sheahan","Antonia Creswell","Dharshan Kumaran","James L. McClelland","Felix Hill"],"pdf_url":"https://arxiv.org/pdf/2207.07051v4.pdf","comment":"Published version of record:\n  https://academic.oup.com/pnasnexus/article/3/7/pgae233/7712372"},{"id":"http://arxiv.org/abs/2407.13035v1","updated":"2024-07-17T21:57:18Z","published":"2024-07-17T21:57:18Z","title":"Pre-Trained Foundation Model representations to uncover Breathing\n  patterns in Speech","summary":"  The process of human speech production involves coordinated respiratory\naction to elicit acoustic speech signals. Typically, speech is produced when\nair is forced from the lungs and is modulated by the vocal tract, where such\nactions are interspersed by moments of breathing in air (inhalation) to refill\nthe lungs again. Respiratory rate (RR) is a vital metric that is used to assess\nthe overall health, fitness, and general well-being of an individual. Existing\napproaches to measure RR (number of breaths one takes in a minute) are\nperformed using specialized equipment or training. Studies have demonstrated\nthat machine learning algorithms can be used to estimate RR using bio-sensor\nsignals as input. Speech-based estimation of RR can offer an effective approach\nto measure the vital metric without requiring any specialized equipment or\nsensors. This work investigates a machine learning based approach to estimate\nRR from speech segments obtained from subjects speaking to a close-talking\nmicrophone device. Data were collected from N=26 individuals, where the\ngroundtruth RR was obtained through commercial grade chest-belts and then\nmanually corrected for any errors. A convolutional long-short term memory\nnetwork (Conv-LSTM) is proposed to estimate respiration time-series data from\nthe speech signal. We demonstrate that the use of pre-trained representations\nobtained from a foundation model, such as Wav2Vec2, can be used to estimate\nrespiration-time-series with low root-mean-squared error and high correlation\ncoefficient, when compared with the baseline. The model-driven time series can\nbe used to estimate $RR$ with a low mean absolute error (MAE) ~ 1.6\nbreaths/min.\n","authors":["Vikramjit Mitra","Anirban Chatterjee","Ke Zhai","Helen Weng","Ayuko Hill","Nicole Hay","Christopher Webb","Jamie Cheng","Erdrin Azemi"],"pdf_url":"https://arxiv.org/pdf/2407.13035v1.pdf","comment":"8 pages, 6 figures, BioKDD workshop paper"},{"id":"http://arxiv.org/abs/2303.11525v4","updated":"2024-07-17T21:57:12Z","published":"2023-03-21T01:06:37Z","title":"Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training\n  Efficiency","summary":"  Recent research has focused on weight sparsity in deep neural network\ntraining to reduce FLOPs, aiming for improved efficiency (test accuracy w.r.t\ntraining FLOPs). However, sparse weight training often compromises accuracy,\nrequiring extended training schedules to attain the accuracy of dense models.\nIn contrast, our approach, Sparse Iso-FLOP Transformations (Sparse-IFT), uses\nsparsity to improve accuracy while maintaining dense model FLOPs. Using a\nsingle hyperparameter (i.e., the sparsity level), Sparse-IFTs efficiently\nreplace dense layers, expanding the search space for optimal sparse masks. In\naddition, dynamic sparse training (DST) with Sparse-IFT models effectively\nnavigate this larger sparse mask-weight space, which is evidenced by a spectral\nanalysis using Ramanujan graph properties. Our study reveals a robust\ncorrelation among mask topology, weights, and final performance. Notably,\nwithout adjusting any training hyperparameters, replacing dense layers with\nSparse-IFT yields significant improvements, such as a +3.5% boost for ResNet-18\non ImageNet and +0.9% for GPT-3 Small on the Open LLM leaderboard. To the best\nof our knowledge, this is the first work to demonstrate the use of sparsity for\nimproving the accuracy of dense models through a set of simple-to-use sparse\ntransformations. Code is available at:\nhttps://github.com/CerebrasResearch/Sparse-IFT.\n","authors":["Vithursan Thangarasa","Shreyas Saxena","Abhay Gupta","Sean Lie"],"pdf_url":"https://arxiv.org/pdf/2303.11525v4.pdf","comment":"14 pages, 5 figures, 6 Tables (Main Paper) + 8 pages (Supplementary\n  Material). Published at ICML 2024"},{"id":"http://arxiv.org/abs/2211.11703v4","updated":"2024-07-17T21:56:04Z","published":"2022-11-21T18:24:34Z","title":"Towards continually learning new languages","summary":"  Multilingual speech recognition with neural networks is often implemented\nwith batch-learning, when all of the languages are available before training.\nAn ability to add new languages after the prior training sessions can be\neconomically beneficial, but the main challenge is catastrophic forgetting. In\nthis work, we combine the qualities of weight factorization and elastic weight\nconsolidation in order to counter catastrophic forgetting and facilitate\nlearning new languages quickly. Such combination allowed us to eliminate\ncatastrophic forgetting while still achieving performance for the new languages\ncomparable with having all languages at once, in experiments of learning from\nan initial 10 languages to achieve 26 languages without catastrophic forgetting\nand a reasonable performance compared to training all languages from scratch.\n","authors":["Ngoc-Quan Pham","Jan Niehues","Alexander Waibel"],"pdf_url":"https://arxiv.org/pdf/2211.11703v4.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2402.09739v3","updated":"2024-07-17T20:50:11Z","published":"2024-02-15T06:36:07Z","title":"QuRating: Selecting High-Quality Data for Training Language Models","summary":"  Selecting high-quality pre-training data is important for creating capable\nlanguage models, but existing methods rely on simple heuristics. We introduce\nQuRating, a method for selecting pre-training data that can capture human\nintuitions about data quality. In this paper, we investigate four qualities -\nwriting style, required expertise, facts & trivia, and educational value - and\nfind that LLMs are able to discern these qualities, especially when making\npairwise judgments of texts. We train a QuRater model to learn scalar ratings\nfrom pairwise judgments, and use it to annotate a 260B training corpus with\nquality ratings for each of the four criteria. In our experiments, we select\n30B tokens according to the different quality ratings and train 1.3B-parameter\nlanguage models on the selected data. We find that it is important to balance\nquality and diversity. When we sample using quality ratings as logits over\ndocuments, our models obtain lower perplexity and stronger in-context learning\nperformance than baselines. Our best model is based on educational value and\nperforms similarly to a model trained with uniform sampling for 50% more steps.\nBeyond data selection, we use the quality ratings to construct a training\ncurriculum which improves performance without changing the training dataset. We\nextensively analyze the quality ratings and discuss their characteristics,\nbiases, and wider implications.\n","authors":["Alexander Wettig","Aatmik Gupta","Saumya Malik","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2402.09739v3.pdf","comment":"Accepted at ICML 2024. The results for top-k selection have been\n  corrected. The code, models and data are available at\n  https://github.com/princeton-nlp/QuRating"},{"id":"http://arxiv.org/abs/2404.05829v2","updated":"2024-07-17T20:30:56Z","published":"2024-04-08T19:48:36Z","title":"SambaLingo: Teaching Large Language Models New Languages","summary":"  Despite the widespread availability of LLMs, there remains a substantial gap\nin their capabilities and availability across diverse languages. One approach\nto address these issues has been to take an existing pre-trained LLM and\ncontinue to train it on new languages. While prior works have experimented with\nlanguage adaptation, many questions around best practices and methodology have\nnot been covered. In this paper, we present a comprehensive investigation into\nthe adaptation of LLMs to new languages. Our study covers the key components in\nthis process, including vocabulary extension, direct preference optimization\nand the data scarcity problem for human alignment in low-resource languages. We\nscale these experiments across 9 languages and 2 parameter scales (7B and 70B).\nWe compare our models against Llama 2, Aya-101, XGLM, BLOOM and existing\nlanguage experts, outperforming all prior published baselines. Additionally,\nall evaluation code and checkpoints are made public to facilitate future\nresearch.\n","authors":["Zoltan Csaki","Bo Li","Jonathan Li","Qiantong Xu","Pian Pawakapan","Leon Zhang","Yun Du","Hengyu Zhao","Changran Hu","Urmish Thakker"],"pdf_url":"https://arxiv.org/pdf/2404.05829v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2406.04604v2","updated":"2024-07-17T20:24:44Z","published":"2024-06-07T03:27:51Z","title":"Learning Task Decomposition to Assist Humans in Competitive Programming","summary":"  When using language models (LMs) to solve complex problems, humans might\nstruggle to understand the LM-generated solutions and repair the flawed ones.\nTo assist humans in repairing them, we propose to automatically decompose\ncomplex solutions into multiple simpler pieces that correspond to specific\nsubtasks. We introduce a novel objective for learning task decomposition,\ntermed assistive value (AssistV), which measures the feasibility and speed for\nhumans to repair the decomposed solution. We collect a dataset of human repair\nexperiences on different decomposed solutions. Utilizing the collected data as\nin-context examples, we then learn to critique, refine, and rank decomposed\nsolutions to improve AssistV. We validate our method under competitive\nprogramming problems: under 177 hours of human study, our method enables\nnon-experts to solve 33.3\\% more problems, speeds them up by 3.3x, and empowers\nthem to match unassisted experts.\n","authors":["Jiaxin Wen","Ruiqi Zhong","Pei Ke","Zhihong Shao","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2406.04604v2.pdf","comment":"ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2407.12994v1","updated":"2024-07-17T20:23:19Z","published":"2024-07-17T20:23:19Z","title":"A Survey of Prompt Engineering Methods in Large Language Models for\n  Different NLP Tasks","summary":"  Large language models (LLMs) have shown remarkable performance on many\ndifferent Natural Language Processing (NLP) tasks. Prompt engineering plays a\nkey role in adding more to the already existing abilities of LLMs to achieve\nsignificant performance gains on various NLP tasks. Prompt engineering requires\ncomposing natural language instructions called prompts to elicit knowledge from\nLLMs in a structured way. Unlike previous state-of-the-art (SoTA) models,\nprompt engineering does not require extensive parameter re-training or\nfine-tuning based on the given NLP task and thus solely operates on the\nembedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently\nextract LLMs' knowledge through a basic natural language conversational\nexchange or prompt engineering, allowing more and more people even without deep\nmathematical machine learning background to experiment with LLMs. With prompt\nengineering gaining popularity in the last two years, researchers have come up\nwith numerous engineering techniques around designing prompts to improve\naccuracy of information extraction from the LLMs. In this paper, we summarize\ndifferent prompting techniques and club them together based on different NLP\ntasks that they have been used for. We further granularly highlight the\nperformance of these prompting strategies on various datasets belonging to that\nNLP task, talk about the corresponding LLMs used, present a taxonomy diagram\nand discuss the possible SoTA for specific datasets. In total, we read and\npresent a survey of 44 research papers which talk about 39 different prompting\nmethods on 29 different NLP tasks of which most of them have been published in\nthe last two years.\n","authors":["Shubham Vatsal","Harsh Dubey"],"pdf_url":"https://arxiv.org/pdf/2407.12994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11186v2","updated":"2024-07-17T20:03:55Z","published":"2024-07-15T19:17:31Z","title":"FarsInstruct: Empowering Large Language Models for Persian Instruction\n  Understanding","summary":"  Instruction-tuned large language models, such as T0, have demonstrated\nremarkable capabilities in following instructions across various domains.\nHowever, their proficiency remains notably deficient in many low-resource\nlanguages. To address this challenge, we introduce FarsInstruct: a\ncomprehensive instruction dataset designed to enhance the instruction-following\nability of large language models specifically for the Persian language, a\nsignificant yet underrepresented language globally. FarsInstruct encompasses a\nwide range of task types and datasets, each containing a mix of straightforward\nto complex manual written instructions, as well as translations from Public\nPool of Prompts, ensuring a rich linguistic and cultural representation.\nFurthermore, we introduce Co-CoLA, a framework designed to enhance the\nmulti-task adaptability of LoRA-tuned models. Through extensive experimental\nanalyses, our study showcases the effectiveness of FarsInstruct dataset coupled\nwith training by Co-CoLA framework, in improving the performance of large\nlanguage models within the Persian context. As of the current writing,\nFarsInstruct comprises more than 200 templates across 21 distinct datasets, and\nwe intend to update it consistently, thus augmenting its applicability.\n","authors":["Hojjat Mokhtarabadi","Ziba Zamani","Abbas Maazallahi","Hossein Manshaei"],"pdf_url":"https://arxiv.org/pdf/2407.11186v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12982v1","updated":"2024-07-17T20:01:21Z","published":"2024-07-17T20:01:21Z","title":"Retrieval-Enhanced Machine Learning: Synthesis and Opportunities","summary":"  In the field of language modeling, models augmented with retrieval components\nhave emerged as a promising solution to address several challenges faced in the\nnatural language processing (NLP) field, including knowledge grounding,\ninterpretability, and scalability. Despite the primary focus on NLP, we posit\nthat the paradigm of retrieval-enhancement can be extended to a broader\nspectrum of machine learning (ML) such as computer vision, time series\nprediction, and computational biology. Therefore, this work introduces a formal\nframework of this paradigm, Retrieval-Enhanced Machine Learning (REML), by\nsynthesizing the literature in various domains in ML with consistent notations\nwhich is missing from the current literature. Also, we found that while a\nnumber of studies employ retrieval components to augment their models, there is\na lack of integration with foundational Information Retrieval (IR) research. We\nbridge this gap between the seminal IR research and contemporary REML studies\nby investigating each component that comprises the REML framework. Ultimately,\nthe goal of this work is to equip researchers across various disciplines with a\ncomprehensive, formally structured framework of retrieval-enhanced models,\nthereby fostering interdisciplinary future research.\n","authors":["To Eun Kim","Alireza Salemi","Andrew Drozdov","Fernando Diaz","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2407.12982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04108v2","updated":"2024-07-17T18:45:46Z","published":"2024-07-04T18:24:09Z","title":"Future Events as Backdoor Triggers: Investigating Temporal\n  Vulnerabilities in LLMs","summary":"  Backdoors are hidden behaviors that are only triggered once an AI system has\nbeen deployed. Bad actors looking to create successful backdoors must design\nthem to avoid activation during training and evaluation. Since data used in\nthese stages often only contains information about events that have already\noccurred, a component of a simple backdoor trigger could be a model recognizing\ndata that is in the future relative to when it was trained. Through prompting\nexperiments and by probing internal activations, we show that current large\nlanguage models (LLMs) can distinguish past from future events, with probes on\nmodel activations achieving 90% accuracy. We train models with backdoors\ntriggered by a temporal distributional shift; they activate when the model is\nexposed to news headlines beyond their training cut-off dates. Fine-tuning on\nhelpful, harmless and honest (HHH) data does not work well for removing simpler\nbackdoor triggers but is effective on our backdoored models, although this\ndistinction is smaller for the larger-scale model we tested. We also find that\nan activation-steering vector representing a model's internal representation of\nthe date influences the rate of backdoor activation. We take these results as\ninitial evidence that, at least for models at the modest scale we test,\nstandard safety measures are enough to remove these backdoors.\n","authors":["Sara Price","Arjun Panickssery","Sam Bowman","Asa Cooper Stickland"],"pdf_url":"https://arxiv.org/pdf/2407.04108v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11139v2","updated":"2024-07-17T18:37:54Z","published":"2024-06-17T01:54:27Z","title":"Breaking Boundaries: Investigating the Effects of Model Editing on\n  Cross-linguistic Performance","summary":"  The integration of pretrained language models (PLMs) like BERT and GPT has\nrevolutionized NLP, particularly for English, but it has also created\nlinguistic imbalances. This paper strategically identifies the need for\nlinguistic equity by examining several knowledge editing techniques in\nmultilingual contexts. We evaluate the performance of models such as Mistral,\nTowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama across languages including\nEnglish, German, French, Italian, Spanish, Hindi, Tamil, and Kannada. Our\nresearch identifies significant discrepancies in normal and merged models\nconcerning cross-lingual consistency. We employ strategies like 'each language\nfor itself' (ELFI) and 'each language for others' (ELFO) to stress-test these\nmodels. Our findings demonstrate the potential for LLMs to overcome linguistic\nbarriers, laying the groundwork for future research in achieving linguistic\ninclusivity in AI technologies.\n","authors":["Somnath Banerjee","Avik Halder","Rajarshi Mandal","Sayan Layek","Ian Soboroff","Rima Hazra","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2406.11139v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2407.12943v1","updated":"2024-07-17T18:21:01Z","published":"2024-07-17T18:21:01Z","title":"Halu-J: Critique-Based Hallucination Judge","summary":"  Large language models (LLMs) frequently generate non-factual content, known\nas hallucinations. Existing retrieval-augmented-based hallucination detection\napproaches typically address this by framing it as a classification task,\nevaluating hallucinations based on their consistency with retrieved evidence.\nHowever, this approach usually lacks detailed explanations for these\nevaluations and does not assess the reliability of these explanations.\nFurthermore, deficiencies in retrieval systems can lead to irrelevant or\npartially relevant evidence retrieval, impairing the detection process.\nMoreover, while real-world hallucination detection requires analyzing multiple\npieces of evidence, current systems usually treat all evidence uniformly\nwithout considering its relevance to the content. To address these challenges,\nwe introduce Halu-J, a critique-based hallucination judge with 7 billion\nparameters. Halu-J enhances hallucination detection by selecting pertinent\nevidence and providing detailed critiques. Our experiments indicate that Halu-J\noutperforms GPT-4o in multiple-evidence hallucination detection and matches its\ncapability in critique generation and evidence selection. We also introduce\nME-FEVER, a new dataset designed for multiple-evidence hallucination detection.\nOur code and dataset can be found in https://github.com/GAIR-NLP/factool .\n","authors":["Binjie Wang","Steffi Chern","Ethan Chern","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.12943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02958v2","updated":"2024-07-17T18:09:22Z","published":"2024-06-05T05:27:02Z","title":"PrE-Text: Training Language Models on Private Federated Data in the Age\n  of LLMs","summary":"  On-device training is currently the most common approach for training machine\nlearning (ML) models on private, distributed user data. Despite this, on-device\ntraining has several drawbacks: (1) most user devices are too small to train\nlarge models on-device, (2) on-device training is communication- and\ncomputation-intensive, and (3) on-device training can be difficult to debug and\ndeploy. To address these problems, we propose Private Evolution-Text\n(PrE-Text), a method for generating differentially private (DP) synthetic\ntextual data. First, we show that across multiple datasets, training small\nmodels (models that fit on user devices) with PrE-Text synthetic data\noutperforms small models trained on-device under practical privacy regimes\n($\\epsilon=1.29$, $\\epsilon=7.58$). We achieve these results while using\n9$\\times$ fewer rounds, 6$\\times$ less client computation per round, and\n100$\\times$ less communication per round. Second, finetuning large models on\nPrE-Text's DP synthetic data improves large language model (LLM) performance on\nprivate data across the same range of privacy budgets. Altogether, these\nresults suggest that training on DP synthetic data can be a better option than\ntraining a model on-device on private distributed data. Code is available at\nhttps://github.com/houcharlie/PrE-Text.\n","authors":["Charlie Hou","Akshat Shrivastava","Hongyuan Zhan","Rylan Conway","Trang Le","Adithya Sagar","Giulia Fanti","Daniel Lazar"],"pdf_url":"https://arxiv.org/pdf/2406.02958v2.pdf","comment":"ICML 2024 (Oral)"},{"id":"http://arxiv.org/abs/2407.12772v1","updated":"2024-07-17T17:51:53Z","published":"2024-07-17T17:51:53Z","title":"LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models","summary":"  The advances of large foundation models necessitate wide-coverage, low-cost,\nand zero-contamination benchmarks. Despite continuous exploration of language\nmodel evaluations, comprehensive studies on the evaluation of Large Multi-modal\nModels (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified\nand standardized multimodal benchmark framework with over 50 tasks and more\nthan 10 models to promote transparent and reproducible evaluations. Although\nLMMS-EVAL offers comprehensive coverage, we find it still falls short in\nachieving low cost and zero contamination. To approach this evaluation\ntrilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that\nemphasizes both coverage and efficiency. Additionally, we present Multimodal\nLIVEBENCH that utilizes continuously updating news and online forums to assess\nmodels' generalization abilities in the wild, featuring a low-cost and\nzero-contamination evaluation approach. In summary, our work highlights the\nimportance of considering the evaluation trilemma and provides practical\nsolutions to navigate the trade-offs in evaluating large multi-modal models,\npaving the way for more effective and reliable benchmarking of LMMs. We\nopensource our codebase and maintain leaderboard of LIVEBENCH at\nhttps://github.com/EvolvingLMMs-Lab/lmms-eval and\nhttps://huggingface.co/spaces/lmms-lab/LiveBench.\n","authors":["Kaichen Zhang","Bo Li","Peiyuan Zhang","Fanyi Pu","Joshua Adrian Cahyono","Kairui Hu","Shuai Liu","Yuanhan Zhang","Jingkang Yang","Chunyuan Li","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.12772v1.pdf","comment":"Code ad leaderboard are available at\n  https://github.com/EvolvingLMMs-Lab/lmms-eval and\n  https://huggingface.co/spaces/lmms-lab/LiveBench"},{"id":"http://arxiv.org/abs/2407.12771v1","updated":"2024-07-17T17:51:49Z","published":"2024-07-17T17:51:49Z","title":"The Role of Network and Identity in the Diffusion of Hashtags","summary":"  Although the spread of behaviors is influenced by many social factors,\nexisting literature tends to study the effects of single factors -- most often,\nproperties of the social network -- on the final cascade. In order to move\ntowards a more integrated view of cascades, this paper offers the first\ncomprehensive investigation into the role of two social factors in the\ndiffusion of 1,337 popular hashtags representing the production of novel\nculture on Twitter: 1) the topology of the Twitter social network and 2)\nperformance of each user's probable demographic identity. Here, we show that\ncascades are best modeled using a combination of network and identity, rather\nthan either factor alone. This combined model best reproduces a composite index\nof ten cascade properties across all 1,337 hashtags. However, there is\nimportant heterogeneity in what social factors are required to reproduce\ndifferent properties of hashtag cascades. For instance, while a combined\nnetwork+identity model best predicts the popularity of cascades, a network-only\nmodel has better performance in predicting cascade growth and an identity-only\nmodel in adopter composition. We are able to predict what type of hashtag is\nbest modeled by each combination of features and use this to further improve\nperformance. Additionally, consistent with prior literature on the combined\nnetwork+identity model most outperforms the single-factor counterfactuals among\nhashtags used for expressing racial or regional identity, stance-taking,\ntalking about sports, or variants of existing cultural trends with very slow-\nor fast-growing communicative need. In sum, our results imply the utility of\nmulti-factor models in predicting cascades, in order to account for the varied\nways in which network, identity, and other social factors play a role in the\ndiffusion of hashtags on Twitter.\n","authors":["Aparna Ananthasubramaniam","Yufei Zhu","David Jurgens","Daniel Romero"],"pdf_url":"https://arxiv.org/pdf/2407.12771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13956v3","updated":"2024-07-17T17:49:42Z","published":"2024-02-21T17:36:07Z","title":"Can You Learn Semantics Through Next-Word Prediction? The Case of\n  Entailment","summary":"  Do LMs infer the semantics of text from co-occurrence patterns in their\ntraining data? Merrill et al. (2022) argue that, in theory, sentence\nco-occurrence probabilities predicted by an optimal LM should reflect the\nentailment relationship of the constituent sentences, but it is unclear whether\nprobabilities predicted by neural LMs encode entailment in this way because of\nstrong assumptions made by Merrill et al. (namely, that humans always avoid\nredundancy). In this work, we investigate whether their theory can be used to\ndecode entailment relations from neural LMs. We find that a test similar to\ntheirs can decode entailment relations between natural sentences, well above\nrandom chance, though not perfectly, across many datasets and LMs. This\nsuggests LMs implicitly model aspects of semantics to predict semantic effects\non sentence co-occurrence patterns. However, we find the test that predicts\nentailment in practice works in the opposite direction to the theoretical test.\nWe thus revisit the assumptions underlying the original test, finding its\nderivation did not adequately account for redundancy in human-written text. We\nargue that better accounting for redundancy related to explanations might\nderive the observed flipped test and, more generally, improve computational\nmodels of speakers in linguistics.\n","authors":["William Merrill","Zhaofeng Wu","Norihito Naka","Yoon Kim","Tal Linzen"],"pdf_url":"https://arxiv.org/pdf/2402.13956v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06508v3","updated":"2024-07-17T17:39:39Z","published":"2024-04-09T17:57:29Z","title":"On the Effect of (Near) Duplicate Subwords in Language Modelling","summary":"  Tokenisation is a core part of language models (LMs). It involves splitting a\ncharacter sequence into subwords which are assigned arbitrary indices before\nbeing served to the LM. While typically lossless, however, this process may\nlead to less sample efficient LM training: as it removes character-level\ninformation, it could make it harder for LMs to generalise across similar\nsubwords, such as now and Now. We refer to such subwords as near duplicates. In\nthis paper, we study the impact of near duplicate subwords on LM training\nefficiency. First, we design an experiment that gives us an upper bound to how\nmuch we should expect a model to improve if we could perfectly generalise\nacross near duplicates. We do this by duplicating each subword in our LM's\nvocabulary, creating perfectly equivalent classes of subwords. Experimentally,\nwe find that LMs need roughly 17% more data when trained in a fully duplicated\nsetting. Second, we investigate the impact of naturally occurring near\nduplicates on LMs. Here, we see that merging them considerably hurts LM\nperformance. Therefore, although subword duplication negatively impacts LM\ntraining efficiency, naturally occurring near duplicates may not be as similar\nas anticipated, limiting the potential for performance improvements.\n","authors":["Anton Schäfer","Thomas Hofmann","Imanol Schlag","Tiago Pimentel"],"pdf_url":"https://arxiv.org/pdf/2404.06508v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12749v1","updated":"2024-07-17T17:11:13Z","published":"2024-07-17T17:11:13Z","title":"HDLCopilot: Hardware Design Library Querying with Natural Language","summary":"  Hardware design engineers routinely work with multiple Process Design Kits\n(PDKs) from various fabrication labs, each containing several standard cell\nlibraries, optimized for specific metric such as speed, power, or density.\nThese libraries include multiple views such as liberty files for timing\ninformation, LEF files for abstract layout details, and technology LEF for\nprocess design rules. Navigating this complex landscape to retrieve specific\ninformation about gates or design rules is often time-consuming and\nerror-prone. To address this, we present HDLCopilot, an LLM-powered PDK query\nsystem that allows engineers to streamline interactions with PDKs in natural\nlanguage format, making information retrieval accurate and more efficient.\nHDLCopilot achieves an accuracy of 94.23\\% on an evaluation set comprised of\ndiverse and complex natural language queries. HDLCopilot positions itself as a\npowerful assistant in the hardware design process, enhancing productivity and\nreducing potential human errors.\n","authors":["Manar Abdelatty","Sherief Reda"],"pdf_url":"https://arxiv.org/pdf/2407.12749v1.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.07982v4","updated":"2024-07-17T16:59:01Z","published":"2024-04-11T17:58:05Z","title":"The Role of Language Imbalance in Cross-lingual Generalisation: Insights\n  from Cloned Language Experiments","summary":"  Multilinguality is crucial for extending recent advancements in language\nmodelling to diverse linguistic communities. To maintain high performance while\nrepresenting multiple languages, multilingual models ideally align\nrepresentations, allowing what is learned in one language to generalise to\nothers. Prior research has emphasised the importance of parallel data and\nshared vocabulary elements as key factors for such alignment. In this study, we\ninvestigate an unintuitive novel driver of cross-lingual generalisation:\nlanguage imbalance. In controlled experiments on perfectly equivalent cloned\nlanguages, we observe that the existence of a predominant language during\ntraining boosts the performance of less frequent languages and leads to\nstronger alignment of model representations across languages. Furthermore, we\nfind that this trend is amplified with scale: with large enough models or long\nenough training, we observe that bilingual training data with a 90/10 language\nsplit yields better performance on both languages than a balanced 50/50 split.\nBuilding on these insights, we design training schemes that can improve\nperformance in all cloned languages, even without altering the training data.\nAs we extend our analysis to real languages, we find that infrequent languages\nstill benefit from frequent ones, yet whether language imbalance causes\ncross-lingual generalisation there is not conclusive.\n","authors":["Anton Schäfer","Shauli Ravfogel","Thomas Hofmann","Tiago Pimentel","Imanol Schlag"],"pdf_url":"https://arxiv.org/pdf/2404.07982v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12734v1","updated":"2024-07-17T16:52:23Z","published":"2024-07-17T16:52:23Z","title":"A LLM Benchmark based on the Minecraft Builder Dialog Agent Task","summary":"  In this work we proposing adapting the Minecraft builder task into an LLM\nbenchmark suitable for evaluating LLM ability in spatially orientated tasks,\nand informing builder agent design. Previous works have proposed corpora with\nvarying complex structures, and human written instructions. We instead attempt\nto provide a comprehensive synthetic benchmark for testing builder agents over\na series of distinct tasks that comprise of common building operations. We\nbelieve this approach allows us to probe specific strengths and weaknesses of\ndifferent agents, and test the ability of LLMs in the challenging area of\nspatial reasoning and vector based math.\n","authors":["Chris Madge","Massimo Poesio"],"pdf_url":"https://arxiv.org/pdf/2407.12734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12725v1","updated":"2024-07-17T16:42:03Z","published":"2024-07-17T16:42:03Z","title":"Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language\n  Models?","summary":"  Elaborating a series of intermediate reasoning steps significantly improves\nthe ability of large language models (LLMs) to solve complex problems, as such\nsteps would evoke LLMs to think sequentially. However, human sarcasm\nunderstanding is often considered an intuitive and holistic cognitive process,\nin which various linguistic, contextual, and emotional cues are integrated to\nform a comprehensive understanding of the speaker's true intention, which is\nargued not be limited to a step-by-step reasoning process. To verify this\nargument, we introduce a new prompting framework called SarcasmCue, which\ncontains four prompting strategies, $viz.$ chain of contradiction (CoC), graph\nof cues (GoC), bagging of cues (BoC) and tensor of cues (ToC), which elicits\nLLMs to detect human sarcasm by considering sequential and non-sequential\nprompting methods. Through a comprehensive empirical comparison on four\nbenchmarking datasets, we show that the proposed four prompting methods\noutperforms standard IO prompting, CoT and ToT with a considerable margin, and\nnon-sequential prompting generally outperforms sequential prompting.\n","authors":["Ben Yao","Yazhou Zhang","Qiuchi Li","Jing Qin"],"pdf_url":"https://arxiv.org/pdf/2407.12725v1.pdf","comment":"13 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.11393v2","updated":"2024-07-17T16:40:05Z","published":"2024-07-16T05:26:12Z","title":"CIC-BART-SSA: Controllable Image Captioning with Structured Semantic\n  Augmentation","summary":"  Controllable Image Captioning (CIC) aims at generating natural language\ndescriptions for an image, conditioned on information provided by end users,\ne.g., regions, entities or events of interest. However, available\nimage-language datasets mainly contain captions that describe the entirety of\nan image, making them ineffective for training CIC models that can potentially\nattend to any subset of regions or relationships. To tackle this challenge, we\npropose a novel, fully automatic method to sample additional focused and\nvisually grounded captions using a unified structured semantic representation\nbuilt on top of the existing set of captions associated with an image. We\nleverage Abstract Meaning Representation (AMR), a cross-lingual graph-based\nsemantic formalism, to encode all possible spatio-semantic relations between\nentities, beyond the typical spatial-relations-only focus of current methods.\nWe use this Structured Semantic Augmentation (SSA) framework to augment\nexisting image-caption datasets with the grounded controlled captions,\nincreasing their spatial and semantic diversity and focal coverage. We then\ndevelop a new model, CIC-BART-SSA, specifically tailored for the CIC task, that\nsources its control signals from SSA-diversified datasets. We empirically show\nthat, compared to SOTA CIC models, CIC-BART-SSA generates captions that are\nsuperior in diversity and text quality, are competitive in controllability,\nand, importantly, minimize the gap between broad and highly focused controlled\ncaptioning performance by efficiently generalizing to the challenging highly\nfocused scenarios. Code is available at\nhttps://github.com/SamsungLabs/CIC-BART-SSA.\n","authors":["Kalliopi Basioti","Mohamed A. Abdelsalam","Federico Fancellu","Vladimir Pavlovic","Afsaneh Fazly"],"pdf_url":"https://arxiv.org/pdf/2407.11393v2.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.12707v1","updated":"2024-07-17T16:30:27Z","published":"2024-07-17T16:30:27Z","title":"TTSDS -- Text-to-Speech Distribution Score","summary":"  Many recently published Text-to-Speech (TTS) systems produce audio close to\nreal speech. However, TTS evaluation needs to be revisited to make sense of the\nresults obtained with the new architectures, approaches and datasets. We\npropose evaluating the quality of synthetic speech as a combination of multiple\nfactors such as prosody, speaker identity, and intelligibility. Our approach\nassesses how well synthetic speech mirrors real speech by obtaining correlates\nof each factor and measuring their distance from both real speech datasets and\nnoise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and\nshow that our score computed as an unweighted average of factors strongly\ncorrelates with the human evaluations from each time period.\n","authors":["Christoph Minixhofer","Ondřej Klejch","Peter Bell"],"pdf_url":"https://arxiv.org/pdf/2407.12707v1.pdf","comment":"Under review for SLT 2024"},{"id":"http://arxiv.org/abs/2407.08348v2","updated":"2024-07-17T16:28:33Z","published":"2024-07-11T09:56:51Z","title":"Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large\n  Language Models -- The Story Goes On","summary":"  In this paper, we investigate the underlying factors that potentially enhance\nthe mathematical reasoning capabilities of large language models (LLMs). We\nargue that the data scaling law for math reasoning capabilities in modern LLMs\nis far from being saturated, highlighting how the model's quality improves with\nincreases in data quantity. To support this claim, we introduce the\nSkywork-Math model series, supervised fine-tuned (SFT) on common 7B LLMs using\nour proposed 2.5M-instance Skywork-MathQA dataset. Skywork-Math 7B has achieved\nimpressive accuracies of 51.2% on the competition-level MATH benchmark and\n83.9% on the GSM8K benchmark using only SFT data, outperforming an early\nversion of GPT-4 on MATH. The superior performance of Skywork-Math models\ncontributes to our novel two-stage data synthesis and model SFT pipelines,\nwhich include three different augmentation methods and a diverse seed problem\nset, ensuring both the quantity and quality of Skywork-MathQA dataset across\nvarying difficulty levels. Most importantly, we provide several practical\ntakeaways to enhance math reasoning abilities in LLMs for both research and\nindustry applications.\n","authors":["Liang Zeng","Liangjun Zhong","Liang Zhao","Tianwen Wei","Liu Yang","Jujie He","Cheng Cheng","Rui Hu","Yang Liu","Shuicheng Yan","Han Fang","Yahui Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.08348v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12703v1","updated":"2024-07-17T16:25:37Z","published":"2024-07-17T16:25:37Z","title":"Subgraph-Aware Training of Text-based Methods for Knowledge Graph\n  Completion","summary":"  Fine-tuning pre-trained language models (PLMs) has recently shown a potential\nto improve knowledge graph completion (KGC). However, most PLM-based methods\nencode only textual information, neglecting various topological structures of\nknowledge graphs (KGs). In this paper, we empirically validate the significant\nrelations between the structural properties of KGs and the performance of the\nPLM-based methods. To leverage the structural knowledge, we propose a\nSubgraph-Aware Training framework for KGC (SATKGC) that combines (i)\nsubgraph-aware mini-batching to encourage hard negative sampling, and (ii) a\nnew contrastive learning method to focus more on harder entities and harder\nnegative triples in terms of the structural properties. To the best of our\nknowledge, this is the first study to comprehensively incorporate the\nstructural inductive bias of the subgraphs into fine-tuning PLMs. Extensive\nexperiments on four KGC benchmarks demonstrate the superiority of SATKGC. Our\ncode is available.\n","authors":["Youmin Ko","Hyemin Yang","Taeuk Kim","Hyunjoon Kim"],"pdf_url":"https://arxiv.org/pdf/2407.12703v1.pdf","comment":"8 pages, including appendix with 8 figures and 12 tables, currently\n  under open review for EMNLP 2024"},{"id":"http://arxiv.org/abs/2404.09956v4","updated":"2024-07-17T16:17:50Z","published":"2024-04-15T17:31:22Z","title":"Tango 2: Aligning Diffusion-based Text-to-Audio Generations through\n  Direct Preference Optimization","summary":"  Generative multimodal content is increasingly prevalent in much of the\ncontent creation arena, as it has the potential to allow artists and media\npersonnel to create pre-production mockups by quickly bringing their ideas to\nlife. The generation of audio from text prompts is an important aspect of such\nprocesses in the music and film industry. Many of the recent diffusion-based\ntext-to-audio models focus on training increasingly sophisticated diffusion\nmodels on a large set of datasets of prompt-audio pairs. These models do not\nexplicitly focus on the presence of concepts or events and their temporal\nordering in the output audio with respect to the input prompt. Our hypothesis\nis focusing on how these aspects of audio generation could improve audio\ngeneration performance in the presence of limited data. As such, in this work,\nusing an existing text-to-audio model Tango, we synthetically create a\npreference dataset where each prompt has a winner audio output and some loser\naudio outputs for the diffusion model to learn from. The loser outputs, in\ntheory, have some concepts from the prompt missing or in an incorrect order. We\nfine-tune the publicly available Tango text-to-audio model using diffusion-DPO\n(direct preference optimization) loss on our preference dataset and show that\nit leads to improved audio output over Tango and AudioLDM2, in terms of both\nautomatic- and manual-evaluation metrics.\n","authors":["Navonil Majumder","Chia-Yu Hung","Deepanway Ghosal","Wei-Ning Hsu","Rada Mihalcea","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2404.09956v4.pdf","comment":"Accepted at ACM MM 2024"},{"id":"http://arxiv.org/abs/2405.06134v2","updated":"2024-07-17T15:59:26Z","published":"2024-05-09T22:59:23Z","title":"Muting Whisper: A Universal Acoustic Adversarial Attack on Speech\n  Foundation Models","summary":"  Recent developments in large speech foundation models like Whisper have led\nto their widespread use in many automatic speech recognition (ASR)\napplications. These systems incorporate `special tokens' in their vocabulary,\nsuch as $\\texttt{<|endoftext|>}$, to guide their language generation process.\nHowever, we demonstrate that these tokens can be exploited by adversarial\nattacks to manipulate the model's behavior. We propose a simple yet effective\nmethod to learn a universal acoustic realization of Whisper's\n$\\texttt{<|endoftext|>}$ token, which, when prepended to any speech signal,\nencourages the model to ignore the speech and only transcribe the special\ntoken, effectively `muting' the model. Our experiments demonstrate that the\nsame, universal 0.64-second adversarial audio segment can successfully mute a\ntarget Whisper ASR model for over 97\\% of speech samples. Moreover, we find\nthat this universal adversarial audio segment often transfers to new datasets\nand tasks. Overall this work demonstrates the vulnerability of Whisper models\nto `muting' adversarial attacks, where such attacks can pose both risks and\npotential benefits in real-world settings: for example the attack can be used\nto bypass speech moderation systems, or conversely the attack can also be used\nto protect private speech data.\n","authors":["Vyas Raina","Rao Ma","Charles McGhee","Kate Knill","Mark Gales"],"pdf_url":"https://arxiv.org/pdf/2405.06134v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02543v2","updated":"2024-07-17T15:55:51Z","published":"2024-06-04T17:58:18Z","title":"To Believe or Not to Believe Your LLM","summary":"  We explore uncertainty quantification in large language models (LLMs), with\nthe goal to identify when uncertainty in responses given a query is large. We\nsimultaneously consider both epistemic and aleatoric uncertainties, where the\nformer comes from the lack of knowledge about the ground truth (such as about\nfacts or the language), and the latter comes from irreducible randomness (such\nas multiple possible answers). In particular, we derive an\ninformation-theoretic metric that allows to reliably detect when only epistemic\nuncertainty is large, in which case the output of the model is unreliable. This\ncondition can be computed based solely on the output of the model obtained\nsimply by some special iterative prompting based on the previous responses.\nSuch quantification, for instance, allows to detect hallucinations (cases when\nepistemic uncertainty is high) in both single- and multi-answer responses. This\nis in contrast to many standard uncertainty quantification strategies (such as\nthresholding the log-likelihood of a response) where hallucinations in the\nmulti-answer case cannot be detected. We conduct a series of experiments which\ndemonstrate the advantage of our formulation. Further, our investigations shed\nsome light on how the probabilities assigned to a given output by an LLM can be\namplified by iterative prompting, which might be of independent interest.\n","authors":["Yasin Abbasi Yadkori","Ilja Kuzborskij","András György","Csaba Szepesvári"],"pdf_url":"https://arxiv.org/pdf/2406.02543v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12665v1","updated":"2024-07-17T15:48:39Z","published":"2024-07-17T15:48:39Z","title":"Patch-Level Training for Large Language Models","summary":"  As Large Language Models (LLMs) achieve remarkable progress in language\nunderstanding and generation, their training efficiency has become a critical\nconcern. Traditionally, LLMs are trained to predict the next token in a\nsequence. Despite the success of token-level training, it suffers from\nconsiderable computational costs due to the need to process an extensive number\nof tokens. To mitigate this issue, this paper introduces patch-level training\nfor LLMs, which reduces the sequence length by compressing multiple tokens into\na single patch. During patch-level training, we feed the language model shorter\nsequences of patches and train it to predict the next patch, thereby processing\nthe majority of the training data at a significantly reduced computational\ncost. Following this, the model continues token-level training on the remaining\ntraining data to align with the inference mode. Experiments on a diverse range\nof models (370M-2.7B parameters) demonstrate that patch-level training can\nreduce overall computational costs to 0.5$\\times$, without compromising the\nmodel performance compared to token-level training. Source code:\n\\url{https://github.com/shaochenze/PatchTrain}.\n","authors":["Chenze Shao","Fandong Meng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.12665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19094v2","updated":"2024-07-17T15:29:18Z","published":"2024-04-29T20:19:25Z","title":"In-Context Symbolic Regression: Leveraging Large Language Models for\n  Function Discovery","summary":"  State of the art Symbolic Regression (SR) methods currently build specialized\nmodels, while the application of Large Language Models (LLMs) remains largely\nunexplored. In this work, we introduce the first comprehensive framework that\nutilizes LLMs for the task of SR. We propose In-Context Symbolic Regression\n(ICSR), an SR method which iteratively refines a functional form with an LLM\nand determines its coefficients with an external optimizer. ICSR leverages\nLLMs' strong mathematical prior both to propose an initial set of possible\nfunctions given the observations and to refine them based on their errors. Our\nfindings reveal that LLMs are able to successfully find symbolic equations that\nfit the given data, matching or outperforming the overall performance of the\nbest SR baselines on four popular benchmarks, while yielding simpler equations\nwith better out of distribution generalization.\n","authors":["Matteo Merler","Katsiaryna Haitsiukevich","Nicola Dainese","Pekka Marttinen"],"pdf_url":"https://arxiv.org/pdf/2404.19094v2.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.12626v1","updated":"2024-07-17T14:52:46Z","published":"2024-07-17T14:52:46Z","title":"Domain-specific or Uncertainty-aware models: Does it really make a\n  difference for biomedical text classification?","summary":"  The success of pretrained language models (PLMs) across a spate of use-cases\nhas led to significant investment from the NLP community towards building\ndomain-specific foundational models. On the other hand, in mission critical\nsettings such as biomedical applications, other aspects also factor in-chief of\nwhich is a model's ability to produce reasonable estimates of its own\nuncertainty. In the present study, we discuss these two desiderata through the\nlens of how they shape the entropy of a model's output probability\ndistribution. We find that domain specificity and uncertainty awareness can\noften be successfully combined, but the exact task at hand weighs in much more\nstrongly.\n","authors":["Aman Sinha","Timothee Mickus","Marianne Clausel","Mathieu Constant","Xavier Coubez"],"pdf_url":"https://arxiv.org/pdf/2407.12626v1.pdf","comment":"BioNLP 2024"},{"id":"http://arxiv.org/abs/2407.12620v1","updated":"2024-07-17T14:46:37Z","published":"2024-07-17T14:46:37Z","title":"Harnessing the Power of Artificial Intelligence to Vitalize Endangered\n  Indigenous Languages: Technologies and Experiences","summary":"  Since 2022 we have been exploring application areas and technologies in which\nArtificial Intelligence (AI) and modern Natural Language Processing (NLP), such\nas Large Language Models (LLMs), can be employed to foster the usage and\nfacilitate the documentation of Indigenous languages which are in danger of\ndisappearing. We start by discussing the decreasing diversity of languages in\nthe world and how working with Indigenous languages poses unique ethical\nchallenges for AI and NLP. To address those challenges, we propose an\nalternative development AI cycle based on community engagement and usage. Then,\nwe report encouraging results in the development of high-quality machine\nlearning translators for Indigenous languages by fine-tuning state-of-the-art\n(SOTA) translators with tiny amounts of data and discuss how to avoid some\ncommon pitfalls in the process. We also present prototypes we have built in\nprojects done in 2023 and 2024 with Indigenous communities in Brazil, aimed at\nfacilitating writing, and discuss the development of Indigenous Language Models\n(ILMs) as a replicable and scalable way to create spell-checkers, next-word\npredictors, and similar tools. Finally, we discuss how we envision a future for\nlanguage documentation where dying languages are preserved as interactive\nlanguage models.\n","authors":["Claudio Pinhanez","Paulo Cavalin","Luciana Storto","Thomas Fimbow","Alexander Cobbinah","Julio Nogima","Marisa Vasconcelos","Pedro Domingues","Priscila de Souza Mizukami","Nicole Grell","Majoí Gongora","Isabel Gonçalves"],"pdf_url":"https://arxiv.org/pdf/2407.12620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05506v3","updated":"2024-07-17T14:46:17Z","published":"2023-10-09T08:18:58Z","title":"MuggleMath: Assessing the Impact of Query and Response Augmentation on\n  Math Reasoning","summary":"  In math reasoning with large language models (LLMs), fine-tuning data\naugmentation by query evolution and diverse reasoning paths is empirically\nverified effective, profoundly narrowing the gap between open-sourced LLMs and\ncutting-edge proprietary LLMs. In this paper, we conduct an investigation for\nsuch data augmentation in math reasoning and are intended to answer: (1) What\nstrategies of data augmentation are more effective; (2) What is the scaling\nrelationship between the amount of augmented data and model performance; and\n(3) Can data augmentation incentivize generalization to out-of-domain\nmathematical reasoning tasks? To this end, we create two new dataset AugGSM8K\nand AugMATH, by complicating and diversifying the queries and sampling multiple\nreasoning paths from GSM8K and MATH. We obtained a series of LLMs called\nMuggleMath by fine-tuning LLaMA models on AugGSM8K and AugMATH. MuggleMath\nsubstantially achieves new state-of-the-art on GSM8K and MATH. A log-linear\nrelationship and a segmented log-linear are presented between MuggleMath's\nperformance and the amount of augmented data on GSM8K and MATH, respectively.\nWe also find that it is weak in out-of-domain math reasoning generalization\nfrom AugGSM8K to MATH and from AugMATH to GSM8K, which suggests that augmenting\nqueries that cover a broader range of subjects is more beneficial for\ngeneralization. We release our codes and augmented data in\nhttps://github.com/OFA-Sys/gsm8k-ScRel.\n","authors":["Chengpeng Li","Zheng Yuan","Hongyi Yuan","Guanting Dong","Keming Lu","Jiancan Wu","Chuanqi Tan","Xiang Wang","Chang Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.05506v3.pdf","comment":"Accepted to ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2407.12613v1","updated":"2024-07-17T14:41:35Z","published":"2024-07-17T14:41:35Z","title":"AudienceView: AI-Assisted Interpretation of Audience Feedback in\n  Journalism","summary":"  Understanding and making use of audience feedback is important but difficult\nfor journalists, who now face an impractically large volume of audience\ncomments online. We introduce AudienceView, an online tool to help journalists\ncategorize and interpret this feedback by leveraging large language models\n(LLMs). AudienceView identifies themes and topics, connects them back to\nspecific comments, provides ways to visualize the sentiment and distribution of\nthe comments, and helps users develop ideas for subsequent reporting projects.\nWe consider how such tools can be useful in a journalist's workflow, and\nemphasize the importance of contextual awareness and human judgment.\n","authors":["William Brannon","Doug Beeferman","Hang Jiang","Andrew Heyward","Deb Roy"],"pdf_url":"https://arxiv.org/pdf/2407.12613v1.pdf","comment":"Accepted at CSCW Demo 2024. 5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2309.12871v8","updated":"2024-07-17T14:33:21Z","published":"2023-09-22T13:52:42Z","title":"AnglE-optimized Text Embeddings","summary":"  High-quality text embedding is pivotal in improving semantic textual\nsimilarity (STS) tasks, which are crucial components in Large Language Model\n(LLM) applications. However, a common challenge existing text embedding models\nface is the problem of vanishing gradients, primarily due to their reliance on\nthe cosine function in the optimization objective, which has saturation zones.\nTo address this issue, this paper proposes a novel angle-optimized text\nembedding model called AnglE. The core idea of AnglE is to introduce angle\noptimization in a complex space. This novel approach effectively mitigates the\nadverse effects of the saturation zone in the cosine function, which can impede\ngradient and hinder optimization processes. To set up a comprehensive STS\nevaluation, we experimented on existing short-text STS datasets and a newly\ncollected long-text STS dataset from GitHub Issues. Furthermore, we examine\ndomain-specific STS scenarios with limited labeled data and explore how AnglE\nworks with LLM-annotated data. Extensive experiments were conducted on various\ntasks including short-text STS, long-text STS, and domain-specific STS tasks.\nThe results show that AnglE outperforms the state-of-the-art (SOTA) STS models\nthat ignore the cosine saturation zone. These findings demonstrate the ability\nof AnglE to generate high-quality text embeddings and the usefulness of angle\noptimization in STS.\n","authors":["Xianming Li","Jing Li"],"pdf_url":"https://arxiv.org/pdf/2309.12871v8.pdf","comment":"Accepted by ACL24 Main Conference"},{"id":"http://arxiv.org/abs/2407.12580v1","updated":"2024-07-17T14:04:12Z","published":"2024-07-17T14:04:12Z","title":"E5-V: Universal Embeddings with Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) have shown promising advancements in\ngeneral visual and language understanding. However, the representation of\nmultimodal information using MLLMs remains largely unexplored. In this work, we\nintroduce a new framework, E5-V, designed to adapt MLLMs for achieving\nuniversal multimodal embeddings. Our findings highlight the significant\npotential of MLLMs in representing multimodal inputs compared to previous\napproaches. By leveraging MLLMs with prompts, E5-V effectively bridges the\nmodality gap between different types of inputs, demonstrating strong\nperformance in multimodal embeddings even without fine-tuning. We propose a\nsingle modality training approach for E5-V, where the model is trained\nexclusively on text pairs. This method demonstrates significant improvements\nover traditional multimodal training on image-text pairs, while reducing\ntraining costs by approximately 95%. Additionally, this approach eliminates the\nneed for costly multimodal training data collection. Extensive experiments\nacross four types of tasks demonstrate the effectiveness of E5-V. As a\nuniversal multimodal model, E5-V not only achieves but often surpasses\nstate-of-the-art performance in each task, despite being trained on a single\nmodality.\n","authors":["Ting Jiang","Minghui Song","Zihan Zhang","Haizhen Huang","Weiwei Deng","Feng Sun","Qi Zhang","Deqing Wang","Fuzhen Zhuang"],"pdf_url":"https://arxiv.org/pdf/2407.12580v1.pdf","comment":"Code and models are available at https://github.com/kongds/E5-V"},{"id":"http://arxiv.org/abs/2402.16844v3","updated":"2024-07-17T13:59:48Z","published":"2024-02-26T18:59:28Z","title":"Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding","summary":"  Large language models (LLMs) have become ubiquitous in practice and are\nwidely used for generation tasks such as translation, summarization and\ninstruction following. However, their enormous size and reliance on\nautoregressive decoding increase deployment costs and complicate their use in\nlatency-critical applications. In this work, we propose a hybrid approach that\ncombines language models of different sizes to increase the efficiency of\nautoregressive decoding while maintaining high performance. Our method utilizes\na pretrained frozen LLM that encodes all prompt tokens once in parallel, and\nuses the resulting representations to condition and guide a small language\nmodel (SLM), which then generates the response more efficiently. We investigate\nthe combination of encoder-decoder LLMs with both encoder-decoder and\ndecoder-only SLMs from different model families and only require fine-tuning of\nthe SLM. Experiments with various benchmarks show substantial speedups of up to\n$4\\times$, with minor performance penalties of $1-2\\%$ for translation and\nsummarization tasks compared to the LLM.\n","authors":["Benjamin Bergner","Andrii Skliar","Amelie Royer","Tijmen Blankevoort","Yuki Asano","Babak Ehteshami Bejnordi"],"pdf_url":"https://arxiv.org/pdf/2402.16844v3.pdf","comment":"Work presented at the ES-FoMo II Workshop at ICML 2024"},{"id":"http://arxiv.org/abs/2407.11068v2","updated":"2024-07-17T13:47:30Z","published":"2024-07-12T14:17:26Z","title":"Show, Don't Tell: Evaluating Large Language Models Beyond Textual\n  Understanding with ChildPlay","summary":"  We explore the hypothesis that LLMs, such as GPT-3.5 and GPT-4, possess\nbroader cognitive functions, particularly in non-linguistic domains. Our\napproach extends beyond standard linguistic benchmarks by incorporating games\nlike Tic-Tac-Toe, Connect Four, and Battleship, encoded via ASCII, to assess\nstrategic thinking and decision-making. To evaluate the models' ability to\ngeneralize beyond their training data, we introduce two additional games. The\nfirst game, LEGO Connect Language (LCL), tests the models' capacity to\nunderstand spatial logic and follow assembly instructions. The second game, the\ngame of shapes, challenges the models to identify shapes represented by 1s\nwithin a matrix of zeros, further testing their spatial reasoning skills. This\n\"show, don't tell\" strategy uses games instead of simply querying the models.\nOur results show that despite their proficiency on standard benchmarks, GPT-3.5\nand GPT-4's abilities to play and reason about fully observable games without\npre-training is mediocre. Both models fail to anticipate losing moves in\nTic-Tac-Toe and Connect Four, and they are unable to play Battleship correctly.\nWhile GPT-4 shows some success in the game of shapes, both models fail at the\nassembly tasks presented in the LCL game. These results suggest that while GPT\nmodels can emulate conversational proficiency and basic rule comprehension,\ntheir performance in strategic gameplay and spatial reasoning tasks is very\nlimited. Importantly, this reveals a blind spot in current LLM benchmarks that\nwe highlight with our gameplay benchmark suite ChildPlay\n(https://github.com/child-play-neurips/child-play). Our findings provide a\ncautionary tale about claims of emergent intelligence and reasoning\ncapabilities of LLMs that are roughly the size of GPT-3.5 and GPT-4.\n","authors":["Gonçalo Hora de Carvalho","Robert Pollice","Oscar Knap"],"pdf_url":"https://arxiv.org/pdf/2407.11068v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12543v1","updated":"2024-07-17T13:27:26Z","published":"2024-07-17T13:27:26Z","title":"Abstraction Alignment: Comparing Model and Human Conceptual\n  Relationships","summary":"  Abstraction -- the process of generalizing specific examples into broad\nreusable patterns -- is central to how people efficiently process and store\ninformation and apply their knowledge to new data. Promisingly, research has\nshown that ML models learn representations that span levels of abstraction,\nfrom specific concepts like \"bolo tie\" and \"car tire\" to more general concepts\nlike \"CEO\" and \"model\". However, existing techniques analyze these\nrepresentations in isolation, treating learned concepts as independent\nartifacts rather than an interconnected web of abstraction. As a result,\nalthough we can identify the concepts a model uses to produce its output, it is\ndifficult to assess if it has learned a human-aligned abstraction of the\nconcepts that will generalize to new data. To address this gap, we introduce\nabstraction alignment, a methodology to measure the agreement between a model's\nlearned abstraction and the expected human abstraction. We quantify abstraction\nalignment by comparing model outputs against a human abstraction graph, such as\nlinguistic relationships or medical disease hierarchies. In evaluation tasks\ninterpreting image models, benchmarking language models, and analyzing medical\ndatasets, abstraction alignment provides a deeper understanding of model\nbehavior and dataset content, differentiating errors based on their agreement\nwith human knowledge, expanding the verbosity of current model quality metrics,\nand revealing ways to improve existing human abstractions.\n","authors":["Angie Boggust","Hyemin Bang","Hendrik Strobelt","Arvind Satyanarayan"],"pdf_url":"https://arxiv.org/pdf/2407.12543v1.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.12532v1","updated":"2024-07-17T13:14:00Z","published":"2024-07-17T13:14:00Z","title":"Towards Collaborative Intelligence: Propagating Intentions and Reasoning\n  for Multi-Agent Coordination with Large Language Models","summary":"  Effective collaboration in multi-agent systems requires communicating goals\nand intentions between agents. Current agent frameworks often suffer from\ndependencies on single-agent execution and lack robust inter-module\ncommunication, frequently leading to suboptimal multi-agent reinforcement\nlearning (MARL) policies and inadequate task coordination. To address these\nchallenges, we present a framework for training large language models (LLMs) as\ncollaborative agents to enable coordinated behaviors in cooperative MARL. Each\nagent maintains a private intention consisting of its current goal and\nassociated sub-tasks. Agents broadcast their intentions periodically, allowing\nother agents to infer coordination tasks. A propagation network transforms\nbroadcast intentions into teammate-specific communication messages, sharing\nrelevant goals with designated teammates. The architecture of our framework is\nstructured into planning, grounding, and execution modules. During execution,\nmultiple agents interact in a downstream environment and communicate intentions\nto enable coordinated behaviors. The grounding module dynamically adapts\ncomprehension strategies based on emerging coordination patterns, while\nfeedback from execution agents influnces the planning module, enabling the\ndynamic re-planning of sub-tasks. Results in collaborative environment\nsimulation demonstrate intention propagation reduces miscoordination errors by\naligning sub-task dependencies between agents. Agents learn when to communicate\nintentions and which teammates require task details, resulting in emergent\ncoordinated behaviors. This demonstrates the efficacy of intention sharing for\ncooperative multi-agent RL based on LLMs.\n","authors":["Xihe Qiu","Haoyu Wang","Xiaoyu Tan","Chao Qu","Yujie Xiong","Yuan Cheng","Yinghui Xu","Wei Chu","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2407.12532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04078v3","updated":"2024-07-17T13:13:05Z","published":"2024-07-04T17:39:16Z","title":"DotaMath: Decomposition of Thought with Code Assistance and\n  Self-correction for Mathematical Reasoning","summary":"  Large language models (LLMs) have made impressive progress in handling simple\nmath problems, yet they still struggle with more challenging and complex\nmathematical tasks. In this paper, we introduce a series of LLMs that employs\nthe Decomposition of thought with code assistance and self-correction for\nmathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex\nmathematical tasks by decomposing them into simpler logical subtasks,\nleveraging code to solve these subtasks, obtaining fine-grained feedback from\nthe code interpreter, and engaging in self-reflection and correction. By\nannotating diverse interactive tool-use trajectories and employing query\nevolution on GSM8K and MATH datasets, we generate an instruction fine-tuning\ndataset called DotaMathQA with 574K query-response pairs. We train a series of\nbase LLMs using imitation learning on DotaMathQA, resulting in DotaMath models\nthat achieve remarkable performance compared to open-source LLMs across various\nin-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases\nan outstanding performance of 64.8% on the competitive MATH dataset and 86.7%\non GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a\nseries of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward,\nwe anticipate that the DotaMath paradigm will open new pathways for addressing\nintricate mathematical problems. Our code is publicly available at\nhttps://github.com/ChengpengLi1003/DotaMath.\n","authors":["Chengpeng Li","Guanting Dong","Mingfeng Xue","Ru Peng","Xiang Wang","Dayiheng Liu"],"pdf_url":"https://arxiv.org/pdf/2407.04078v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2407.12529v1","updated":"2024-07-17T13:11:28Z","published":"2024-07-17T13:11:28Z","title":"Crafting the Path: Robust Query Rewriting for Information Retrieval","summary":"  Query rewriting aims to generate a new query that can complement the original\nquery to improve the information retrieval system. Recent studies on query\nrewriting, such as query2doc (Q2D), query2expand (Q2E) and querey2cot (Q2C),\nrely on the internal knowledge of Large Language Models (LLMs) to generate a\nrelevant passage to add information to the query. Nevertheless, the efficacy of\nthese methodologies may markedly decline in instances where the requisite\nknowledge is not encapsulated within the model's intrinsic parameters. In this\npaper, we propose a novel structured query rewriting method called Crafting the\nPath tailored for retrieval systems. Crafting the Path involves a three-step\nprocess that crafts query-related information necessary for finding the\npassages to be searched in each step. Specifically, the Crafting the Path\nbegins with Query Concept Comprehension, proceeds to Query Type Identification,\nand finally conducts Expected Answer Extraction. Experimental results show that\nour method outperforms previous rewriting methods, especially in less familiar\ndomains for LLMs. We demonstrate that our method is less dependent on the\ninternal parameter knowledge of the model and generates queries with fewer\nfactual inaccuracies. Furthermore, we observe that Crafting the Path has less\nlatency compared to the baselines.\n","authors":["Ingeol Baek","Jimin Lee","Joonho Yang","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2407.12529v1.pdf","comment":"1 figure, 12 tables"},{"id":"http://arxiv.org/abs/2407.12522v1","updated":"2024-07-17T13:06:25Z","published":"2024-07-17T13:06:25Z","title":"Struct-X: Enhancing Large Language Models Reasoning with Structured Data","summary":"  Structured data, rich in logical and relational information, has the\npotential to enhance the reasoning abilities of large language models (LLMs).\nStill, its integration poses a challenge due to the risk of overwhelming LLMs\nwith excessive tokens and irrelevant context information. To address this, we\npropose Struct-X, a novel framework that operates through five key phases:\n``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize\nstructured data. It begins by encoding structured data into a topological space\nusing graph embeddings, followed by filling in missing entity information with\nknowledge retrieval modules, and filtering out irrelevant tokens via a\nself-supervised module. The final phase involves constructing a topological\nnetwork with selected tokens to further reduce the total token length for more\neffective LLM inference. Additionally, Struct-X includes an Auxiliary Module\ntrained to generate prompts, aiding LLMs in analyzing structured data.\nExtensive experiments on benchmarks, including the knowledge graph\nquestion-answer task and the long document reading comprehension task, show\nthat Struct-X notably improves LLM reasoning, demonstrating the effectiveness\nof structured data augmentation in improving LLM inference with complex input\ncontext.\n","authors":["Xiaoyu Tan","Haoyu Wang","Xihe Qiu","Yuan Cheng","Yinghui Xu","Wei Chu","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2407.12522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06979v2","updated":"2024-07-17T13:03:08Z","published":"2023-09-13T14:15:03Z","title":"Auto-Regressive Next-Token Predictors are Universal Learners","summary":"  Large language models display remarkable capabilities in logical and\nmathematical reasoning, allowing them to solve complex tasks. Interestingly,\nthese abilities emerge in networks trained on the simple task of next-token\nprediction. In this work, we present a theoretical framework for studying\nauto-regressive next-token predictors. We demonstrate that even simple models\nsuch as linear next-token predictors, trained on Chain-of-Thought (CoT) data,\ncan approximate any function efficiently computed by a Turing machine. We\nintroduce a new complexity measure -- length complexity -- which measures the\nnumber of intermediate tokens in a CoT sequence required to approximate some\ntarget function, and analyze the interplay between length complexity and other\nnotions of complexity. Finally, we show experimentally that simple next-token\npredictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs),\ndisplay non-trivial performance on text generation and arithmetic tasks. Our\nresults demonstrate that the power of today's LLMs can be attributed, to a\ngreat extent, to the auto-regressive next-token training scheme, and not\nnecessarily to a particular choice of architecture.\n","authors":["Eran Malach"],"pdf_url":"https://arxiv.org/pdf/2309.06979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04482v2","updated":"2024-07-17T13:01:26Z","published":"2024-01-09T10:39:17Z","title":"Continuously Learning New Words in Automatic Speech Recognition","summary":"  Despite recent advances, Automatic Speech Recognition (ASR) systems are still\nfar from perfect. Typical errors include acronyms, named entities and\ndomain-specific special words for which little or no data is available. To\naddress the problem of recognizing these words, we propose an self-supervised\ncontinual learning approach. Given the audio of a lecture talk with\ncorresponding slides, we bias the model towards decoding new words from the\nslides by using a memory-enhanced ASR model from previous work. Then, we\nperform inference on the talk, collecting utterances that contain detected new\nwords into an adaptation dataset. Continual learning is then performed on this\nset by adapting low-rank matrix weights added to each weight matrix of the\nmodel. The whole procedure is iterated for many talks. We show that with this\napproach, we obtain increasing performance on the new words when they occur\nmore frequently (more than 80% recall) while preserving the general performance\nof the model.\n","authors":["Christian Huber","Alexander Waibel"],"pdf_url":"https://arxiv.org/pdf/2401.04482v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12514v1","updated":"2024-07-17T11:57:10Z","published":"2024-07-17T11:57:10Z","title":"On Initializing Transformers with Pre-trained Embeddings","summary":"  It has become common practice now to use random initialization schemes,\nrather than the pre-trained embeddings, when training transformer based models\nfrom scratch. Indeed, we find that pre-trained word embeddings from GloVe, and\nsome sub-word embeddings extracted from language models such as T5 and mT5 fare\nmuch worse compared to random initialization. This is counter-intuitive given\nthe well-known representational and transfer-learning advantages of\npre-training. Interestingly, we also find that BERT and mBERT embeddings fare\nbetter than random initialization, showing the advantages of pre-trained\nrepresentations. In this work, we posit two potential factors that contribute\nto these mixed results: the model sensitivity to parameter distribution and the\nembedding interactions with position encodings. We observe that pre-trained\nGloVe, T5, and mT5 embeddings have a wider distribution of values. As argued in\nthe initialization studies, such large value initializations can lead to poor\ntraining because of saturated outputs. Further, the larger embedding values\ncan, in effect, absorb the smaller position encoding values when added\ntogether, thus losing position information. Standardizing the pre-trained\nembeddings to a narrow range (e.g. as prescribed by Xavier) leads to\nsubstantial gains for Glove, T5, and mT5 embeddings. On the other hand, BERT\npre-trained embeddings, while larger, are still relatively closer to Xavier\ninitialization range which may allow it to effectively transfer the pre-trained\nknowledge.\n","authors":["Ha Young Kim","Niranjan Balasubramanian","Byungkon Kang"],"pdf_url":"https://arxiv.org/pdf/2407.12514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12512v1","updated":"2024-07-17T11:53:39Z","published":"2024-07-17T11:53:39Z","title":"$\\textit{GeoHard}$: Towards Measuring Class-wise Hardness through\n  Modelling Class Semantics","summary":"  Recent advances in measuring hardness-wise properties of data guide language\nmodels in sample selection within low-resource scenarios. However,\nclass-specific properties are overlooked for task setup and learning. How will\nthese properties influence model learning and is it generalizable across\ndatasets? To answer this question, this work formally initiates the concept of\n$\\textit{class-wise hardness}$. Experiments across eight natural language\nunderstanding (NLU) datasets demonstrate a consistent hardness distribution\nacross learning paradigms, models, and human judgment. Subsequent experiments\nunveil a notable challenge in measuring such class-wise hardness with\ninstance-level metrics in previous works. To address this, we propose\n$\\textit{GeoHard}$ for class-wise hardness measurement by modeling class\ngeometry in the semantic embedding space. $\\textit{GeoHard}$ surpasses\ninstance-level metrics by over 59 percent on $\\textit{Pearson}$'s correlation\non measuring class-wise hardness. Our analysis theoretically and empirically\nunderscores the generality of $\\textit{GeoHard}$ as a fresh perspective on data\ndiagnosis. Additionally, we showcase how understanding class-wise hardness can\npractically aid in improving task learning.\n","authors":["Fengyu Cai","Xinran Zhao","Hongming Zhang","Iryna Gurevych","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2407.12512v1.pdf","comment":"Findings of ACL 2024"},{"id":"http://arxiv.org/abs/2407.12508v1","updated":"2024-07-17T11:45:02Z","published":"2024-07-17T11:45:02Z","title":"MERLIN: Multimodal Embedding Refinement via LLM-based Iterative\n  Navigation for Text-Video Retrieval-Rerank Pipeline","summary":"  The rapid expansion of multimedia content has made accurately retrieving\nrelevant videos from large collections increasingly challenging. Recent\nadvancements in text-video retrieval have focused on cross-modal interactions,\nlarge-scale foundation model training, and probabilistic modeling, yet often\nneglect the crucial user perspective, leading to discrepancies between user\nqueries and the content retrieved. To address this, we introduce MERLIN\n(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,\ntraining-free pipeline that leverages Large Language Models (LLMs) for\niterative feedback learning. MERLIN refines query embeddings from a user\nperspective, enhancing alignment between queries and video content through a\ndynamic question answering process. Experimental results on datasets like\nMSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves\nRecall@1, outperforming existing systems and confirming the benefits of\nintegrating LLMs into multimodal retrieval systems for more responsive and\ncontext-aware multimedia retrieval.\n","authors":["Donghoon Han","Eunhwan Park","Gisang Lee","Adam Lee","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.12508v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2407.12504v1","updated":"2024-07-17T11:35:00Z","published":"2024-07-17T11:35:00Z","title":"Case2Code: Learning Inductive Reasoning with Synthetic Data","summary":"  Complex reasoning is an impressive ability shown by large language models\n(LLMs). Most LLMs are skilled in deductive reasoning, such as chain-of-thought\nprompting or iterative tool-using to solve challenging tasks step-by-step. In\nthis paper, we hope to focus on evaluating and teaching LLMs to conduct\ninductive reasoning, that is, LLMs are supposed to infer underlying rules by\nobserving examples or sequential transformations. However, collecting\nlarge-scale and diverse human-generated inductive data is challenging. We focus\non data synthesis in the code domain and propose a \\textbf{Case2Code} task by\nexploiting the expressiveness and correctness of programs. Specifically, we\ncollect a diverse set of executable programs, synthesize input-output\ntransformations for each program, and force LLMs to infer the underlying code\nimplementations based on the synthetic I/O cases. We first evaluate\nrepresentative LLMs on the synthesized Case2Code task and demonstrate that the\nCase-to-code induction is challenging for LLMs. Then, we synthesize large-scale\nCase2Code training samples to train LLMs to perform inductive reasoning.\nExperimental results show that such induction training benefits not only in\ndistribution Case2Code performance but also enhances various coding abilities\nof trained LLMs, demonstrating the great potential of learning inductive\nreasoning via synthetic data.\n","authors":["Yunfan Shao","Linyang Li","Yichuan Ma","Peiji Li","Demin Song","Qinyuan Cheng","Shimin Li","Xiaonan Li","Pengyu Wang","Qipeng Guo","Hang Yan","Xipeng Qiu","Xuanjing Huang","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2407.12504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12500v1","updated":"2024-07-17T11:30:04Z","published":"2024-07-17T11:30:04Z","title":"Automate or Assist? The Role of Computational Models in Identifying\n  Gendered Discourse in US Capital Trial Transcripts","summary":"  The language used by US courtroom actors in criminal trials has long been\nstudied for biases. However, systematic studies for bias in high-stakes court\ntrials have been difficult, due to the nuanced nature of bias and the legal\nexpertise required. New large language models offer the possibility to automate\nannotation, saving time and cost. But validating these approaches requires both\nhigh quantitative performance as well as an understanding of how automated\nmethods fit in existing workflows, and what they really offer. In this paper we\npresent a case study of adding an automated system to a complex and high-stakes\nproblem: identifying gender-biased language in US capital trials for women\ndefendants. Our team of experienced death-penalty lawyers and NLP technologists\npursued a three-phase study: first annotating manually, then training and\nevaluating computational models, and finally comparing human annotations to\nmodel predictions. Unlike many typical NLP tasks, annotating for gender bias in\nmonths-long capital trials was a complicated task that involves with many\nindividual judgment calls. In contrast to standard arguments for automation\nthat are based on efficiency and scalability, legal experts found the\ncomputational models most useful in challenging their personal bias in\nannotation and providing opportunities to refine and build consensus on rules\nfor annotation. This suggests that seeking to replace experts with\ncomputational models is both unrealistic and undesirable. Rather, computational\nmodels offer valuable opportunities to assist the legal experts in\nannotation-based studies.\n","authors":["Andrea W Wen-Yi","Kathryn Adamson","Nathalie Greenfield","Rachel Goldberg","Sandra Babcock","David Mimno","Allison Koenecke"],"pdf_url":"https://arxiv.org/pdf/2407.12500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03415v3","updated":"2024-07-17T11:29:10Z","published":"2023-08-07T09:06:20Z","title":"End-to-End Evaluation for Low-Latency Simultaneous Speech Translation","summary":"  The challenge of low-latency speech translation has recently draw significant\ninterest in the research community as shown by several publications and shared\ntasks. Therefore, it is essential to evaluate these different approaches in\nrealistic scenarios. However, currently only specific aspects of the systems\nare evaluated and often it is not possible to compare different approaches.\n  In this work, we propose the first framework to perform and evaluate the\nvarious aspects of low-latency speech translation under realistic conditions.\nThe evaluation is carried out in an end-to-end fashion. This includes the\nsegmentation of the audio as well as the run-time of the different components.\n  Secondly, we compare different approaches to low-latency speech translation\nusing this framework. We evaluate models with the option to revise the output\nas well as methods with fixed output. Furthermore, we directly compare\nstate-of-the-art cascaded as well as end-to-end systems. Finally, the framework\nallows to automatically evaluate the translation quality as well as latency and\nalso provides a web interface to show the low-latency model outputs to the\nuser.\n","authors":["Christian Huber","Tu Anh Dinh","Carlos Mullov","Ngoc Quan Pham","Thai Binh Nguyen","Fabian Retkowski","Stefan Constantin","Enes Yavuz Ugan","Danni Liu","Zhaolin Li","Sai Koneru","Jan Niehues","Alexander Waibel"],"pdf_url":"https://arxiv.org/pdf/2308.03415v3.pdf","comment":"Demo paper at EMNLP 2023"},{"id":"http://arxiv.org/abs/2407.12498v1","updated":"2024-07-17T11:26:47Z","published":"2024-07-17T11:26:47Z","title":"Evaluating Linguistic Capabilities of Multimodal LLMs in the Lens of\n  Few-Shot Learning","summary":"  The linguistic capabilities of Multimodal Large Language Models (MLLMs) are\ncritical for their effective application across diverse tasks. This study aims\nto evaluate the performance of MLLMs on the VALSE benchmark, focusing on the\nefficacy of few-shot In-Context Learning (ICL), and Chain-of-Thought (CoT)\nprompting. We conducted a comprehensive assessment of state-of-the-art MLLMs,\nvarying in model size and pretraining datasets. The experimental results reveal\nthat ICL and CoT prompting significantly boost model performance, particularly\nin tasks requiring complex reasoning and contextual understanding. Models\npretrained on captioning datasets show superior zero-shot performance, while\nthose trained on interleaved image-text data benefit from few-shot learning.\nOur findings provide valuable insights into optimizing MLLMs for better\ngrounding of language in visual contexts, highlighting the importance of the\ncomposition of pretraining data and the potential of few-shot learning\nstrategies to improve the reasoning abilities of MLLMs.\n","authors":["Mustafa Dogan","Ilker Kesen","Iacer Calixto","Aykut Erdem","Erkut Erdem"],"pdf_url":"https://arxiv.org/pdf/2407.12498v1.pdf","comment":"Preprint. 33 pages, 17 Figures, 3 Tables"},{"id":"http://arxiv.org/abs/2312.03766v2","updated":"2024-07-17T11:12:26Z","published":"2023-12-05T20:07:34Z","title":"Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment","summary":"  While existing image-text alignment models reach high quality binary\nassessments, they fall short of pinpointing the exact source of misalignment.\nIn this paper, we present a method to provide detailed textual and visual\nexplanation of detected misalignments between text-image pairs. We leverage\nlarge language models and visual grounding models to automatically construct a\ntraining set that holds plausible misaligned captions for a given image and\ncorresponding textual explanations and visual indicators. We also publish a new\nhuman curated test set comprising ground-truth textual and visual misalignment\nannotations. Empirical results show that fine-tuning vision language models on\nour training set enables them to articulate misalignments and visually indicate\nthem within images, outperforming strong baselines both on the binary alignment\nclassification and the explanation generation tasks. Our method code and human\ncurated test set are available at: https://mismatch-quest.github.io/\n","authors":["Brian Gordon","Yonatan Bitton","Yonatan Shafir","Roopal Garg","Xi Chen","Dani Lischinski","Daniel Cohen-Or","Idan Szpektor"],"pdf_url":"https://arxiv.org/pdf/2312.03766v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12481v1","updated":"2024-07-17T11:06:27Z","published":"2024-07-17T11:06:27Z","title":"Pretraining Data and Tokenizer for Indic LLM","summary":"  We present a novel approach to data preparation for developing multilingual\nIndic large language model. Our meticulous data acquisition spans open-source\nand proprietary sources, including Common Crawl, Indic books, news articles,\nand Wikipedia, ensuring a diverse and rich linguistic representation. For each\nIndic language, we design a custom preprocessing pipeline to effectively\neliminate redundant and low-quality text content. Additionally, we perform\ndeduplication on Common Crawl data to address the redundancy present in 70% of\nthe crawled web pages. This study focuses on developing high-quality data,\noptimizing tokenization for our multilingual dataset for Indic large language\nmodels with 3B and 7B parameters, engineered for superior performance in Indic\nlanguages. We introduce a novel multilingual tokenizer training strategy,\ndemonstrating our custom-trained Indic tokenizer outperforms the\nstate-of-the-art OpenAI Tiktoken tokenizer, achieving a superior token-to-word\nratio for Indic languages.\n","authors":["Rahul Kumar","Shubham Kakde","Divyansh Rajput","Daud Ibrahim","Rishabh Nahata","Pidathala Sowjanya","Deepak Kumar"],"pdf_url":"https://arxiv.org/pdf/2407.12481v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.12982v1","updated":"2024-07-17T20:01:21Z","published":"2024-07-17T20:01:21Z","title":"Retrieval-Enhanced Machine Learning: Synthesis and Opportunities","summary":"  In the field of language modeling, models augmented with retrieval components\nhave emerged as a promising solution to address several challenges faced in the\nnatural language processing (NLP) field, including knowledge grounding,\ninterpretability, and scalability. Despite the primary focus on NLP, we posit\nthat the paradigm of retrieval-enhancement can be extended to a broader\nspectrum of machine learning (ML) such as computer vision, time series\nprediction, and computational biology. Therefore, this work introduces a formal\nframework of this paradigm, Retrieval-Enhanced Machine Learning (REML), by\nsynthesizing the literature in various domains in ML with consistent notations\nwhich is missing from the current literature. Also, we found that while a\nnumber of studies employ retrieval components to augment their models, there is\na lack of integration with foundational Information Retrieval (IR) research. We\nbridge this gap between the seminal IR research and contemporary REML studies\nby investigating each component that comprises the REML framework. Ultimately,\nthe goal of this work is to equip researchers across various disciplines with a\ncomprehensive, formally structured framework of retrieval-enhanced models,\nthereby fostering interdisciplinary future research.\n","authors":["To Eun Kim","Alireza Salemi","Andrew Drozdov","Fernando Diaz","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2407.12982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12784v1","updated":"2024-07-17T17:59:47Z","published":"2024-07-17T17:59:47Z","title":"AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge\n  Bases","summary":"  LLM agents have demonstrated remarkable performance across various\napplications, primarily due to their advanced capabilities in reasoning,\nutilizing external knowledge and tools, calling APIs, and executing actions to\ninteract with environments. Current agents typically utilize a memory module or\na retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and\ninstances with similar embeddings from knowledge bases to inform task planning\nand execution. However, the reliance on unverified knowledge bases raises\nsignificant concerns about their safety and trustworthiness. To uncover such\nvulnerabilities, we propose a novel red teaming approach AgentPoison, the first\nbackdoor attack targeting generic and RAG-based LLM agents by poisoning their\nlong-term memory or RAG knowledge base. In particular, we form the trigger\ngeneration process as a constrained optimization to optimize backdoor triggers\nby mapping the triggered instances to a unique embedding space, so as to ensure\nthat whenever a user instruction contains the optimized backdoor trigger, the\nmalicious demonstrations are retrieved from the poisoned memory or knowledge\nbase with high probability. In the meantime, benign instructions without the\ntrigger will still maintain normal performance. Unlike conventional backdoor\nattacks, AgentPoison requires no additional model training or fine-tuning, and\nthe optimized backdoor trigger exhibits superior transferability, in-context\ncoherence, and stealthiness. Extensive experiments demonstrate AgentPoison's\neffectiveness in attacking three types of real-world LLM agents: RAG-based\nautonomous driving agent, knowledge-intensive QA agent, and healthcare\nEHRAgent. On each agent, AgentPoison achieves an average attack success rate\nhigher than 80% with minimal impact on benign performance (less than 1%) with a\npoison rate less than 0.1%.\n","authors":["Zhaorun Chen","Zhen Xiang","Chaowei Xiao","Dawn Song","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2407.12784v1.pdf","comment":"22 pages, 13 figures, 7 tables"},{"id":"http://arxiv.org/abs/2407.12580v1","updated":"2024-07-17T14:04:12Z","published":"2024-07-17T14:04:12Z","title":"E5-V: Universal Embeddings with Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) have shown promising advancements in\ngeneral visual and language understanding. However, the representation of\nmultimodal information using MLLMs remains largely unexplored. In this work, we\nintroduce a new framework, E5-V, designed to adapt MLLMs for achieving\nuniversal multimodal embeddings. Our findings highlight the significant\npotential of MLLMs in representing multimodal inputs compared to previous\napproaches. By leveraging MLLMs with prompts, E5-V effectively bridges the\nmodality gap between different types of inputs, demonstrating strong\nperformance in multimodal embeddings even without fine-tuning. We propose a\nsingle modality training approach for E5-V, where the model is trained\nexclusively on text pairs. This method demonstrates significant improvements\nover traditional multimodal training on image-text pairs, while reducing\ntraining costs by approximately 95%. Additionally, this approach eliminates the\nneed for costly multimodal training data collection. Extensive experiments\nacross four types of tasks demonstrate the effectiveness of E5-V. As a\nuniversal multimodal model, E5-V not only achieves but often surpasses\nstate-of-the-art performance in each task, despite being trained on a single\nmodality.\n","authors":["Ting Jiang","Minghui Song","Zihan Zhang","Haizhen Huang","Weiwei Deng","Feng Sun","Qi Zhang","Deqing Wang","Fuzhen Zhuang"],"pdf_url":"https://arxiv.org/pdf/2407.12580v1.pdf","comment":"Code and models are available at https://github.com/kongds/E5-V"},{"id":"http://arxiv.org/abs/2407.11712v2","updated":"2024-07-17T13:57:55Z","published":"2024-07-16T13:30:14Z","title":"Harnessing Large Language Models for Multimodal Product Bundling","summary":"  Product bundling provides clients with a strategic combination of individual\nitems. And it has gained significant attention in recent years as a fundamental\nprerequisite for online services. Recent methods utilize multimodal information\nthrough sophisticated extractors for bundling, but remain limited by inferior\nsemantic understanding, the restricted scope of knowledge, and an inability to\nhandle cold-start issues. Despite the extensive knowledge and complex reasoning\ncapabilities of large language models (LLMs), their direct utilization fails to\nprocess multimodalities and exploit their knowledge for multimodal product\nbundling. Adapting LLMs for this purpose involves demonstrating the synergies\namong different modalities and designing an effective optimization strategy for\nbundling, which remains challenging. To this end, we introduce Bundle-LLM to\nbridge the gap between LLMs and product bundling tasks. Specifically, we\nutilize a hybrid item tokenization to integrate multimodal information, where a\nsimple yet powerful multimodal fusion module followed by a trainable projector\nembeds all non-textual features into a single token. This module not only\nexplicitly exhibits the interplays among modalities but also shortens the\nprompt length, thereby boosting efficiency. By designing a prompt template, we\nformulate product bundling as a multiple-choice question given candidate items.\nFurthermore, we adopt progressive optimization strategy to fine-tune the LLMs\nfor disentangled objectives, achieving effective product bundling capability\nwith comprehensive multimodal semantic understanding. Extensive experiments on\nfour datasets from two application domains show that our approach outperforms a\nrange of state-of-the-art (SOTA) methods.\n","authors":["Xiaohao Liu","Jie Wu","Zhulin Tao","Yunshan Ma","Yinwei Wei","Tat-seng Chua"],"pdf_url":"https://arxiv.org/pdf/2407.11712v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2312.04763v2","updated":"2024-07-17T13:37:11Z","published":"2023-12-08T00:27:54Z","title":"Enhancing Recipe Retrieval with Foundation Models: A Data Augmentation\n  Perspective","summary":"  Learning recipe and food image representation in common embedding space is\nnon-trivial but crucial for cross-modal recipe retrieval. In this paper, we\npropose a new perspective for this problem by utilizing foundation models for\ndata augmentation. Leveraging on the remarkable capabilities of foundation\nmodels (i.e., Llama2 and SAM), we propose to augment recipe and food image by\nextracting alignable information related to the counterpart. Specifically,\nLlama2 is employed to generate a textual description from the recipe, aiming to\ncapture the visual cues of a food image, and SAM is used to produce image\nsegments that correspond to key ingredients in the recipe. To make full use of\nthe augmented data, we introduce Data Augmented Retrieval framework (DAR) to\nenhance recipe and image representation learning for cross-modal retrieval. We\nfirst inject adapter layers to pre-trained CLIP model to reduce computation\ncost rather than fully fine-tuning all the parameters. In addition, multi-level\ncircle loss is proposed to align the original and augmented data pairs, which\nassigns different penalties for positive and negative pairs. On the Recipe1M\ndataset, our DAR outperforms all existing methods by a large margin. Extensive\nablation studies validate the effectiveness of each component of DAR.\n","authors":["Fangzhou Song","Bin Zhu","Yanbin Hao","Shuo Wang"],"pdf_url":"https://arxiv.org/pdf/2312.04763v2.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2406.09818v2","updated":"2024-07-17T12:10:09Z","published":"2024-06-14T08:21:42Z","title":"ClimRetrieve: A Benchmarking Dataset for Information Retrieval from\n  Corporate Climate Disclosures","summary":"  To handle the vast amounts of qualitative data produced in corporate climate\ncommunication, stakeholders increasingly rely on Retrieval Augmented Generation\n(RAG) systems. However, a significant gap remains in evaluating domain-specific\ninformation retrieval - the basis for answer generation. To address this\nchallenge, this work simulates the typical tasks of a sustainability analyst by\nexamining 30 sustainability reports with 16 detailed climate-related questions.\nAs a result, we obtain a dataset with over 8.5K unique question-source-answer\npairs labeled by different levels of relevance. Furthermore, we develop a use\ncase with the dataset to investigate the integration of expert knowledge into\ninformation retrieval with embeddings. Although we show that incorporating\nexpert knowledge works, we also outline the critical limitations of embeddings\nin knowledge-intensive downstream domains like climate change communication.\n","authors":["Tobias Schimanski","Jingwei Ni","Roberto Spacey","Nicola Ranger","Markus Leippold"],"pdf_url":"https://arxiv.org/pdf/2406.09818v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00635v2","updated":"2024-07-17T10:26:36Z","published":"2024-06-30T09:25:42Z","title":"Dense Retrieval with Continuous Explicit Feedback for Systematic Review\n  Screening Prioritisation","summary":"  The goal of screening prioritisation in systematic reviews is to identify\nrelevant documents with high recall and rank them in early positions for\nreview. This saves reviewing effort if paired with a stopping criterion, and\nspeeds up review completion if performed alongside downstream tasks. Recent\nstudies have shown that neural models have good potential on this task, but\ntheir time-consuming fine-tuning and inference discourage their widespread use\nfor screening prioritisation. In this paper, we propose an alternative approach\nthat still relies on neural models, but leverages dense representations and\nrelevance feedback to enhance screening prioritisation, without the need for\ncostly model fine-tuning and inference. This method exploits continuous\nrelevance feedback from reviewers during document screening to efficiently\nupdate the dense query representation, which is then applied to rank the\nremaining documents to be screened. We evaluate this approach across the CLEF\nTAR datasets for this task. Results suggest that the investigated dense\nquery-driven approach is more efficient than directly using neural models and\nshows promising effectiveness compared to previous methods developed on the\nconsidered datasets. Our code is available at\nhttps://github.com/ielab/dense-screening-feedback.\n","authors":["Xinyu Mao","Shengyao Zhuang","Bevan Koopman","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2407.00635v2.pdf","comment":"Accepted at SIGIR 2024;typos corrected"},{"id":"http://arxiv.org/abs/2407.12385v1","updated":"2024-07-17T08:07:37Z","published":"2024-07-17T08:07:37Z","title":"RankTower: A Synergistic Framework for Enhancing Two-Tower Pre-Ranking\n  Model","summary":"  In large-scale ranking systems, cascading architectures have been widely\nadopted to achieve a balance between efficiency and effectiveness. The\npre-ranking module plays a vital role in selecting a subset of candidates for\nthe subsequent ranking module. It is crucial for the pre-ranking model to\nmaintain a balance between efficiency and accuracy to adhere to online latency\nconstraints. In this paper, we propose a novel neural network architecture\ncalled RankTower, which is designed to efficiently capture user-item\ninteractions while following the user-item decoupling paradigm to ensure online\ninference efficiency. The proposed approach employs a hybrid training objective\nthat learns from samples obtained from the full stage of the cascade ranking\nsystem, optimizing different objectives for varying sample spaces. This\nstrategy aims to enhance the pre-ranking model's ranking capability and\nimprovement alignment with the existing cascade ranking system. Experimental\nresults conducted on public datasets demonstrate that RankTower significantly\noutperforms state-of-the-art pre-ranking models.\n","authors":["YaChen Yan","Liubo Li"],"pdf_url":"https://arxiv.org/pdf/2407.12385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12374v1","updated":"2024-07-17T07:52:45Z","published":"2024-07-17T07:52:45Z","title":"Graph Signal Processing for Cross-Domain Recommendation","summary":"  Cross-domain recommendation (CDR) extends conventional recommender systems by\nleveraging user-item interactions from dense domains to mitigate data sparsity\nand the cold start problem. While CDR offers substantial potential for\nenhancing recommendation performance, most existing CDR methods suffer from\nsensitivity to the ratio of overlapping users and intrinsic discrepancy between\nsource and target domains. To overcome these limitations, in this work, we\nexplore the application of graph signal processing (GSP) in CDR scenarios. We\npropose CGSP, a unified CDR framework based on GSP, which employs a\ncross-domain similarity graph constructed by flexibly combining target-only\nsimilarity and source-bridged similarity. By processing personalized graph\nsignals computed for users from either the source or target domain, our\nframework effectively supports both inter-domain and intra-domain\nrecommendations. Our empirical evaluation demonstrates that CGSP consistently\noutperforms various encoder-based CDR approaches in both intra-domain and\ninter-domain recommendation scenarios, especially when the ratio of overlapping\nusers is low, highlighting its significant practical implication in real-world\napplications.\n","authors":["Jeongeun Lee","Seongku Kang","Won-Yong Shin","Jeongwhan Choi","Noseong Park","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2407.12374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12346v1","updated":"2024-07-17T06:42:14Z","published":"2024-07-17T06:42:14Z","title":"Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval","summary":"  The pre-trained vision and language (V\\&L) models have substantially improved\nthe performance of cross-modal image-text retrieval. In general, however, V\\&L\nmodels have limited retrieval performance for small objects because of the\nrough alignment between words and the small objects in the image. In contrast,\nit is known that human cognition is object-centric, and we pay more attention\nto important objects, even if they are small. To bridge this gap between the\nhuman cognition and the V\\&L model's capability, we propose a cross-modal\nimage-text retrieval framework based on ``object-aware query perturbation.''\nThe proposed method generates a key feature subspace of the detected objects\nand perturbs the corresponding queries using this subspace to improve the\nobject awareness in the image. In our proposed method, object-aware cross-modal\nimage-text retrieval is possible while keeping the rich expressive power and\nretrieval performance of existing V\\&L models without additional fine-tuning.\nComprehensive experiments on four public datasets show that our method\noutperforms conventional algorithms.\n","authors":["Naoya Sogi","Takashi Shibata","Makoto Terao"],"pdf_url":"https://arxiv.org/pdf/2407.12346v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.12338v1","updated":"2024-07-17T06:29:00Z","published":"2024-07-17T06:29:00Z","title":"GUME: Graphs and User Modalities Enhancement for Long-Tail Multimodal\n  Recommendation","summary":"  Multimodal recommendation systems (MMRS) have received considerable attention\nfrom the research community due to their ability to jointly utilize information\nfrom user behavior and product images and text. Previous research has two main\nissues. First, many long-tail items in recommendation systems have limited\ninteraction data, making it difficult to learn comprehensive and informative\nrepresentations. However, past MMRS studies have overlooked this issue.\nSecondly, users' modality preferences are crucial to their behavior. However,\nprevious research has primarily focused on learning item modality\nrepresentations, while user modality representations have remained relatively\nsimplistic.To address these challenges, we propose a novel Graphs and User\nModalities Enhancement (GUME) for long-tail multimodal recommendation.\nSpecifically, we first enhance the user-item graph using multimodal similarity\nbetween items. This improves the connectivity of long-tail items and helps them\nlearn high-quality representations through graph propagation. Then, we\nconstruct two types of user modalities: explicit interaction features and\nextended interest features. By using the user modality enhancement strategy to\nmaximize mutual information between these two features, we improve the\ngeneralization ability of user modality representations. Additionally, we\ndesign an alignment strategy for modality data to remove noise from both\ninternal and external perspectives. Extensive experiments on four publicly\navailable datasets demonstrate the effectiveness of our approach.\n","authors":["Guojiao Lin","Zhen Meng","Dongjie Wang","Qingqing Long","Yuanchun Zhou","Meng Xiao"],"pdf_url":"https://arxiv.org/pdf/2407.12338v1.pdf","comment":"11 pages, accepted by CIKM 2024"},{"id":"http://arxiv.org/abs/2407.12325v1","updated":"2024-07-17T05:50:32Z","published":"2024-07-17T05:50:32Z","title":"Optimizing Query Generation for Enhanced Document Retrieval in RAG","summary":"  Large Language Models (LLMs) excel in various language tasks but they often\ngenerate incorrect information, a phenomenon known as \"hallucinations\".\nRetrieval-Augmented Generation (RAG) aims to mitigate this by using document\nretrieval for accurate responses. However, RAG still faces hallucinations due\nto vague queries. This study aims to improve RAG by optimizing query generation\nwith a query-document alignment score, refining queries using LLMs for better\nprecision and efficiency of document retrieval. Experiments have shown that our\napproach improves document retrieval, resulting in an average accuracy gain of\n1.6%.\n","authors":["Hamin Koo","Minseon Kim","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2407.12325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13062v5","updated":"2024-07-17T05:26:03Z","published":"2023-05-22T14:23:46Z","title":"Table Meets LLM: Can Large Language Models Understand Structured Table\n  Data? A Benchmark and Empirical Study","summary":"  Large language models (LLMs) are becoming attractive as few-shot reasoners to\nsolve Natural Language (NL)-related tasks. However, the understanding of their\ncapability to process structured data like tables remains an under-explored\narea. While tables can be serialized as input for LLMs, there is a lack of\ncomprehensive studies on whether LLMs genuinely comprehend this data. In this\npaper, we try to understand this by designing a benchmark to evaluate the\nstructural understanding capabilities of LLMs through seven distinct tasks,\ne.g., cell lookup, row retrieval and size detection. Specially, we perform a\nseries of evaluations on the recent most advanced LLM models, GPT-3.5 and GPT-4\nand observe that performance varied with different input choices, including\ntable input format, content order, role prompting, and partition marks. Drawing\nfrom the insights gained through the benchmark evaluations, we propose\n$\\textit{self-augmentation}$ for effective structural prompting, such as\ncritical value / range identification using internal knowledge of LLMs. When\ncombined with carefully chosen input choices, these structural prompting\nmethods lead to promising improvements in LLM performance on a variety of\ntabular tasks, e.g., TabFact($\\uparrow2.31\\%$), HybridQA($\\uparrow2.13\\%$),\nSQA($\\uparrow2.72\\%$), Feverous($\\uparrow0.84\\%$), and ToTTo($\\uparrow5.68\\%$).\nWe believe that our open source benchmark and proposed prompting methods can\nserve as a simple yet generic selection for future research. The code and data\nof this paper will be temporality released at\nhttps://anonymous.4open.science/r/StructuredLLM-76F3/README.md and will be\nreplaced with an official one at https://github.com/microsoft/TableProvider\nlater.\n","authors":["Yuan Sui","Mengyu Zhou","Mingjie Zhou","Shi Han","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.13062v5.pdf","comment":"This paper has been accepted as a full paper at WSDM 2024. Explore\n  the MS research blog of our work at\n  https://www.microsoft.com/en-us/research/blog/improving-llm-understanding-of-structured-data-and-exploring-advanced-prompting-methods/"},{"id":"http://arxiv.org/abs/2307.02147v2","updated":"2024-07-17T05:14:56Z","published":"2023-07-05T09:42:51Z","title":"Recommendation Unlearning via Influence Function","summary":"  Recommendation unlearning is an emerging task to serve users for erasing\nunusable data (e.g., some historical behaviors) from a well-trained recommender\nmodel. Existing methods process unlearning requests by fully or partially\nretraining the model after removing the unusable data. However, these methods\nare impractical due to the high computation cost of full retraining and the\nhighly possible performance damage of partial training. In this light, a\ndesired recommendation unlearning method should obtain a similar model as full\nretraining in a more efficient manner, i.e., achieving complete, efficient and\nharmless unlearning.\n  In this work, we propose a new Influence Function-based Recommendation\nUnlearning (IFRU) framework, which efficiently updates the model without\nretraining by estimating the influence of the unusable data on the model via\nthe influence function. In the light that recent recommender models use\nhistorical data for both the constructions of the optimization loss and the\ncomputational graph (e.g., neighborhood aggregation), IFRU jointly estimates\nthe direct influence of unusable data on optimization loss and the spillover\ninfluence on the computational graph to pursue complete unlearning.\nFurthermore, we propose an importance-based pruning algorithm to reduce the\ncost of the influence function. IFRU is harmless and applicable to mainstream\ndifferentiable models. Extensive experiments demonstrate that IFRU achieves\nmore than 250 times acceleration compared to retraining-based methods with\nrecommendation performance comparable to full retraining. Codes are avaiable at\nhttps://github.com/baiyimeng/IFRU.\n","authors":["Yang Zhang","Zhiyu Hu","Yimeng Bai","Jiancan Wu","Qifan Wang","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2307.02147v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12315v1","updated":"2024-07-17T04:49:56Z","published":"2024-07-17T04:49:56Z","title":"ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via\n  Modal Fusion Map","summary":"  Multi-modal embeddings form the foundation for vision-language models, such\nas CLIP embeddings, the most widely used text-image embeddings. However, these\nembeddings are vulnerable to subtle misalignment of cross-modal features,\nresulting in decreased model performance and diminished generalization. To\naddress this problem, we design ModalChorus, an interactive system for visual\nprobing and alignment of multi-modal embeddings. ModalChorus primarily offers a\ntwo-stage process: 1) embedding probing with Modal Fusion Map (MFM), a novel\nparametric dimensionality reduction method that integrates both metric and\nnonmetric objectives to enhance modality fusion; and 2) embedding alignment\nthat allows users to interactively articulate intentions for both point-set and\nset-set alignments. Quantitative and qualitative comparisons for CLIP\nembeddings with existing dimensionality reduction (e.g., t-SNE and MDS) and\ndata fusion (e.g., data context map) methods demonstrate the advantages of MFM\nin showcasing cross-modal features over common vision-language datasets. Case\nstudies reveal that ModalChorus can facilitate intuitive discovery of\nmisalignment and efficient re-alignment in scenarios ranging from zero-shot\nclassification to cross-modal retrieval and generation.\n","authors":["Yilin Ye","Shishi Xiao","Xingchen Zeng","Wei Zeng"],"pdf_url":"https://arxiv.org/pdf/2407.12315v1.pdf","comment":"Accepted by VIS 2024"},{"id":"http://arxiv.org/abs/2402.13417v2","updated":"2024-07-17T04:39:11Z","published":"2024-02-20T23:04:06Z","title":"Unlocking the `Why' of Buying: Introducing a New Dataset and Benchmark\n  for Purchase Reason and Post-Purchase Experience","summary":"  Explanations are crucial for enhancing user trust and understanding within\nmodern recommendation systems. To build truly explainable systems, we need\nhigh-quality datasets that elucidate why users make choices. While previous\nefforts have focused on extracting users' post-purchase sentiment in reviews,\nthey ignore the reasons behind the decision to buy.\n  In our work, we propose a novel purchase reason explanation task. To this\nend, we introduce an LLM-based approach to generate a dataset that consists of\ntextual explanations of why real users make certain purchase decisions. We\ninduce LLMs to explicitly distinguish between the reasons behind purchasing a\nproduct and the experience after the purchase in a user review. An automated,\nLLM-driven evaluation, as well as a small scale human evaluation, confirms the\neffectiveness of our approach to obtaining high-quality, personalized\nexplanations. We benchmark this dataset on two personalized explanation\ngeneration tasks. We release the code and prompts to spur further research.\n","authors":["Tao Chen","Siqi Zuo","Cheng Li","Mingyang Zhang","Qiaozhu Mei","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2402.13417v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14230v2","updated":"2024-07-17T01:09:31Z","published":"2024-02-22T02:21:59Z","title":"MerRec: A Large-scale Multipurpose Mercari Dataset for\n  Consumer-to-Consumer Recommendation Systems","summary":"  In the evolving e-commerce field, recommendation systems crucially shape user\nexperience and engagement. The rise of Consumer-to-Consumer (C2C)\nrecommendation systems, noted for their flexibility and ease of access for\ncustomer vendors, marks a significant trend. However, the academic focus\nremains largely on Business-to-Consumer (B2C) models, leaving a gap filled by\nthe limited C2C recommendation datasets that lack in item attributes, user\ndiversity, and scale. The intricacy of C2C recommendation systems is further\naccentuated by the dual roles users assume as both sellers and buyers,\nintroducing a spectrum of less uniform and varied inputs. Addressing this, we\nintroduce MerRec, the first large-scale dataset specifically for C2C\nrecommendations, sourced from the Mercari e-commerce platform, covering\nmillions of users and products over 6 months in 2023. MerRec not only includes\nstandard features such as user_id, item_id, and session_id, but also unique\nelements like timestamped action types, product taxonomy, and textual product\nattributes, offering a comprehensive dataset for research. This dataset,\nextensively evaluated across four recommendation tasks, establishes a new\nbenchmark for the development of advanced recommendation algorithms in\nreal-world scenarios, bridging the gap between academia and industry and\npropelling the study of C2C recommendations. Our experiment code is available\nat https://github.com/mercari/mercari-ml-merrec-pub-us and dataset at\nhttps://huggingface.co/datasets/mercari-us/merrec.\n","authors":["Lichi Li","Zainul Abi Din","Zhen Tan","Sam London","Tianlong Chen","Ajay Daptardar"],"pdf_url":"https://arxiv.org/pdf/2402.14230v2.pdf","comment":"Dataset available at:\n  https://huggingface.co/datasets/mercari-us/merrec, code available at:\n  https://github.com/mercari/mercari-ml-merrec-pub-us"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.12899v1","updated":"2024-07-17T17:54:12Z","published":"2024-07-17T17:54:12Z","title":"DreamStory: Open-Domain Story Visualization by LLM-Guided Multi-Subject\n  Consistent Diffusion","summary":"  Story visualization aims to create visually compelling images or videos\ncorresponding to textual narratives. Despite recent advances in diffusion\nmodels yielding promising results, existing methods still struggle to create a\ncoherent sequence of subject-consistent frames based solely on a story. To this\nend, we propose DreamStory, an automatic open-domain story visualization\nframework by leveraging the LLMs and a novel multi-subject consistent diffusion\nmodel. DreamStory consists of (1) an LLM acting as a story director and (2) an\ninnovative Multi-Subject consistent Diffusion model (MSD) for generating\nconsistent multi-subject across the images. First, DreamStory employs the LLM\nto generate descriptive prompts for subjects and scenes aligned with the story,\nannotating each scene's subjects for subsequent subject-consistent generation.\nSecond, DreamStory utilizes these detailed subject descriptions to create\nportraits of the subjects, with these portraits and their corresponding textual\ninformation serving as multimodal anchors (guidance). Finally, the MSD uses\nthese multimodal anchors to generate story scenes with consistent\nmulti-subject. Specifically, the MSD includes Masked Mutual Self-Attention\n(MMSA) and Masked Mutual Cross-Attention (MMCA) modules. MMSA and MMCA modules\nensure appearance and semantic consistency with reference images and text,\nrespectively. Both modules employ masking mechanisms to prevent subject\nblending. To validate our approach and promote progress in story visualization,\nwe established a benchmark, DS-500, which can assess the overall performance of\nthe story visualization framework, subject-identification accuracy, and the\nconsistency of the generation model. Extensive experiments validate the\neffectiveness of DreamStory in both subjective and objective evaluations.\nPlease visit our project homepage at https://dream-xyz.github.io/dreamstory.\n","authors":["Huiguo He","Huan Yang","Zixi Tuo","Yuan Zhou","Qiuyue Wang","Yuhang Zhang","Zeyu Liu","Wenhao Huang","Hongyang Chao","Jian Yin"],"pdf_url":"https://arxiv.org/pdf/2407.12899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15597v4","updated":"2024-07-17T16:01:00Z","published":"2022-11-28T17:50:19Z","title":"Lightning Fast Video Anomaly Detection via Adversarial Knowledge\n  Distillation","summary":"  We propose a very fast frame-level model for anomaly detection in video,\nwhich learns to detect anomalies by distilling knowledge from multiple highly\naccurate object-level teacher models. To improve the fidelity of our student,\nwe distill the low-resolution anomaly maps of the teachers by jointly applying\nstandard and adversarial distillation, introducing an adversarial discriminator\nfor each teacher to distinguish between target and generated anomaly maps. We\nconduct experiments on three benchmarks (Avenue, ShanghaiTech, UCSD Ped2),\nshowing that our method is over 7 times faster than the fastest competing\nmethod, and between 28 and 62 times faster than object-centric models, while\nobtaining comparable results to recent methods. Our evaluation also indicates\nthat our model achieves the best trade-off between speed and accuracy, due to\nits previously unheard-of speed of 1480 FPS. In addition, we carry out a\ncomprehensive ablation study to justify our architectural design choices. Our\ncode is freely available at: https://github.com/ristea/fast-aed.\n","authors":["Florinel-Alin Croitoru","Nicolae-Catalin Ristea","Dana Dascalescu","Radu Tudor Ionescu","Fahad Shahbaz Khan","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2211.15597v4.pdf","comment":"Accepted in Computer Vision and Image Understanding"},{"id":"http://arxiv.org/abs/2407.11650v2","updated":"2024-07-17T11:41:59Z","published":"2024-07-16T12:15:41Z","title":"Statistics-aware Audio-visual Deepfake Detector","summary":"  In this paper, we propose an enhanced audio-visual deep detection method.\nRecent methods in audio-visual deepfake detection mostly assess the\nsynchronization between audio and visual features. Although they have shown\npromising results, they are based on the maximization/minimization of isolated\nfeature distances without considering feature statistics. Moreover, they rely\non cumbersome deep learning architectures and are heavily dependent on\nempirically fixed hyperparameters. Herein, to overcome these limitations, we\npropose: (1) a statistical feature loss to enhance the discrimination\ncapability of the model, instead of relying solely on feature distances; (2)\nusing the waveform for describing the audio as a replacement of frequency-based\nrepresentations; (3) a post-processing normalization of the fakeness score; (4)\nthe use of shallower network for reducing the computational complexity.\nExperiments on the DFDC and FakeAVCeleb datasets demonstrate the relevance of\nthe proposed method.\n","authors":["Marcella Astrid","Enjie Ghorbel","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2407.11650v2.pdf","comment":"Accepted in ICIP 2024"},{"id":"http://arxiv.org/abs/2407.12465v1","updated":"2024-07-17T10:34:49Z","published":"2024-07-17T10:34:49Z","title":"Enhancing Film Grain Coding in VVC: Improving Encoding Quality and\n  Efficiency","summary":"  This paper presents an in-depth analysis of film grain handling in\nopen-source implementations of the Versatile Video Coding (VVC) standard. We\nfocus on two key components: the Film Grain Analysis (FGA) module implemented\nin VVenC and the Film Grain Synthesis (FGS) module implemented in VVdeC. We\ndescribe the methodologies used to implement these modules and discuss the\ngeneration of Supplementary Enhancement Information (SEI) parameters to signal\nfilm grain characteristics in the encoded video sequences. Additionally, we\nconduct subjective and objective evaluations across Full HD videos to assess\nthe effectiveness of film grain handling. Our results demonstrate the\ncapability of the FGA and FGS techniques to accurately analyze and synthesize\nfilm grain, thereby improving the visual quality of encoded video content.\nOverall, our study contributes to advancing the understanding and\nimplementation of film grain handling techniques in VVC open-source\nimplementations, with implications for enhancing the viewing experience in\nmultimedia applications.\n","authors":["Vignesh V Menon","Adam Wieckowski","Christian Stoffers","Jens Brandenburg","Christian Lehmann","Benjamin Bross","Thomas Schierl","Detlev Marpe"],"pdf_url":"https://arxiv.org/pdf/2407.12465v1.pdf","comment":"Accepted at IBC'24"},{"id":"http://arxiv.org/abs/2308.03024v2","updated":"2024-07-17T09:53:23Z","published":"2023-08-06T05:23:25Z","title":"Show Me the World in My Language: Establishing the First Baseline for\n  Scene-Text to Scene-Text Translation","summary":"  In this work, we study the task of visually translating scene text from a\nsource language (e.g., Hindi) to a target language (e.g., English). Visual\ntranslation involves not just the recognition and translation of scene text but\nalso the generation of the translated image that preserves visual features of\nthe source scene text, such as font, size, and background. There are several\nchallenges associated with this task, such as translation with limited context,\ndeciding between translation and transliteration, accommodating varying text\nlengths within fixed spatial boundaries, and preserving the font and background\nstyles of the source scene text in the target language. To address this\nproblem, we make the following contributions: (i) We study visual translation\nas a standalone problem for the first time in the literature. (ii) We present a\ncascaded framework for visual translation that combines state-of-the-art\nmodules for scene text recognition, machine translation, and scene text\nsynthesis as a baseline for the task. (iii) We propose a set of task-specific\ndesign enhancements to design a variant of the baseline to obtain performance\nimprovements. (iv) Currently, the existing related literature lacks any\ncomprehensive performance evaluation for this novel task. To fill this gap, we\nintroduce several automatic and user-assisted evaluation metrics designed\nexplicitly for evaluating visual translation. Further, we evaluate presented\nbaselines for translating scene text between Hindi and English. Our experiments\ndemonstrate that although we can effectively perform visual translation over a\nlarge collection of scene text images, the presented baseline only partially\naddresses challenges posed by visual translation tasks. We firmly believe that\nthis new task and the limitations of existing models, as reported in this\npaper, should encourage further research in visual translation.\n","authors":["Shreyas Vaidya","Arvind Kumar Sharma","Prajwal Gatti","Anand Mishra"],"pdf_url":"https://arxiv.org/pdf/2308.03024v2.pdf","comment":"Please be advised that the previous version contained some technical\n  issues. Kindly refer to this updated version for accurate information"},{"id":"http://arxiv.org/abs/2407.12341v1","updated":"2024-07-17T06:33:51Z","published":"2024-07-17T06:33:51Z","title":"LLM-based query paraphrasing for video search","summary":"  Text-to-video retrieval answers user queries through search by concepts and\nembeddings. Limited by the size of the concept bank and the amount of training\ndata, answering queries in the wild is not always effective due to the\nout-of-vocabulary problem. Furthermore, neither concept-based nor\nembedding-based search can perform reasoning to consolidate the search results\nfor complex queries mixed with logical and spatial constraints. To address\nthese problems, we leverage large language models (LLM) to paraphrase the query\nby text-to-text (T2T), text-to-image (T2I), and image-to-text (I2T)\ntransformations. These transformations rephrase abstract concepts into simple\nwords to address the out-of-vocabulary problem. Furthermore, the complex\nrelationship in a query can be decoupled into simpler sub-queries, yielding\nbetter retrieval performance when fusing the search results of these\nsub-queries. To address the LLM hallucination problem, this paper also proposes\na novel consistency-based verification strategy to filter the paraphrased\nqueries that are factually incorrect. Extensive experiments are conducted for\nad-hoc video search and known-item search on the TRECVid datasets. We provide\nempirical insights into how traditionally difficult-to-answer queries can be\nresolved by query paraphrasing.\n","authors":["Jiaxin Wu","Chong-Wah Ngo","Wing-Kwong Chan","Sheng-Hua Zhong"],"pdf_url":"https://arxiv.org/pdf/2407.12341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12751v3","updated":"2024-07-17T03:34:39Z","published":"2023-11-21T17:52:30Z","title":"Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with\n  Spatial Relation Matching","summary":"  Navigating drones through natural language commands remains challenging due\nto the dearth of accessible multi-modal datasets and the stringent precision\nrequirements for aligning visual and textual data. To address this pressing\nneed, we introduce GeoText-1652, a new natural language-guided geo-localization\nbenchmark. This dataset is systematically constructed through an interactive\nhuman-computer process leveraging Large Language Model (LLM) driven annotation\ntechniques in conjunction with pre-trained vision models. GeoText-1652 extends\nthe established University-1652 image dataset with spatial-aware text\nannotations, thereby establishing one-to-one correspondences between image,\ntext, and bounding box elements. We further introduce a new optimization\nobjective to leverage fine-grained spatial associations, called blending\nspatial matching, for region-level spatial relation matching. Extensive\nexperiments reveal that our approach maintains a competitive recall rate\ncomparing other prevailing cross-modality methods. This underscores the\npromising potential of our approach in elevating drone control and navigation\nthrough the seamless integration of natural language commands in real-world\nscenarios.\n","authors":["Meng Chu","Zhedong Zheng","Wei Ji","Tingyu Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2311.12751v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2311.17072v2","updated":"2024-07-17T00:10:24Z","published":"2023-11-27T19:00:06Z","title":"IG Captioner: Information Gain Captioners are Strong Zero-shot\n  Classifiers","summary":"  Generative training has been demonstrated to be powerful for building\nvisual-language models. However, on zero-shot discriminative benchmarks, there\nis still a performance gap between models trained with generative and\ndiscriminative objectives. In this paper, we aim to narrow this gap by\nimproving the efficacy of generative training on classification tasks, without\nany finetuning processes or additional modules.\n  Specifically, we focus on narrowing the gap between the generative captioner\nand the CLIP classifier. We begin by analysing the predictions made by the\ncaptioner and classifier and observe that the caption generation inherits the\ndistribution bias from the language model trained with pure text modality,\nmaking it less grounded on the visual signal. To tackle this problem, we\nredesign the scoring objective for the captioner to alleviate the\ndistributional bias and focus on measuring the gain of information brought by\nthe visual inputs. We further design a generative training objective to match\nthe evaluation objective. We name our model trained and evaluated from the\nnovel procedures as Information Gain (IG) captioner. We pretrain the models on\nthe public Laion-5B dataset and perform a series of discriminative evaluations.\nFor the zero-shot classification on ImageNet, IG captioner achieves $> 18\\%$\nimprovements over the standard captioner, achieving comparable performances\nwith the CLIP classifier. IG captioner also demonstrated strong performance on\nzero-shot image-text retrieval tasks on MSCOCO and Flickr30K. We hope this\npaper inspires further research towards unifying generative and discriminative\ntraining procedures for visual-language models.\n","authors":["Chenglin Yang","Siyuan Qiao","Yuan Cao","Yu Zhang","Tao Zhu","Alan Yuille","Jiahui Yu"],"pdf_url":"https://arxiv.org/pdf/2311.17072v2.pdf","comment":"To appear in ECCV 2024"}]},"2024-07-16T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.12216v1","updated":"2024-07-16T23:50:07Z","published":"2024-07-16T23:50:07Z","title":"Mindful-RAG: A Study of Points of Failure in Retrieval Augmented\n  Generation","summary":"  Large Language Models (LLMs) are proficient at generating coherent and\ncontextually relevant text but face challenges when addressing\nknowledge-intensive queries in domain-specific and factual question-answering\ntasks. Retrieval-augmented generation (RAG) systems mitigate this by\nincorporating external knowledge sources, such as structured knowledge graphs\n(KGs). However, LLMs often struggle to produce accurate answers despite access\nto KG-extracted information containing necessary facts. Our study investigates\nthis dilemma by analyzing error patterns in existing KG-based RAG methods and\nidentifying eight critical failure points. We observed that these errors\npredominantly occur due to insufficient focus on discerning the question's\nintent and adequately gathering relevant context from the knowledge graph\nfacts. Drawing on this analysis, we propose the Mindful-RAG approach, a\nframework designed for intent-based and contextually aligned knowledge\nretrieval. This method explicitly targets the identified failures and offers\nimprovements in the correctness and relevance of responses provided by LLMs,\nrepresenting a significant step forward from existing methods.\n","authors":["Garima Agrawal","Tharindu Kumarage","Zeyad Alghamdi","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2407.12216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12193v1","updated":"2024-07-16T21:38:45Z","published":"2024-07-16T21:38:45Z","title":"ClaimCompare: A Data Pipeline for Evaluation of Novelty Destroying\n  Patent Pairs","summary":"  A fundamental step in the patent application process is the determination of\nwhether there exist prior patents that are novelty destroying. This step is\nroutinely performed by both applicants and examiners, in order to assess the\nnovelty of proposed inventions among the millions of applications filed\nannually. However, conducting this search is time and labor-intensive, as\nsearchers must navigate complex legal and technical jargon while covering a\nlarge amount of legal claims. Automated approaches using information retrieval\nand machine learning approaches to detect novelty destroying patents present a\npromising avenue to streamline this process, yet research focusing on this\nspace remains limited. In this paper, we introduce a novel data pipeline,\nClaimCompare, designed to generate labeled patent claim datasets suitable for\ntraining IR and ML models to address this challenge of novelty destruction\nassessment. To the best of our knowledge, ClaimCompare is the first pipeline\nthat can generate multiple novelty destroying patent datasets. To illustrate\nthe practical relevance of this pipeline, we utilize it to construct a sample\ndataset comprising of over 27K patents in the electrochemical domain: 1,045\nbase patents from USPTO, each associated with 25 related patents labeled\naccording to their novelty destruction towards the base patent. Subsequently,\nwe conduct preliminary experiments showcasing the efficacy of this dataset in\nfine-tuning transformer models to identify novelty destroying patents,\ndemonstrating 29.2% and 32.7% absolute improvement in MRR and P@1,\nrespectively.\n","authors":["Arav Parikh","Shiri Dori-Hacohen"],"pdf_url":"https://arxiv.org/pdf/2407.12193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12170v1","updated":"2024-07-16T20:47:54Z","published":"2024-07-16T20:47:54Z","title":"Neural Passage Quality Estimation for Static Pruning","summary":"  Neural networks -- especially those that use large, pre-trained language\nmodels -- have improved search engines in various ways. Most prominently, they\ncan estimate the relevance of a passage or document to a user's query. In this\nwork, we depart from this direction by exploring whether neural networks can\neffectively predict which of a document's passages are unlikely to be relevant\nto any query submitted to the search engine. We refer to this query-agnostic\nestimation of passage relevance as a passage's quality. We find that our novel\nmethods for estimating passage quality allow passage corpora to be pruned\nconsiderably while maintaining statistically equivalent effectiveness; our best\nmethods can consistently prune >25% of passages in a corpora, across various\nretrieval pipelines. Such substantial pruning reduces the operating costs of\nneural search engines in terms of computing resources, power usage, and carbon\nfootprint -- both when processing queries (thanks to a smaller index size) and\nwhen indexing (lightweight models can prune low-quality passages prior to the\ncostly dense or learned sparse encoding step). This work sets the stage for\ndeveloping more advanced neural \"learning-what-to-index\" methods.\n","authors":["Xuejun Chang","Debabrata Mishra","Craig Macdonald","Sean MacAvaney"],"pdf_url":"https://arxiv.org/pdf/2407.12170v1.pdf","comment":"SIGIR 2024"},{"id":"http://arxiv.org/abs/2407.09653v2","updated":"2024-07-16T19:34:40Z","published":"2024-07-12T19:22:17Z","title":"Bridging the Gap Between Information Seeking and Product Search Systems:\n  Q&A Recommendation for E-commerce","summary":"  Consumers on a shopping mission often leverage both product search and\ninformation seeking systems, such as web search engines and Question Answering\n(QA) systems, in an iterative process to improve their understanding of\navailable products and reach a purchase decision. While product search is\nuseful for shoppers to find the actual products meeting their requirements in\nthe catalog, information seeking systems can be utilized to answer any\nquestions they may have to refine those requirements. The recent success of\nLarge Language Models (LLMs) has opened up an opportunity to bridge the gap\nbetween the two tasks to help customers achieve their goals quickly and\neffectively by integrating conversational QA within product search. In this\npaper, we propose to recommend users Question-Answer (Q&A) pairs that are\nrelevant to their product search and can help them make a purchase decision. We\ndiscuss the different aspects of the problem including the requirements and\ncharacteristics of the Q&A pairs, their generation, and the optimization of the\nQ&A recommendation task. We highlight the challenges, open problems, and\nsuggested solutions to encourage future research in this emerging area.\n","authors":["Saar Kuzi","Shervin Malmasi"],"pdf_url":"https://arxiv.org/pdf/2407.09653v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00247v2","updated":"2024-07-16T18:01:55Z","published":"2024-06-01T00:52:41Z","title":"Large Language Models for Relevance Judgment in Product Search","summary":"  High relevance of retrieved and re-ranked items to the search query is the\ncornerstone of successful product search, yet measuring relevance of items to\nqueries is one of the most challenging tasks in product information retrieval,\nand quality of product search is highly influenced by the precision and scale\nof available relevance-labelled data. In this paper, we present an array of\ntechniques for leveraging Large Language Models (LLMs) for automating the\nrelevance judgment of query-item pairs (QIPs) at scale. Using a unique dataset\nof multi-million QIPs, annotated by human evaluators, we test and optimize\nhyper parameters for finetuning billion-parameter LLMs with and without Low\nRank Adaption (LoRA), as well as various modes of item attribute concatenation\nand prompting in LLM finetuning, and consider trade offs in item attribute\ninclusion for quality of relevance predictions. We demonstrate considerable\nimprovement over baselines of prior generations of LLMs, as well as\noff-the-shelf models, towards relevance annotations on par with the human\nrelevance evaluators. Our findings have immediate implications for the growing\nfield of relevance judgment automation in product search.\n","authors":["Navid Mehrdad","Hrushikesh Mohapatra","Mossaab Bagdouri","Prijith Chandran","Alessandro Magnani","Xunfan Cai","Ajit Puthenputhussery","Sachin Yadav","Tony Lee","ChengXiang Zhai","Ciya Liao"],"pdf_url":"https://arxiv.org/pdf/2406.00247v2.pdf","comment":"10 pages, 1 figure, 11 tables - SIGIR 2024, LLM4Eval"},{"id":"http://arxiv.org/abs/2407.12883v1","updated":"2024-07-16T17:58:27Z","published":"2024-07-16T17:58:27Z","title":"BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive\n  Retrieval","summary":"  Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398\nreal-world queries collected from diverse domains (such as economics,\npsychology, robotics, software engineering, earth sciences, etc.), sourced from\nnaturally occurring or carefully curated human data. Extensive evaluation\nreveals that even state-of-the-art retrieval models perform poorly on BRIGHT.\nThe leading model on the MTEB leaderboard [38 ], which achieves a score of 59.0\nnDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate\nthat augmenting queries with Chain-of-Thought reasoning generated by large\nlanguage models (LLMs) improves performance by up to 12.2 points. Moreover,\nBRIGHT is robust against data leakage during pretraining of the benchmarked\nmodels as we validate by showing similar performance even when documents from\nthe benchmark are included in the training data. We believe that BRIGHT paves\nthe way for future research on retrieval systems in more realistic and\nchallenging settings. Our code and data are available at\nhttps://brightbenchmark.github.io.\n","authors":["Hongjin Su","Howard Yen","Mengzhou Xia","Weijia Shi","Niklas Muennighoff","Han-yu Wang","Haisu Liu","Quan Shi","Zachary S. Siegel","Michael Tang","Ruoxi Sun","Jinsung Yoon","Sercan O. Arik","Danqi Chen","Tao Yu"],"pdf_url":"https://arxiv.org/pdf/2407.12883v1.pdf","comment":"50 pages"},{"id":"http://arxiv.org/abs/2405.11612v2","updated":"2024-07-16T15:47:13Z","published":"2024-05-19T17:04:39Z","title":"Sociotechnical Implications of Generative Artificial Intelligence for\n  Information Access","summary":"  Robust access to trustworthy information is a critical need for society with\nimplications for knowledge production, public health education, and promoting\ninformed citizenry in democratic societies. Generative AI technologies may\nenable new ways to access information and improve effectiveness of existing\ninformation retrieval systems but we are only starting to understand and\ngrapple with their long-term social implications. In this chapter, we present\nan overview of some of the systemic consequences and risks of employing\ngenerative AI in the context of information access. We also provide\nrecommendations for evaluation and mitigation, and discuss challenges for\nfuture research.\n","authors":["Bhaskar Mitra","Henriette Cramer","Olya Gurevich"],"pdf_url":"https://arxiv.org/pdf/2405.11612v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18470v3","updated":"2024-07-16T15:20:16Z","published":"2024-06-26T16:28:24Z","title":"UniRec: A Dual Enhancement of Uniformity and Frequency in Sequential\n  Recommendations","summary":"  Representation learning in sequential recommendation is critical for\naccurately modeling user interaction patterns and improving recommendation\nprecision. However, existing approaches predominantly emphasize item-to-item\ntransitions, often neglecting the time intervals between interactions, which\nare closely related to behavior pattern changes. Additionally, broader\ninteraction attributes, such as item frequency, are frequently overlooked. We\nfound that both sequences with more uniform time intervals and items with\nhigher frequency yield better prediction performance. Conversely, non-uniform\nsequences exacerbate user interest drift and less-frequent items are difficult\nto model due to sparse sampling, presenting unique challenges inadequately\naddressed by current methods. In this paper, we propose UniRec, a novel\nbidirectional enhancement sequential recommendation method. UniRec leverages\nsequence uniformity and item frequency to enhance performance, particularly\nimproving the representation of non-uniform sequences and less-frequent items.\nThese two branches mutually reinforce each other, driving comprehensive\nperformance optimization in complex sequential recommendation scenarios.\nAdditionally, we present a multidimensional time module to further enhance\nadaptability. To the best of our knowledge, UniRec is the first method to\nutilize the characteristics of uniformity and frequency for feature\naugmentation. Comparing with eleven advanced models across four datasets, we\ndemonstrate that UniRec outperforms SOTA models significantly. The code is\navailable at https://github.com/Linxi000/UniRec.\n","authors":["Yang Liu","Yitong Wang","Chenyue Feng"],"pdf_url":"https://arxiv.org/pdf/2406.18470v3.pdf","comment":"15 pages, 8 figures, accepted by CIKM'24, for source code, see\n  https://github.com/Linxi000/UniRec"},{"id":"http://arxiv.org/abs/2311.00388v2","updated":"2024-07-16T14:33:24Z","published":"2023-11-01T09:25:21Z","title":"AutoSAM: Towards Automatic Sampling of User Behaviors for Sequential\n  Recommender Systems","summary":"  Sequential recommender systems (SRS) have gained widespread popularity in\nrecommendation due to their ability to effectively capture dynamic user\npreferences. One default setting in the current SRS is to uniformly consider\neach historical behavior as a positive interaction. Actually, this setting has\nthe potential to yield sub-optimal performance, as each item makes a distinct\ncontribution to the user's interest. For example, purchased items should be\ngiven more importance than clicked ones. Hence, we propose a general automatic\nsampling framework, named AutoSAM, to non-uniformly treat historical behaviors.\nSpecifically, AutoSAM augments the standard sequential recommendation\narchitecture with an additional sampler layer to adaptively learn the skew\ndistribution of the raw input, and then sample informative sub-sets to build\nmore generalizable SRS. To overcome the challenges of non-differentiable\nsampling actions and also introduce multiple decision factors for sampling, we\nfurther introduce a novel reinforcement learning based method to guide the\ntraining of the sampler. We theoretically design multi-objective sampling\nrewards including Future Prediction and Sequence Perplexity, and then optimize\nthe whole framework in an end-to-end manner by combining the policy gradient.\nWe conduct extensive experiments on benchmark recommender models and four\nreal-world datasets. The experimental results demonstrate the effectiveness of\nthe proposed approach. We will make our code publicly available after the\nacceptance.\n","authors":["Hao Zhang","Mingyue Cheng","Qi Liu","Zhiding Liu","Junzhe Jiang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2311.00388v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11638v1","updated":"2024-07-16T11:58:54Z","published":"2024-07-16T11:58:54Z","title":"A Comprehensive Evaluation of Large Language Models on Temporal Event\n  Forecasting","summary":"  Recently, Large Language Models (LLMs) have demonstrated great potential in\nvarious data mining tasks, such as knowledge question answering, mathematical\nreasoning, and commonsense reasoning. However, the reasoning capability of LLMs\non temporal event forecasting has been under-explored. To systematically\ninvestigate their abilities in temporal event forecasting, we conduct a\ncomprehensive evaluation of LLM-based methods for temporal event forecasting.\nDue to the lack of a high-quality dataset that involves both graph and textual\ndata, we first construct a benchmark dataset, named MidEast-TE-mini. Based on\nthis dataset, we design a series of baseline methods, characterized by various\ninput formats and retrieval augmented generation(RAG) modules. From extensive\nexperiments, we find that directly integrating raw texts into the input of LLMs\ndoes not enhance zero-shot extrapolation performance. In contrast,\nincorporating raw texts in specific complex events and fine-tuning LLMs\nsignificantly improves performance. Moreover, enhanced with retrieval modules,\nLLM can effectively capture temporal relational patterns hidden in historical\nevents. Meanwhile, issues such as popularity bias and the long-tail problem\nstill persist in LLMs, particularly in the RAG-based method. These findings not\nonly deepen our understanding of LLM-based event forecasting methods but also\nhighlight several promising research directions.We consider that this\ncomprehensive evaluation, along with the identified research opportunities,\nwill significantly contribute to future research on temporal event forecasting\nthrough LLMs.\n","authors":["He Chang","Chenchen Ye","Zhulin Tao","Jie Wu","Zhengmao Yang","Yunshan Ma","Xianglin Huang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2407.11638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11605v1","updated":"2024-07-16T11:12:22Z","published":"2024-07-16T11:12:22Z","title":"Interactions with Generative Information Retrieval Systems","summary":"  At its core, information access and seeking is an interactive process. In\nexisting search engines, interactions are limited to a few pre-defined actions,\nsuch as \"requery\", \"click on a document\", \"scrolling up/down\", \"going to the\nnext result page\", \"leaving the search engine\", etc. A major benefit of moving\ntowards generative IR systems is enabling users with a richer expression of\ninformation need and feedback and free-form interactions in natural language\nand beyond. In other words, the actions users take are no longer limited by the\nclickable links and buttons available on the search engine result page and\nusers can express themselves freely through natural language. This can go even\nbeyond natural language, through images, videos, gestures, and sensors using\nmulti-modal generative IR systems. This chapter briefly discusses the role of\ninteraction in generative IR systems. We will first discuss different ways\nusers can express their information needs by interacting with generative IR\nsystems. We then explain how users can provide explicit or implicit feedback to\ngenerative IR systems and how they can consume such feedback. Next, we will\ncover how users interactively can refine retrieval results. We will expand upon\nmixed-initiative interactions and discuss clarification and preference\nelicitation in more detail. We then discuss proactive generative IR systems,\nincluding context-aware recommendation, following up past conversations,\ncontributing to multi-party conversations, and feedback requests. Providing\nexplanation is another interaction type that we briefly discuss in this\nchapter. We will also briefly describe multi-modal interactions in generative\ninformation retrieval. Finally, we describe emerging frameworks and solutions\nfor user interfaces with generative AI systems.\n","authors":["Mohammad Aliannejadi","Jacek Gwizdka","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2407.11605v1.pdf","comment":"Draft of a chapter intended to appear in a forthcoming book on\n  generative information retrieval, co-edited by Chirag Shah and Ryen White"},{"id":"http://arxiv.org/abs/2407.11548v1","updated":"2024-07-16T09:52:42Z","published":"2024-07-16T09:52:42Z","title":"A PLMs based protein retrieval framework","summary":"  Protein retrieval, which targets the deconstruction of the relationship\nbetween sequences, structures and functions, empowers the advancing of biology.\nBasic Local Alignment Search Tool (BLAST), a sequence-similarity-based\nalgorithm, has proved the efficiency of this field. Despite the existing tools\nfor protein retrieval, they prioritize sequence similarity and probably\noverlook proteins that are dissimilar but share homology or functionality. In\norder to tackle this problem, we propose a novel protein retrieval framework\nthat mitigates the bias towards sequence similarity. Our framework initiatively\nharnesses protein language models (PLMs) to embed protein sequences within a\nhigh-dimensional feature space, thereby enhancing the representation capacity\nfor subsequent analysis. Subsequently, an accelerated indexed vector database\nis constructed to facilitate expedited access and retrieval of dense vectors.\nExtensive experiments demonstrate that our framework can equally retrieve both\nsimilar and dissimilar proteins. Moreover, this approach enables the\nidentification of proteins that conventional methods fail to uncover. This\nframework will effectively assist in protein mining and empower the development\nof biology.\n","authors":["Yuxuan Wu","Xiao Yi","Yang Tan","Huiqun Yu","Guisheng Fan"],"pdf_url":"https://arxiv.org/pdf/2407.11548v1.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2401.11742v3","updated":"2024-07-16T08:44:56Z","published":"2024-01-22T08:00:49Z","title":"SciConNav: Knowledge navigation through contextual learning of extensive\n  scientific research trajectories","summary":"  New knowledge builds upon existing foundations, which means an interdependent\nrelationship exists between knowledge, manifested in the historical development\nof the scientific system for hundreds of years. By leveraging natural language\nprocessing techniques, this study introduces the Scientific Concept Navigator\n(SciConNav), an embedding-based navigation model to infer the \"knowledge\npathway\" from the research trajectories of millions of scholars. We validate\nthat the learned representations effectively delineate disciplinary boundaries\nand capture the intricate relationships between diverse concepts. The utility\nof the inferred navigation space is showcased through multiple applications.\nFirstly, we demonstrated the multi-step analogy inferences within the knowledge\nspace and the interconnectivity between concepts in different disciplines.\nSecondly, we formulated the attribute dimensions of knowledge across domains,\nobserving the distributional shifts in the arrangement of 19 disciplines along\nthese conceptual dimensions, including \"Theoretical\" to \"Applied\", and\n\"Chemical\" to \"Biomedical', highlighting the evolution of functional attributes\nwithin knowledge domains. Lastly, by analyzing the high-dimensional knowledge\nnetwork structure, we found that knowledge connects with shorter global\npathways, and interdisciplinary knowledge plays a critical role in the\naccessibility of the global knowledge network. Our framework offers a novel\napproach to mining knowledge inheritance pathways in extensive scientific\nliterature, which is of great significance for understanding scientific\nprogression patterns, tailoring scientific learning trajectories, and\naccelerating scientific progress.\n","authors":["Shibing Xiang","Xin Jiang","Bing Liu","Yurui Huang","Chaolin Tian","Yifang Ma"],"pdf_url":"https://arxiv.org/pdf/2401.11742v3.pdf","comment":"21pages, 13 figures, 6 tables"},{"id":"http://arxiv.org/abs/2407.11504v1","updated":"2024-07-16T08:42:36Z","published":"2024-07-16T08:42:36Z","title":"Bootstrapped Pre-training with Dynamic Identifier Prediction for\n  Generative Retrieval","summary":"  Generative retrieval uses differentiable search indexes to directly generate\nrelevant document identifiers in response to a query. Recent studies have\nhighlighted the potential of a strong generative retrieval model, trained with\ncarefully crafted pre-training tasks, to enhance downstream retrieval tasks via\nfine-tuning. However, the full power of pre-training for generative retrieval\nremains underexploited due to its reliance on pre-defined static document\nidentifiers, which may not align with evolving model parameters. In this work,\nwe introduce BootRet, a bootstrapped pre-training method for generative\nretrieval that dynamically adjusts document identifiers during pre-training to\naccommodate the continuing memorization of the corpus. BootRet involves three\nkey training phases: (i) initial identifier generation, (ii) pre-training via\ncorpus indexing and relevance prediction tasks, and (iii) bootstrapping for\nidentifier updates. To facilitate the pre-training phase, we further introduce\nnoisy documents and pseudo-queries, generated by large language models, to\nresemble semantic connections in both indexing and retrieval tasks.\nExperimental results demonstrate that BootRet significantly outperforms\nexisting pre-training generative retrieval baselines and performs well even in\nzero-shot settings.\n","authors":["Yubao Tang","Ruqing Zhang","Jiafeng Guo","Maarten de Rijke","Yixing Fan","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2407.11504v1.pdf","comment":"Accepted by ACL Findings 2024"},{"id":"http://arxiv.org/abs/2407.11401v1","updated":"2024-07-16T05:40:17Z","published":"2024-07-16T05:40:17Z","title":"EndoFinder: Online Image Retrieval for Explainable Colorectal Polyp\n  Diagnosis","summary":"  Determining the necessity of resecting malignant polyps during colonoscopy\nscreen is crucial for patient outcomes, yet challenging due to the\ntime-consuming and costly nature of histopathology examination. While deep\nlearning-based classification models have shown promise in achieving optical\nbiopsy with endoscopic images, they often suffer from a lack of explainability.\nTo overcome this limitation, we introduce EndoFinder, a content-based image\nretrieval framework to find the 'digital twin' polyp in the reference database\ngiven a newly detected polyp. The clinical semantics of the new polyp can be\ninferred referring to the matched ones. EndoFinder pioneers a polyp-aware image\nencoder that is pre-trained on a large polyp dataset in a self-supervised way,\nmerging masked image modeling with contrastive learning. This results in a\ngeneric embedding space ready for different downstream clinical tasks based on\nimage retrieval. We validate the framework on polyp re-identification and\noptical biopsy tasks, with extensive experiments demonstrating that EndoFinder\nnot only achieves explainable diagnostics but also matches the performance of\nsupervised classification models. EndoFinder's reliance on image retrieval has\nthe potential to support diverse downstream decision-making tasks during\nreal-time colonoscopy procedures.\n","authors":["Ruijie Yang","Yan Zhu","Peiyao Fu","Yizhe Zhang","Zhihua Wang","Quanlin Li","Pinghong Zhou","Xian Yang","Shuo Wang"],"pdf_url":"https://arxiv.org/pdf/2407.11401v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2303.11916v4","updated":"2024-07-16T04:23:37Z","published":"2023-03-21T15:06:35Z","title":"CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion","summary":"  This paper proposes a novel diffusion-based model, CompoDiff, for solving\nzero-shot Composed Image Retrieval (ZS-CIR) with latent diffusion. This paper\nalso introduces a new synthetic dataset, named SynthTriplets18M, with 18.8\nmillion reference images, conditions, and corresponding target image triplets\nto train CIR models. CompoDiff and SynthTriplets18M tackle the shortages of the\nprevious CIR approaches, such as poor generalizability due to the small dataset\nscale and the limited types of conditions. CompoDiff not only achieves a new\nstate-of-the-art on four ZS-CIR benchmarks, including FashionIQ, CIRR, CIRCO,\nand GeneCIS, but also enables a more versatile and controllable CIR by\naccepting various conditions, such as negative text, and image mask conditions.\nCompoDiff also shows the controllability of the condition strength between text\nand image queries and the trade-off between inference speed and performance,\nwhich are unavailable with existing CIR methods. The code and dataset are\navailable at https://github.com/navervision/CompoDiff\n","authors":["Geonmo Gu","Sanghyuk Chun","Wonjae Kim","HeeJae Jun","Yoohoon Kang","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2303.11916v4.pdf","comment":"TMLR camera-ready; First two authors contributed equally; TMLR Expert\n  Certification; 30 pages, 5.9MB"}],"Multimedia":[{"id":"http://arxiv.org/abs/2312.03641v2","updated":"2024-07-16T17:27:10Z","published":"2023-12-06T17:49:57Z","title":"MotionCtrl: A Unified and Flexible Motion Controller for Video\n  Generation","summary":"  Motions in a video primarily consist of camera motion, induced by camera\nmovement, and object motion, resulting from object movement. Accurate control\nof both camera and object motion is essential for video generation. However,\nexisting works either mainly focus on one type of motion or do not clearly\ndistinguish between the two, limiting their control capabilities and diversity.\nTherefore, this paper presents MotionCtrl, a unified and flexible motion\ncontroller for video generation designed to effectively and independently\ncontrol camera and object motion. The architecture and training strategy of\nMotionCtrl are carefully devised, taking into account the inherent properties\nof camera motion, object motion, and imperfect training data. Compared to\nprevious methods, MotionCtrl offers three main advantages: 1) It effectively\nand independently controls camera motion and object motion, enabling more\nfine-grained motion control and facilitating flexible and diverse combinations\nof both types of motion. 2) Its motion conditions are determined by camera\nposes and trajectories, which are appearance-free and minimally impact the\nappearance or shape of objects in generated videos. 3) It is a relatively\ngeneralizable model that can adapt to a wide array of camera poses and\ntrajectories once trained. Extensive qualitative and quantitative experiments\nhave been conducted to demonstrate the superiority of MotionCtrl over existing\nmethods. Project Page: https://wzhouxiff.github.io/projects/MotionCtrl/\n","authors":["Zhouxia Wang","Ziyang Yuan","Xintao Wang","Tianshui Chen","Menghan Xia","Ping Luo","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2312.03641v2.pdf","comment":"SIGGRAPH 2024 Conference Proceedings"},{"id":"http://arxiv.org/abs/2405.19802v3","updated":"2024-07-16T14:43:51Z","published":"2024-05-30T08:12:08Z","title":"Exploring the Robustness of Decision-Level Through Adversarial Attacks\n  on LLM-Based Embodied Models","summary":"  Embodied intelligence empowers agents with a profound sense of perception,\nenabling them to respond in a manner closely aligned with real-world\nsituations. Large Language Models (LLMs) delve into language instructions with\ndepth, serving a crucial role in generating plans for intricate tasks. Thus,\nLLM-based embodied models further enhance the agent's capacity to comprehend\nand process information. However, this amalgamation also ushers in new\nchallenges in the pursuit of heightened intelligence. Specifically, attackers\ncan manipulate LLMs to produce irrelevant or even malicious outputs by altering\ntheir prompts. Confronted with this challenge, we observe a notable absence of\nmulti-modal datasets essential for comprehensively evaluating the robustness of\nLLM-based embodied models. Consequently, we construct the Embodied Intelligent\nRobot Attack Dataset (EIRAD), tailored specifically for robustness evaluation.\nAdditionally, two attack strategies are devised, including untargeted attacks\nand targeted attacks, to effectively simulate a range of diverse attack\nscenarios. At the same time, during the attack process, to more accurately\nascertain whether our method is successful in attacking the LLM-based embodied\nmodel, we devise a new attack success evaluation method utilizing the BLIP2\nmodel. Recognizing the time and cost-intensive nature of the GCG algorithm in\nattacks, we devise a scheme for prompt suffix initialization based on various\ntarget tasks, thus expediting the convergence process. Experimental results\ndemonstrate that our method exhibits a superior attack success rate when\ntargeting LLM-based embodied models, indicating a lower level of decision-level\nrobustness in these models.\n","authors":["Shuyuan Liu","Jiawei Chen","Shouwei Ruan","Hang Su","Zhaoxia Yin"],"pdf_url":"https://arxiv.org/pdf/2405.19802v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11566v1","updated":"2024-07-16T10:19:14Z","published":"2024-07-16T10:19:14Z","title":"TGIF: Text-Guided Inpainting Forgery Dataset","summary":"  Digital image manipulation has become increasingly accessible and realistic\nwith the advent of generative AI technologies. Recent developments allow for\ntext-guided inpainting, making sophisticated image edits possible with minimal\neffort. This poses new challenges for digital media forensics. For example,\ndiffusion model-based approaches could either splice the inpainted region into\nthe original image, or regenerate the entire image. In the latter case,\ntraditional image forgery localization (IFL) methods typically fail. This paper\nintroduces the Text-Guided Inpainting Forgery (TGIF) dataset, a comprehensive\ncollection of images designed to support the training and evaluation of image\nforgery localization and synthetic image detection (SID) methods. The TGIF\ndataset includes approximately 80k forged images, originating from popular\nopen-source and commercial methods; SD2, SDXL, and Adobe Firefly. Using this\ndata, we benchmark several state-of-the-art IFL and SID methods. Whereas\ntraditional IFL methods can detect spliced images, they fail to detect\nregenerated inpainted images. Moreover, traditional SID may detect the\nregenerated inpainted images to be fake, but cannot localize the inpainted\narea. Finally, both types of methods fail when exposed to stronger compression,\nwhile they are less robust to modern compression algorithms, such as WEBP. As\nsuch, this work demonstrates the inefficiency of state-of-the-art detectors on\nlocal manipulations performed by modern generative approaches, and aspires to\nhelp with the development of more capable IFL and SID methods. The dataset can\nbe downloaded at https://github.com/IDLabMedia/tgif-dataset.\n","authors":["Hannes Mareen","Dimitrios Karageorgiou","Glenn Van Wallendael","Peter Lambert","Symeon Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.11566v1.pdf","comment":"6 pages, submitted to conference"},{"id":"http://arxiv.org/abs/2407.11496v1","updated":"2024-07-16T08:33:55Z","published":"2024-07-16T08:33:55Z","title":"ReLaX-VQA: Residual Fragment and Layer Stack Extraction for Enhancing\n  Video Quality Assessment","summary":"  With the rapid growth of User-Generated Content (UGC) exchanged between users\nand sharing platforms, the need for video quality assessment in the wild has\nemerged. UGC is mostly acquired using consumer devices and undergoes multiple\nrounds of compression or transcoding before reaching the end user. Therefore,\ntraditional quality metrics that require the original content as a reference\ncannot be used. In this paper, we propose ReLaX-VQA, a novel No-Reference Video\nQuality Assessment (NR-VQA) model that aims to address the challenges of\nevaluating the diversity of video content and the assessment of its quality\nwithout reference videos. ReLaX-VQA uses fragments of residual frames and\noptical flow, along with different expressions of spatial features of the\nsampled frames, to enhance motion and spatial perception. Furthermore, the\nmodel enhances abstraction by employing layer-stacking techniques in deep\nneural network features (from Residual Networks and Vision Transformers).\nExtensive testing on four UGC datasets confirms that ReLaX-VQA outperforms\nexisting NR-VQA methods with an average SRCC value of 0.8658 and PLCC value of\n0.8872. We will open source the code and trained models to facilitate further\nresearch and applications of NR-VQA: https://github.com/xinyiW915/ReLaX-VQA.\n","authors":["Xinyi Wang","Angeliki Katsenou","David Bull"],"pdf_url":"https://arxiv.org/pdf/2407.11496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11492v1","updated":"2024-07-16T08:26:59Z","published":"2024-07-16T08:26:59Z","title":"MMSD-Net: Towards Multi-modal Stuttering Detection","summary":"  Stuttering is a common speech impediment that is caused by irregular\ndisruptions in speech production, affecting over 70 million people across the\nworld. Standard automatic speech processing tools do not take speech ailments\ninto account and are thereby not able to generate meaningful results when\npresented with stuttered speech as input. The automatic detection of stuttering\nis an integral step towards building efficient, context-aware speech processing\nsystems. While previous approaches explore both statistical and neural\napproaches for stuttering detection, all of these methods are uni-modal in\nnature. This paper presents MMSD-Net, the first multi-modal neural framework\nfor stuttering detection. Experiments and results demonstrate that\nincorporating the visual signal significantly aids stuttering detection, and\nour model yields an improvement of 2-17% in the F1-score over existing\nstate-of-the-art uni-modal approaches.\n","authors":["Liangyu Nie","Sudarsana Reddy Kadiri","Ruchit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.11492v1.pdf","comment":"Accepted at INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.09833v3","updated":"2024-07-16T08:09:56Z","published":"2024-06-14T08:43:31Z","title":"SHMamba: Structured Hyperbolic State Space Model for Audio-Visual\n  Question Answering","summary":"  The Audio-Visual Question Answering (AVQA) task holds significant potential\nfor applications. Compared to traditional unimodal approaches, the multi-modal\ninput of AVQA makes feature extraction and fusion processes more challenging.\nEuclidean space is difficult to effectively represent multi-dimensional\nrelationships of data. Especially when extracting and processing data with a\ntree structure or hierarchical structure, Euclidean space is not suitable as an\nembedding space. Additionally, the self-attention mechanism in Transformers is\neffective in capturing the dynamic relationships between elements in a\nsequence. However, the self-attention mechanism's limitations in window\nmodeling and quadratic computational complexity reduce its effectiveness in\nmodeling long sequences. To address these limitations, we propose SHMamba:\nStructured Hyperbolic State Space Model to integrate the advantages of\nhyperbolic geometry and state space models. Specifically, SHMamba leverages the\nintrinsic properties of hyperbolic space to represent hierarchical structures\nand complex relationships in audio-visual data. Meanwhile, the state space\nmodel captures dynamic changes over time by globally modeling the entire\nsequence. Furthermore, we introduce an adaptive curvature hyperbolic alignment\nmodule and a cross fusion block to enhance the understanding of hierarchical\nstructures and the dynamic exchange of cross-modal information, respectively.\nExtensive experiments demonstrate that SHMamba outperforms previous methods\nwith fewer parameters and computational costs. Our learnable parameters are\nreduced by 78.12\\%, while the average performance improves by 2.53\\%.\nExperiments show that our method demonstrates superiority among all current\nmajor methods and is more suitable for practical application scenarios.\n","authors":["Zhe Yang","Wenrui Li","Guanghui Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.09833v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05645v2","updated":"2024-07-16T07:50:44Z","published":"2024-07-08T06:14:37Z","title":"OneDiff: A Generalist Model for Image Difference Captioning","summary":"  In computer vision, Image Difference Captioning (IDC) is crucial for\naccurately describing variations between closely related images. Traditional\nIDC methods often rely on specialist models, which restrict their applicability\nacross varied contexts. This paper introduces the OneDiff model, a novel\ngeneralist approach that utilizes a robust vision-language model architecture,\nintegrating a siamese image encoder with a Visual Delta Module. This innovative\nconfiguration allows for the precise detection and articulation of fine-grained\ndifferences between image pairs. OneDiff is trained through a dual-phase\nstrategy, encompassing Coupled Sample Training and multi-task learning across a\ndiverse array of data types, supported by our newly developed DiffCap Dataset.\nThis dataset merges real-world and synthetic data, enhancing the training\nprocess and bolstering the model's robustness. Extensive testing on diverse IDC\nbenchmarks, such as Spot-the-Diff, CLEVR-Change, and Birds-to-Words, shows that\nOneDiff consistently outperforms existing state-of-the-art models in accuracy\nand adaptability, achieving improvements of up to 85\\% CIDEr points in average.\nBy setting a new benchmark in IDC, OneDiff paves the way for more versatile and\neffective applications in detecting and describing visual differences. The\ncode, models, and data will be made publicly available.\n","authors":["Erdong Hu","Longteng Guo","Tongtian Yue","Zijia Zhao","Shuning Xue","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2407.05645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05060v3","updated":"2024-07-16T07:16:56Z","published":"2024-03-08T05:15:05Z","title":"Multimodal Infusion Tuning for Large Models","summary":"  Recent advancements in large-scale models have showcased remarkable\ngeneralization capabilities in various tasks. However, integrating multimodal\nprocessing into these models presents a significant challenge, as it often\ncomes with a high computational burden. To address this challenge, we introduce\na new parameter-efficient multimodal tuning strategy for large models in this\npaper, referred to as Multimodal Infusion Tuning (MiT). MiT leverages decoupled\nself-attention mechanisms within large language models to effectively integrate\ninformation from diverse modalities such as images and acoustics. In MiT, we\nalso design a novel adaptive rescaling strategy at the attention head level,\nwhich optimizes the representation of infused multimodal features. Notably, all\nfoundation models are kept frozen during the tuning process to reduce the\ncomputational burden and only 2.5\\% parameters are tunable. We conduct\nexperiments across a range of multimodal tasks, including image-related tasks\nlike referring segmentation and non-image tasks such as sentiment analysis. Our\nresults showcase that MiT achieves state-of-the-art performance in multimodal\nunderstanding while significantly reducing computational overhead(10\\% of\nprevious methods). Moreover, our tuned model exhibits robust reasoning\nabilities even in complex scenarios.\n","authors":["Hao Sun","Yu Song","Xinyao Yu","Jiaqing Liu","Yen-Wei Chen","Lanfen Lin"],"pdf_url":"https://arxiv.org/pdf/2403.05060v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09266v2","updated":"2024-07-16T07:09:27Z","published":"2024-05-15T11:33:07Z","title":"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation","summary":"  Automated choreography advances by generating dance from music. Current\nmethods create skeleton keypoint sequences, not full dance videos, and cannot\nmake specific individuals dance, limiting their real-world use. These methods\nalso need precise keypoint annotations, making data collection difficult and\nrestricting the use of self-made video datasets. To overcome these challenges,\nwe introduce a novel task: generating dance videos directly from images of\nindividuals guided by music. This task enables the dance generation of specific\nindividuals without requiring keypoint annotations, making it more versatile\nand applicable to various situations. Our solution, the Dance Any Beat\nDiffusion model (DabFusion), utilizes a reference image and a music piece to\ngenerate dance videos featuring various dance types and choreographies. The\nmusic is analyzed by our specially designed music encoder, which identifies\nessential features including dance style, movement, and rhythm. DabFusion\nexcels in generating dance videos not only for individuals in the training\ndataset but also for any previously unseen person. This versatility stems from\nits approach of generating latent optical flow, which contains all necessary\nmotion information to animate any person in the image. We evaluate DabFusion's\nperformance using the AIST++ dataset, focusing on video quality, audio-video\nsynchronization, and motion-music alignment. We propose a 2D Motion-Music\nAlignment Score (2D-MM Align), which builds on the Beat Alignment Score to more\neffectively evaluate motion-music alignment for this new task. Experiments show\nthat our DabFusion establishes a solid baseline for this innovative task. Video\nresults can be found on our project page: https://DabFusion.github.io.\n","authors":["Xuanchen Wang","Heng Wang","Dongnan Liu","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2405.09266v2.pdf","comment":"11 pages, 6 figures, demo page: https://DabFusion.github.io"},{"id":"http://arxiv.org/abs/2407.12064v1","updated":"2024-07-16T02:19:02Z","published":"2024-07-16T02:19:02Z","title":"LiteGPT: Large Vision-Language Model for Joint Chest X-ray Localization\n  and Classification Task","summary":"  Vision-language models have been extensively explored across a wide range of\ntasks, achieving satisfactory performance; however, their application in\nmedical imaging remains underexplored. In this work, we propose a unified\nframework - LiteGPT - for the medical imaging. We leverage multiple pre-trained\nvisual encoders to enrich information and enhance the performance of\nvision-language models. To the best of our knowledge, this is the first study\nto utilize vision-language models for the novel task of joint localization and\nclassification in medical images. Besides, we are pioneers in providing\nbaselines for disease localization in chest X-rays. Finally, we set new\nstate-of-the-art performance in the image classification task on the\nwell-benchmarked VinDr-CXR dataset. All code and models are publicly available\nonline: https://github.com/leduckhai/LiteGPT\n","authors":["Khai Le-Duc","Ryan Zhang","Ngoc Son Nguyen","Tan-Hanh Pham","Anh Dao","Ba Hung Ngo","Anh Totti Nguyen","Truong-Son Hy"],"pdf_url":"https://arxiv.org/pdf/2407.12064v1.pdf","comment":"Preprint, 19 pages"}]},"2024-07-15T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.07103v2","updated":"2024-07-15T23:36:18Z","published":"2024-04-10T15:41:53Z","title":"Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on\n  Graphs","summary":"  Large language models (LLMs), while exhibiting exceptional performance,\nsuffer from hallucinations, especially on knowledge-intensive tasks. Existing\nworks propose to augment LLMs with individual text units retrieved from\nexternal knowledge corpora to alleviate the issue. However, in many domains,\ntexts are interconnected (e.g., academic papers in a bibliographic graph are\nlinked by citations and co-authorships) which form a (text-attributed) graph.\nThe knowledge in such graphs is encoded not only in single texts/nodes but also\nin their associated connections. To facilitate the research of augmenting LLMs\nwith graphs, we manually construct a Graph Reasoning Benchmark dataset called\nGRBench, containing 1,740 questions that can be answered with the knowledge\nfrom 10 domain graphs. Then, we propose a simple and effective framework called\nGraph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging\nLLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of\nthree sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We\nconduct systematic experiments with three LLM backbones on GRBench, where\nGraph-CoT outperforms the baselines consistently. The code is available at\nhttps://github.com/PeterGriffinJin/Graph-CoT.\n","authors":["Bowen Jin","Chulin Xie","Jiawei Zhang","Kashob Kumar Roy","Yu Zhang","Zheng Li","Ruirui Li","Xianfeng Tang","Suhang Wang","Yu Meng","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2404.07103v2.pdf","comment":"21 pages. Code: https://github.com/PeterGriffinJin/Graph-CoT"},{"id":"http://arxiv.org/abs/2407.11245v1","updated":"2024-07-15T21:14:13Z","published":"2024-07-15T21:14:13Z","title":"Pacer and Runner: Cooperative Learning Framework between Single- and\n  Cross-Domain Sequential Recommendation","summary":"  Cross-Domain Sequential Recommendation (CDSR) improves recommendation\nperformance by utilizing information from multiple domains, which contrasts\nwith Single-Domain Sequential Recommendation (SDSR) that relies on a historical\ninteraction within a specific domain. However, CDSR may underperform compared\nto the SDSR approach in certain domains due to negative transfer, which occurs\nwhen there is a lack of relation between domains or different levels of data\nsparsity. To address the issue of negative transfer, our proposed CDSR model\nestimates the degree of negative transfer of each domain and adaptively assigns\nit as a weight factor to the prediction loss, to control gradient flows through\ndomains with significant negative transfer. To this end, our model compares the\nperformance of a model trained on multiple domains (CDSR) with a model trained\nsolely on the specific domain (SDSR) to evaluate the negative transfer of each\ndomain using our asymmetric cooperative network. In addition, to facilitate the\ntransfer of valuable cues between the SDSR and CDSR tasks, we developed an\nauxiliary loss that maximizes the mutual information between the representation\npairs from both tasks on a per-domain basis. This cooperative learning between\nSDSR and CDSR tasks is similar to the collaborative dynamics between pacers and\nrunners in a marathon. Our model outperformed numerous previous works in\nextensive experiments on two real-world industrial datasets across ten service\ndomains. We also have deployed our model in the recommendation system of our\npersonal assistant app service, resulting in 21.4% increase in click-through\nrate compared to existing models, which is valuable to real-world business.\n","authors":["Chung Park","Taesan Kim","Hyungjun Yoon","Junui Hong","Yelim Yu","Mincheol Cho","Minsung Choi","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2407.11245v1.pdf","comment":"Accepted at SIGIR'24"},{"id":"http://arxiv.org/abs/2311.08662v2","updated":"2024-07-15T20:59:49Z","published":"2023-11-15T02:59:10Z","title":"Evaluating Concurrent Robustness of Language Models Across Diverse\n  Challenge Sets","summary":"  Language models, characterized by their black-box nature, often hallucinate\nand display sensitivity to input perturbations, causing concerns about trust.\nTo enhance trust, it is imperative to gain a comprehensive understanding of the\nmodel's failure modes and develop effective strategies to improve their\nperformance. In this study, we introduce a methodology designed to examine how\ninput perturbations affect language models across various scales, including\npre-trained models and large language models (LLMs). Utilizing fine-tuning, we\nenhance the model's robustness to input perturbations. Additionally, we\ninvestigate whether exposure to one perturbation enhances or diminishes the\nmodel's performance with respect to other perturbations. To address robustness\nagainst multiple perturbations, we present three distinct fine-tuning\nstrategies. Furthermore, we broaden the scope of our methodology to encompass\nlarge language models (LLMs) by leveraging a chain of thought (CoT) prompting\napproach augmented with exemplars. We employ the Tabular-NLI task to showcase\nhow our proposed strategies adeptly train a robust model, enabling it to\naddress diverse perturbations while maintaining accuracy on the original\ndataset.\n","authors":["Vatsal Gupta","Pranshu Pandya","Tushar Kataria","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2311.08662v2.pdf","comment":"23 pages, 16 Figure, 10 Tables"},{"id":"http://arxiv.org/abs/2407.12873v1","updated":"2024-07-15T17:40:15Z","published":"2024-07-15T17:40:15Z","title":"Evaluation of RAG Metrics for Question Answering in the Telecom Domain","summary":"  Retrieval Augmented Generation (RAG) is widely used to enable Large Language\nModels (LLMs) perform Question Answering (QA) tasks in various domains.\nHowever, RAG based on open-source LLM for specialized domains has challenges of\nevaluating generated responses. A popular framework in the literature is the\nRAG Assessment (RAGAS), a publicly available library which uses LLMs for\nevaluation. One disadvantage of RAGAS is the lack of details of derivation of\nnumerical value of the evaluation metrics. One of the outcomes of this work is\na modified version of this package for few metrics (faithfulness, context\nrelevance, answer relevance, answer correctness, answer similarity and factual\ncorrectness) through which we provide the intermediate outputs of the prompts\nby using any LLMs. Next, we analyse the expert evaluations of the output of the\nmodified RAGAS package and observe the challenges of using it in the telecom\ndomain. We also study the effect of the metrics under correct vs. wrong\nretrieval and observe that few of the metrics have higher values for correct\nretrieval. We also study for differences in metrics between base embeddings and\nthose domain adapted via pre-training and fine-tuning. Finally, we comment on\nthe suitability and challenges of using these metrics for in-the-wild telecom\nQA task.\n","authors":["Sujoy Roychowdhury","Sumit Soman","H G Ranjani","Neeraj Gunda","Vansh Chhabra","Sai Krishna Bala"],"pdf_url":"https://arxiv.org/pdf/2407.12873v1.pdf","comment":"Accepted for publication in ICML 2024 Workshop on Foundation Models\n  in the Wild"},{"id":"http://arxiv.org/abs/2407.10829v1","updated":"2024-07-15T15:42:22Z","published":"2024-07-15T15:42:22Z","title":"BiasScanner: Automatic Detection and Classification of News Bias to\n  Strengthen Democracy","summary":"  The increasing consumption of news online in the 21st century coincided with\nincreased publication of disinformation, biased reporting, hate speech and\nother unwanted Web content. We describe BiasScanner, an application that aims\nto strengthen democracy by supporting news consumers with scrutinizing news\narticles they are reading online. BiasScanner contains a server-side\npre-trained large language model to identify biased sentences of news articles\nand a front-end Web browser plug-in. At the time of writing, BiasScanner can\nidentify and classify more than two dozen types of media bias at the sentence\nlevel, making it the most fine-grained model and only deployed application\n(automatic system in use) of its kind. It was implemented in a light-weight and\nprivacy-respecting manner, and in addition to highlighting likely biased\nsentence it also provides explanations for each classification decision as well\nas a summary analysis for each news article. While prior research has addressed\nnews bias detection, we are not aware of any work that resulted in a deployed\nbrowser plug-in (c.f. also biasscanner.org for a Web demo).\n","authors":["Tim Menzner","Jochen L. Leidner"],"pdf_url":"https://arxiv.org/pdf/2407.10829v1.pdf","comment":"10 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.18684v2","updated":"2024-07-15T14:48:09Z","published":"2024-03-27T15:27:36Z","title":"Scaling Laws For Dense Retrieval","summary":"  Scaling up neural models has yielded significant advancements in a wide array\nof tasks, particularly in language generation. Previous studies have found that\nthe performance of neural models frequently adheres to predictable scaling\nlaws, correlated with factors such as training set size and model size. This\ninsight is invaluable, especially as large-scale experiments grow increasingly\nresource-intensive. Yet, such scaling law has not been fully explored in dense\nretrieval due to the discrete nature of retrieval metrics and complex\nrelationships between training data and model sizes in retrieval tasks. In this\nstudy, we investigate whether the performance of dense retrieval models follows\nthe scaling law as other neural models. We propose to use contrastive\nlog-likelihood as the evaluation metric and conduct extensive experiments with\ndense retrieval models implemented with different numbers of parameters and\ntrained with different amounts of annotated data. Results indicate that, under\nour settings, the performance of dense retrieval models follows a precise\npower-law scaling related to the model size and the number of annotations.\nAdditionally, we examine scaling with prevalent data augmentation methods to\nassess the impact of annotation quality, and apply the scaling law to find the\nbest resource allocation strategy under a budget constraint. We believe that\nthese insights will significantly contribute to understanding the scaling\neffect of dense retrieval models and offer meaningful guidance for future\nresearch endeavors.\n","authors":["Yan Fang","Jingtao Zhan","Qingyao Ai","Jiaxin Mao","Weihang Su","Jia Chen","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18684v2.pdf","comment":"Accepted at SIGIR 2024. V2 fixes a bug in the experiments"},{"id":"http://arxiv.org/abs/2407.07871v2","updated":"2024-07-15T14:23:35Z","published":"2024-07-10T17:37:15Z","title":"Enhancing HNSW Index for Real-Time Updates: Addressing Unreachable\n  Points and Performance Degradation","summary":"  The approximate nearest neighbor search (ANNS) is a fundamental and essential\ncomponent in data mining and information retrieval, with graph-based\nmethodologies demonstrating superior performance compared to alternative\napproaches. Extensive research efforts have been dedicated to improving search\nefficiency by developing various graph-based indices, such as HNSW\n(Hierarchical Navigable Small World). However, the performance of HNSW and most\ngraph-based indices become unacceptable when faced with a large number of\nreal-time deletions, insertions, and updates. Furthermore, during update\noperations, HNSW can result in some data points becoming unreachable, a\nsituation we refer to as the `unreachable points phenomenon'. This phenomenon\ncould significantly affect the search accuracy of the graph in certain\nsituations.\n  To address these issues, we present efficient measures to overcome the\nshortcomings of HNSW, specifically addressing poor performance over long\nperiods of delete and update operations and resolving the issues caused by the\nunreachable points phenomenon. Our proposed MN-RU algorithm effectively\nimproves update efficiency and suppresses the growth rate of unreachable\npoints, ensuring better overall performance and maintaining the integrity of\nthe graph. Our results demonstrate that our methods outperform existing\napproaches. Furthermore, since our methods are based on HNSW, they can be\neasily integrated with existing indices widely used in the industrial field,\nmaking them practical for future real-world applications. Code is available at\n\\url{https://github.com/xwt1/MN-RU.git}\n","authors":["Wentao Xiao","Yueyang Zhan","Rui Xi","Mengshu Hou","Jianming Liao"],"pdf_url":"https://arxiv.org/pdf/2407.07871v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10714v1","updated":"2024-07-15T13:33:30Z","published":"2024-07-15T13:33:30Z","title":"SEMINAR: Search Enhanced Multi-modal Interest Network and Approximate\n  Retrieval for Lifelong Sequential Recommendation","summary":"  The modeling of users' behaviors is crucial in modern recommendation systems.\nA lot of research focuses on modeling users' lifelong sequences, which can be\nextremely long and sometimes exceed thousands of items. These models use the\ntarget item to search for the most relevant items from the historical sequence.\nHowever, training lifelong sequences in click through rate (CTR) prediction or\npersonalized search ranking (PSR) is extremely difficult due to the\ninsufficient learning problem of ID embedding, especially when the IDs in the\nlifelong sequence features do not exist in the samples of training dataset.\nAdditionally, existing target attention mechanisms struggle to learn the\nmulti-modal representations of items in the sequence well. The distribution of\nmulti-modal embedding (text, image and attributes) output of user's interacted\nitems are not properly aligned and there exist divergence across modalities. We\nalso observe that users' search query sequences and item browsing sequences can\nfully depict users' intents and benefit from each other. To address these\nchallenges, we propose a unified lifelong multi-modal sequence model called\nSEMINAR-Search Enhanced Multi-Modal Interest Network and Approximate Retrieval.\nSpecifically, a network called Pretraining Search Unit (PSU) learns the\nlifelong sequences of multi-modal query-item pairs in a pretraining-finetuning\nmanner with multiple objectives: multi-modal alignment, next query-item pair\nprediction, query-item relevance prediction, etc. After pretraining, the\ndownstream model restores the pretrained embedding as initialization and\nfinetunes the network. To accelerate the online retrieval speed of multi-modal\nembedding, we propose a multi-modal codebook-based product quantization\nstrategy to approximate the exact attention calculati\n","authors":["Kaiming Shen","Xichen Ding","Zixiang Zheng","Yuqi Gong","Qianqian Li","Zhongyi Liu","Guannan Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.10714v1.pdf","comment":"9 pages,code released"},{"id":"http://arxiv.org/abs/2402.05070v2","updated":"2024-07-15T13:06:13Z","published":"2024-02-07T18:21:17Z","title":"A Roadmap to Pluralistic Alignment","summary":"  With increased power and prevalence of AI systems, it is ever more critical\nthat AI systems are designed to serve all, i.e., people with diverse values and\nperspectives. However, aligning models to serve pluralistic human values\nremains an open research question. In this piece, we propose a roadmap to\npluralistic alignment, specifically using language models as a test bed. We\nidentify and formalize three possible ways to define and operationalize\npluralism in AI systems: 1) Overton pluralistic models that present a spectrum\nof reasonable responses; 2) Steerably pluralistic models that can steer to\nreflect certain perspectives; and 3) Distributionally pluralistic models that\nare well-calibrated to a given population in distribution. We also formalize\nand discuss three possible classes of pluralistic benchmarks: 1)\nMulti-objective benchmarks, 2) Trade-off steerable benchmarks, which\nincentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic\nbenchmarks which explicitly model diverse human ratings. We use this framework\nto argue that current alignment techniques may be fundamentally limited for\npluralistic AI; indeed, we highlight empirical evidence, both from our own\nexperiments and from other work, that standard alignment procedures might\nreduce distributional pluralism in models, motivating the need for further\nresearch on pluralistic alignment.\n","authors":["Taylor Sorensen","Jared Moore","Jillian Fisher","Mitchell Gordon","Niloofar Mireshghallah","Christopher Michael Rytting","Andre Ye","Liwei Jiang","Ximing Lu","Nouha Dziri","Tim Althoff","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2402.05070v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2407.10691v1","updated":"2024-07-15T13:04:09Z","published":"2024-07-15T13:04:09Z","title":"$\\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific\n  Domain through Complementary Granularity","summary":"  Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers,\nrespectively, averaged on queries containing multiple subqueries from five\nscientific retrieval datasets. Moreover, the efficacy of two downstream\nscientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$to boost the application of LLMs in the scientific domain.\n","authors":["Fengyu Cai","Xinran Zhao","Tong Chen","Sihao Chen","Hongming Zhang","Iryna Gurevych","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2407.10691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10599v1","updated":"2024-07-15T10:24:00Z","published":"2024-07-15T10:24:00Z","title":"General algorithm of assigning raster features to vector maps at any\n  resolution or scale","summary":"  The fusion of multi-source data is essential for a comprehensive analysis of\ngeographic applications. Due to distinct data structures, the fusion process\ntends to encounter technical difficulties in terms of preservation of the\nintactness of each source data. Furthermore, a lack of generalized methods is a\nproblem when the method is expected to be applicable in multiple resolutions,\nsizes, or scales of raster and vector data, to what is being processed. In this\nstudy, we propose a general algorithm of assigning features from raster data\n(concentrations of air pollutants) to vector components (roads represented by\nedges) in city maps through the iterative construction of virtual layers to\nexpand geolocation from a city centre to boundaries in a 2D projected map. The\nconstruction follows the rule of perfect squares with a slight difference\ndepending on the oddness or evenness of the ratio of city size to raster\nresolution. We demonstrate the algorithm by applying it to assign accurate\nPM$_{2.5}$ and NO$_{2}$ concentrations to roads in 1692 cities globally for a\npotential graph-based pollution analysis. This method could pave the way for\nagile studies on urgent climate issues by providing a generic and efficient\nmethod to accurately fuse multiple datasets of varying scales and compositions.\n","authors":["Nan Xu","Mark Stevenson","Kerry A. Nice","Sachith Seneviratne"],"pdf_url":"https://arxiv.org/pdf/2407.10599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20994v2","updated":"2024-07-15T08:50:29Z","published":"2024-05-31T16:38:54Z","title":"CWRCzech: 100M Query-Document Czech Click Dataset and Its Application to\n  Web Relevance Ranking","summary":"  We present CWRCzech, Click Web Ranking dataset for Czech, a 100M\nquery-document Czech click dataset for relevance ranking with user behavior\ndata collected from search engine logs of Seznam$.$cz. To the best of our\nknowledge, CWRCzech is the largest click dataset with raw text published so\nfar. It provides document positions in the search results as well as\ninformation about user behavior: 27.6M clicked documents and 10.8M dwell times.\nIn addition, we also publish a manually annotated Czech test for the relevance\ntask, containing nearly 50k query-document pairs, each annotated by at least 2\nannotators. Finally, we analyze how the user behavior data improve relevance\nranking and show that models trained on data automatically harnessed at\nsufficient scale can surpass the performance of models trained on human\nannotated data. CWRCzech is published under an academic non-commercial license\nand is available to the research community at\nhttps://github.com/seznam/CWRCzech.\n","authors":["Josef Vonášek","Milan Straka","Rostislav Krč","Lenka Lasoňová","Ekaterina Egorova","Jana Straková","Jakub Náplava"],"pdf_url":"https://arxiv.org/pdf/2405.20994v2.pdf","comment":"Accepted to SIGIR 2024"},{"id":"http://arxiv.org/abs/2303.12973v2","updated":"2024-07-15T01:57:30Z","published":"2023-03-23T00:42:48Z","title":"Uncertainty Calibration for Counterfactual Propensity Estimation in\n  Recommendation","summary":"  Post-click conversion rate (CVR) is a reliable indicator of online customers'\npreferences, making it crucial for developing recommender systems. A major\nchallenge in predicting CVR is severe selection bias, arising from users'\ninherent self-selection behavior and the system's item selection process. To\nmitigate this issue, the inverse propensity score (IPS) is employed to weight\nthe prediction error of each observed instance. However, current propensity\nscore estimations are unreliable due to the lack of a quality measure. To\naddress this, we evaluate the quality of propensity scores from the perspective\nof uncertainty calibration, proposing the use of expected calibration error\n(ECE) as a measure of propensity-score quality. We argue that the performance\nof IPS-based recommendations is hampered by miscalibration in propensity\nestimation. We introduce a model-agnostic calibration framework for\npropensity-based debiasing of CVR predictions. Theoretical analysis on bias and\ngeneralization bounds demonstrates the superiority of calibrated propensity\nestimates over uncalibrated ones. Experiments conducted on the Coat, Yahoo and\nKuaiRand datasets show improved uncertainty calibration, as evidenced by lower\nECE values, leading to enhanced CVR prediction outcomes.\n","authors":["Wenbo Hu","Xin Sun","Qiang liu","Le Wu","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.12973v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10380v1","updated":"2024-07-15T01:21:56Z","published":"2024-07-15T01:21:56Z","title":"NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models","summary":"  Cognitive textual and visual reasoning tasks, such as puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. While LLMs and VLMs, through extensive\ntraining on large amounts of human-curated data, have attained a high level of\npseudo-human intelligence in some common sense reasoning tasks, they still\nstruggle with more complex reasoning tasks that require cognitive\nunderstanding. In this work, we introduce a new dataset, NTSEBench, designed to\nevaluate the cognitive multi-modal reasoning and problem-solving skills of\nlarge models. The dataset comprises 2,728 multiple-choice questions comprising\nof a total of 4,642 images across 26 categories sampled from the NTSE\nexamination conducted nationwide in India, featuring both visual and textual\ngeneral aptitude questions that do not rely on rote learning. We establish\nbaselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a\ncomparison between open source and propriety models, we propose four distinct\nmodeling strategies to handle different modalities (text and images) in the\ndataset instances.\n","authors":["Pranshu Pandya","Agney S Talwarr","Vatsal Gupta","Tushar Kataria","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2407.10380v1.pdf","comment":"15 pages, 2 figures, 5 tables"}],"Multimedia":[{"id":"http://arxiv.org/abs/2402.14947v2","updated":"2024-07-15T20:05:45Z","published":"2024-02-22T20:04:10Z","title":"An Avalanche of Images on Telegram Preceded Russia's Full-Scale Invasion\n  of Ukraine","summary":"  Governments use propaganda, including through visual content -- or\nPolitically Salient Image Patterns (PSIP) -- on social media, to influence and\nmanipulate public opinion. In the present work, we collected Telegram\npost-history of from 989 Russian milbloggers to better understand the social\nand political narratives that circulated online in the months surrounding\nRussia's 2022 full-scale invasion of Ukraine. Overall, we found an 8,925%\nincrease (p<0.001) in the number of posts and a 5,352% increase (p<0.001) in\nthe number of images posted by these accounts in the two weeks prior to the\ninvasion. We also observed a similar increase in the number and intensity of\npolitically salient manipulated images that circulated on Telegram. Although\nthis paper does not evaluate malice or coordination in these activities, we do\nconclude with a call for further research into the role that manipulated visual\nmedia has in the lead-up to instability events and armed conflict.\n","authors":["William Theisen","Michael Yankoski","Kristina Hook","Ernesto Verdeja","Walter Scheirer","Tim Weninger"],"pdf_url":"https://arxiv.org/pdf/2402.14947v2.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.10736v1","updated":"2024-07-15T14:01:35Z","published":"2024-07-15T14:01:35Z","title":"When Synthetic Traces Hide Real Content: Analysis of Stable Diffusion\n  Image Laundering","summary":"  In recent years, methods for producing highly realistic synthetic images have\nsignificantly advanced, allowing the creation of high-quality images from text\nprompts that describe the desired content. Even more impressively, Stable\nDiffusion (SD) models now provide users with the option of creating synthetic\nimages in an image-to-image translation fashion, modifying images in the latent\nspace of advanced autoencoders. This striking evolution, however, brings an\nalarming consequence: it is possible to pass an image through SD autoencoders\nto reproduce a synthetic copy of the image with high realism and almost no\nvisual artifacts. This process, known as SD image laundering, can transform\nreal images into lookalike synthetic ones and risks complicating forensic\nanalysis for content authenticity verification. Our paper investigates the\nforensic implications of image laundering, revealing a serious potential to\nobscure traces of real content, including sensitive and harmful materials that\ncould be mistakenly classified as synthetic, thereby undermining the protection\nof individuals depicted. To address this issue, we propose a two-stage\ndetection pipeline that effectively differentiates between pristine, laundered,\nand fully synthetic images (those generated from text prompts), showing\nrobustness across various conditions. Finally, we highlight another alarming\nproperty of image laundering, which appears to mask the unique artifacts\nexploited by forensic detectors to solve the camera model identification task,\nstrongly undermining their performance. Our experimental code is available at\nhttps://github.com/polimi-ispl/synthetic-image-detection.\n","authors":["Sara Mandelli","Paolo Bestagini","Stefano Tubaro"],"pdf_url":"https://arxiv.org/pdf/2407.10736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.08619v3","updated":"2024-07-15T08:53:55Z","published":"2024-05-14T13:59:24Z","title":"ALMol: Aligned Language-Molecule Translation LLMs through Offline\n  Preference Contrastive Optimisation","summary":"  The field of chemistry and Artificial Intelligence (AI) intersection is an\narea of active research that aims to accelerate scientific discovery. The\nintegration of large language models (LLMs) with scientific modalities has\nshown significant promise in this endeavour. However, challenges persist in\neffectively addressing training efficacy and the out-of-distribution problem,\nparticularly as existing approaches rely on larger models and datasets. In this\ncontext, we focus on machine language-molecule translation and deploy a novel\ntraining approach called contrastive preference optimisation, which avoids\ngenerating translations that are merely adequate but not perfect. To ensure\ngeneralisability and mitigate memorisation effects, we conduct experiments\nusing only 10% of the data. Our results demonstrate that our models achieve up\nto a 32% improvement compared to counterpart models. Finally, we introduce a\nfine-grained, domain-agnostic evaluation method to assess hallucination in LLMs\nand promote responsible use.\n","authors":["Dimitris Gkoumas"],"pdf_url":"https://arxiv.org/pdf/2405.08619v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10474v1","updated":"2024-07-15T07:01:05Z","published":"2024-07-15T07:01:05Z","title":"Multi-source Knowledge Enhanced Graph Attention Networks for Multimodal\n  Fact Verification","summary":"  Multimodal fact verification is an under-explored and emerging field that has\ngained increasing attention in recent years. The goal is to assess the veracity\nof claims that involve multiple modalities by analyzing the retrieved evidence.\nThe main challenge in this area is to effectively fuse features from different\nmodalities to learn meaningful multimodal representations. To this end, we\npropose a novel model named Multi-Source Knowledge-enhanced Graph Attention\nNetwork (MultiKE-GAT). MultiKE-GAT introduces external multimodal knowledge\nfrom different sources and constructs a heterogeneous graph to capture complex\ncross-modal and cross-source interactions. We exploit a Knowledge-aware Graph\nFusion (KGF) module to learn knowledge-enhanced representations for each claim\nand evidence and eliminate inconsistencies and noises introduced by redundant\nentities. Experiments on two public benchmark datasets demonstrate that our\nmodel outperforms other comparison methods, showing the effectiveness and\nsuperiority of the proposed model.\n","authors":["Han Cao","Lingwei Wei","Wei Zhou","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2407.10474v1.pdf","comment":"Accepted by ICME 2024"},{"id":"http://arxiv.org/abs/2407.10462v1","updated":"2024-07-15T06:33:25Z","published":"2024-07-15T06:33:25Z","title":"BandControlNet: Parallel Transformers-based Steerable Popular Music\n  Generation with Fine-Grained Spatiotemporal Features","summary":"  Controllable music generation promotes the interaction between humans and\ncomposition systems by projecting the users' intent on their desired music. The\nchallenge of introducing controllability is an increasingly important issue in\nthe symbolic music generation field. When building controllable generative\npopular multi-instrument music systems, two main challenges typically present\nthemselves, namely weak controllability and poor music quality. To address\nthese issues, we first propose spatiotemporal features as powerful and\nfine-grained controls to enhance the controllability of the generative model.\nIn addition, an efficient music representation called REMI_Track is designed to\nconvert multitrack music into multiple parallel music sequences and shorten the\nsequence length of each track with Byte Pair Encoding (BPE) techniques.\nSubsequently, we release BandControlNet, a conditional model based on parallel\nTransformers, to tackle the multiple music sequences and generate high-quality\nmusic samples that are conditioned to the given spatiotemporal control\nfeatures. More concretely, the two specially designed modules of\nBandControlNet, namely structure-enhanced self-attention (SE-SA) and\nCross-Track Transformer (CTT), are utilized to strengthen the resulting musical\nstructure and inter-track harmony modeling respectively. Experimental results\ntested on two popular music datasets of different lengths demonstrate that the\nproposed BandControlNet outperforms other conditional music generation models\non most objective metrics in terms of fidelity and inference speed and shows\ngreat robustness in generating long music samples. The subjective evaluations\nshow BandControlNet trained on short datasets can generate music with\ncomparable quality to state-of-the-art models, while outperforming them\nsignificantly using longer datasets.\n","authors":["Jing Luo","Xinyu Yang","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2407.10462v1.pdf","comment":"Demo page: https://chinglohsiu.github.io/files/bandcontrolnet.html"}]},"2024-07-14T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.10283v1","updated":"2024-07-14T17:56:11Z","published":"2024-07-14T17:56:11Z","title":"Numbers Matter! Bringing Quantity-awareness to Retrieval Systems","summary":"  Quantitative information plays a crucial role in understanding and\ninterpreting the content of documents. Many user queries contain quantities and\ncannot be resolved without understanding their semantics, e.g., ``car that\ncosts less than $10k''. Yet, modern search engines apply the same ranking\nmechanisms for both words and quantities, overlooking magnitude and unit\ninformation. In this paper, we introduce two quantity-aware ranking techniques\ndesigned to rank both the quantity and textual content either jointly or\nindependently. These techniques incorporate quantity information in available\nretrieval systems and can address queries with numerical conditions equal,\ngreater than, and less than. To evaluate the effectiveness of our proposed\nmodels, we introduce two novel quantity-aware benchmark datasets in the domains\nof finance and medicine and compare our method against various lexical and\nneural models. The code and data are available under\nhttps://github.com/satya77/QuantityAwareRankers.\n","authors":["Satya Almasian","Milena Bruseva","Michael Gertz"],"pdf_url":"https://arxiv.org/pdf/2407.10283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10245v1","updated":"2024-07-14T15:25:08Z","published":"2024-07-14T15:25:08Z","title":"GenSco: Can Question Decomposition based Passage Alignment improve\n  Question Answering?","summary":"  Retrieval augmented generation (RAG) with large language models (LLMs) for\nQuestion Answering (QA) entails furnishing relevant context within the prompt\nto facilitate the LLM in answer generation. During the generation, inaccuracies\nor hallucinations frequently occur due to two primary factors: inadequate or\ndistracting context in the prompts, and the inability of LLMs to effectively\nreason through the facts. In this paper, we investigate whether providing\naligned context via a carefully selected passage sequence leads to better\nanswer generation by the LLM for multi-hop QA. We introduce, \"GenSco\", a novel\napproach of selecting passages based on the predicted decomposition of the\nmulti-hop questions}. The framework consists of two distinct LLMs: (i)\nGenerator LLM, which is used for question decomposition and final answer\ngeneration; (ii) an auxiliary open-sourced LLM, used as the scorer, to\nsemantically guide the Generator for passage selection. The generator is\ninvoked only once for the answer generation, resulting in a cost-effective and\nefficient approach. We evaluate on three broadly established multi-hop question\nanswering datasets: 2WikiMultiHop, Adversarial HotPotQA and MuSiQue and achieve\nan absolute gain of $15.1$ and $5.9$ points in Exact Match score with respect\nto the best performing baselines over MuSiQue and 2WikiMultiHop respectively.\n","authors":["Barah Fazili","Koustava Goswami","Natwar Modani","Inderjeet Nair"],"pdf_url":"https://arxiv.org/pdf/2407.10245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14945v2","updated":"2024-07-14T14:01:01Z","published":"2023-12-06T15:24:01Z","title":"Empowering ChatGPT-Like Large-Scale Language Models with Local Knowledge\n  Base for Industrial Prognostics and Health Management","summary":"  Prognostics and health management (PHM) is essential for industrial operation\nand maintenance, focusing on predicting, diagnosing, and managing the health\nstatus of industrial systems. The emergence of the ChatGPT-Like large-scale\nlanguage model (LLM) has begun to lead a new round of innovation in the AI\nfield. It has extensively promoted the level of intelligence in various fields.\nTherefore, it is also expected further to change the application paradigm in\nindustrial PHM and promote PHM to become intelligent. Although ChatGPT-Like\nLLMs have rich knowledge reserves and powerful language understanding and\ngeneration capabilities, they lack domain-specific expertise, significantly\nlimiting their practicability in PHM applications. To this end, this study\nexplores the ChatGPT-Like LLM empowered by the local knowledge base (LKB) in\nindustrial PHM to solve the above limitations. In addition, we introduce the\nmethod and steps of combining the LKB with LLMs, including LKB preparation, LKB\nvectorization, prompt engineering, etc. Experimental analysis of real cases\nshows that combining the LKB with ChatGPT-Like LLM can significantly improve\nits performance and make ChatGPT-Like LLMs more accurate, relevant, and able to\nprovide more insightful information. This can promote the development of\nChatGPT-Like LLMs in industrial PHM and promote their efficiency and quality.\n","authors":["Huan Wang","Yan-Fu Li","Min Xie"],"pdf_url":"https://arxiv.org/pdf/2312.14945v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10184v1","updated":"2024-07-14T13:03:35Z","published":"2024-07-14T13:03:35Z","title":"Towards Robust Recommendation via Decision Boundary-aware Graph\n  Contrastive Learning","summary":"  In recent years, graph contrastive learning (GCL) has received increasing\nattention in recommender systems due to its effectiveness in reducing bias\ncaused by data sparsity. However, most existing GCL models rely on heuristic\napproaches and usually assume entity independence when constructing contrastive\nviews. We argue that these methods struggle to strike a balance between\nsemantic invariance and view hardness across the dynamic training process, both\nof which are critical factors in graph contrastive learning.\n  To address the above issues, we propose a novel GCL-based recommendation\nframework RGCL, which effectively maintains the semantic invariance of\ncontrastive pairs and dynamically adapts as the model capability evolves\nthrough the training process. Specifically, RGCL first introduces decision\nboundary-aware adversarial perturbations to constrain the exploration space of\ncontrastive augmented views, avoiding the decrease of task-specific\ninformation. Furthermore, to incorporate global user-user and item-item\ncollaboration relationships for guiding on the generation of hard contrastive\nviews, we propose an adversarial-contrastive learning objective to construct a\nrelation-aware view-generator. Besides, considering that unsupervised GCL could\npotentially narrower margins between data points and the decision boundary,\nresulting in decreased model robustness, we introduce the adversarial examples\nbased on maximum perturbations to achieve margin maximization. We also provide\ntheoretical analyses on the effectiveness of our designs. Through extensive\nexperiments on five public datasets, we demonstrate the superiority of RGCL\ncompared against twelve baseline models.\n","authors":["Jiakai Tang","Sunhao Dai","Zexu Sun","Xu Chen","Jun Xu","Wenhui Yu","Lantao Hu","Peng Jiang","Han Li"],"pdf_url":"https://arxiv.org/pdf/2407.10184v1.pdf","comment":"KDD 2024"},{"id":"http://arxiv.org/abs/2402.16664v2","updated":"2024-07-14T10:35:23Z","published":"2024-02-26T15:35:24Z","title":"LLM-Assisted Multi-Teacher Continual Learning for Visual Question\n  Answering in Robotic Surgery","summary":"  Visual question answering (VQA) is crucial for promoting surgical education.\nIn practice, the needs of trainees are constantly evolving, such as learning\nmore surgical types, adapting to different robots, and learning new surgical\ninstruments and techniques for various surgeries. However, patient data privacy\noften restricts the availability of old data when updating the model,\nnecessitating an exemplar-free continual learning (CL) setup. Prior CL studies\noverlooked two vital problems in the surgical domain: 1) large domain shifts\nfrom diverse surgical operations collected from multiple sources, and 2) severe\ndata imbalance arising from the uneven presence of surgical instruments or\nactivities. This paper proposes addressing these problems with a multimodal\nlarge language model (LLM) and an adaptive weight assignment methodology. We\nfirst develop a new multi-teacher CL framework that leverages a multimodal LLM\nas the additional teacher. The strong generalization ability of the LLM can\nbridge the knowledge gap when domain shifts and data imbalances occur. We then\nput forth a novel data processing method that transforms complex LLM embeddings\ninto logits compatible with our CL framework. We further design an adaptive\nweight assignment approach that balances the generalization ability of the LLM\nand the domain expertise of the old CL model. Finally, to comprehensively test\nthe effectiveness of our proposed method, we have also constructed two new\nsurgical VQA datasets that are largely different from existing ones and could\nbe valuable resources for future research. Extensive experimental results on\nthe tested datasets demonstrate the superiority of our method to other advanced\nCL schemes.\n","authors":["Yuyang Du","Kexin Chen","Yue Zhan","Chang Han Low","Tao You","Mobarakol Islam","Ziyu Guo","Yueming Jin","Guangyong Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2402.16664v2.pdf","comment":"This paper has been accapted by 2024 IEEE International Conference on\n  Robotics and Automation (ICRA)"},{"id":"http://arxiv.org/abs/2407.10115v1","updated":"2024-07-14T08:10:20Z","published":"2024-07-14T08:10:20Z","title":"A Bag of Tricks for Scaling CPU-based Deep FFMs to more than 300m\n  Predictions per Second","summary":"  Field-aware Factorization Machines (FFMs) have emerged as a powerful model\nfor click-through rate prediction, particularly excelling in capturing complex\nfeature interactions. In this work, we present an in-depth analysis of our\nin-house, Rust-based Deep FFM implementation, and detail its deployment on a\nCPU-only, multi-data-center scale. We overview key optimizations devised for\nboth training and inference, demonstrated by previously unpublished benchmark\nresults in efficient model search and online training. Further, we detail an\nin-house weight quantization that resulted in more than an order of magnitude\nreduction in bandwidth footprint related to weight transfers across\ndata-centres. We disclose the engine and associated techniques under an\nopen-source license to contribute to the broader machine learning community.\nThis paper showcases one of the first successful CPU-only deployments of Deep\nFFMs at such scale, marking a significant stride in practical, low-footprint\nclick-through rate prediction methodologies.\n","authors":["Blaž Škrlj","Benjamin Ben-Shalom","Grega Gašperšič","Adi Schwartz","Ramzi Hoseisi","Naama Ziporin","Davorin Kopič","Andraž Tori"],"pdf_url":"https://arxiv.org/pdf/2407.10115v1.pdf","comment":"6p, KDD2024 - AdKDD workshop"},{"id":"http://arxiv.org/abs/2407.10112v1","updated":"2024-07-14T07:58:13Z","published":"2024-07-14T07:58:13Z","title":"Warming Up Cold-Start CTR Prediction by Learning Item-Specific Feature\n  Interactions","summary":"  In recommendation systems, new items are continuously introduced, initially\nlacking interaction records but gradually accumulating them over time.\nAccurately predicting the click-through rate (CTR) for these items is crucial\nfor enhancing both revenue and user experience. While existing methods focus on\nenhancing item ID embeddings for new items within general CTR models, they tend\nto adopt a global feature interaction approach, often overshadowing new items\nwith sparse data by those with abundant interactions. Addressing this, our work\nintroduces EmerG, a novel approach that warms up cold-start CTR prediction by\nlearning item-specific feature interaction patterns. EmerG utilizes\nhypernetworks to generate an item-specific feature graph based on item\ncharacteristics, which is then processed by a Graph Neural Network (GNN). This\nGNN is specially tailored to provably capture feature interactions at any order\nthrough a customized message passing mechanism. We further design a meta\nlearning strategy that optimizes parameters of hypernetworks and GNN across\nvarious item CTR prediction tasks, while only adjusting a minimal set of\nitem-specific parameters within each task. This strategy effectively reduces\nthe risk of overfitting when dealing with limited data. Extensive experiments\non benchmark datasets validate that EmerG consistently performs the best given\nno, a few and sufficient instances of new items.\n","authors":["Yaqing Wang","Hongming Piao","Daxiang Dong","Quanming Yao","Jingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.10112v1.pdf","comment":"KDD 2024"},{"id":"http://arxiv.org/abs/2407.10081v1","updated":"2024-07-14T05:02:21Z","published":"2024-07-14T05:02:21Z","title":"All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems\n  Across the LLM Era","summary":"  Recommender systems (RS) are vital for managing information overload and\ndelivering personalized content, responding to users' diverse information\nneeds. The emergence of large language models (LLMs) offers a new horizon for\nredefining recommender systems with vast general knowledge and reasoning\ncapabilities. Standing across this LLM era, we aim to integrate recommender\nsystems into a broader picture, and pave the way for more comprehensive\nsolutions for future research. Therefore, we first offer a comprehensive\noverview of the technical progression of recommender systems, particularly\nfocusing on language foundation models and their applications in\nrecommendation. We identify two evolution paths of modern recommender systems\n-- via list-wise recommendation and conversational recommendation. These two\npaths finally converge at LLM agents with superior capabilities of long-term\nmemory, reflection, and tool intelligence. Along these two paths, we point out\nthat the information effectiveness of the recommendation is increased, while\nthe user's acquisition cost is decreased. Technical features, research\nmethodologies, and inherent challenges for each milestone along the path are\ncarefully investigated -- from traditional list-wise recommendation to\nLLM-enhanced recommendation to recommendation with LLM agents. Finally, we\nhighlight several unresolved challenges crucial for the development of future\npersonalization technologies and interfaces and discuss the future prospects.\n","authors":["Bo Chen","Xinyi Dai","Huifeng Guo","Wei Guo","Weiwen Liu","Yong Liu","Jiarui Qin","Ruiming Tang","Yichao Wang","Chuhan Wu","Yaxiong Wu","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.10081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10078v1","updated":"2024-07-14T04:53:36Z","published":"2024-07-14T04:53:36Z","title":"Semantic Understanding and Data Imputation using Large Language Model to\n  Accelerate Recommendation System","summary":"  This paper aims to address the challenge of sparse and missing data in\nrecommendation systems, a significant hurdle in the age of big data.\nTraditional imputation methods struggle to capture complex relationships within\nthe data. We propose a novel approach that fine-tune Large Language Model (LLM)\nand use it impute missing data for recommendation systems. LLM which is trained\non vast amounts of text, is able to understand complex relationship among data\nand intelligently fill in missing information. This enriched data is then used\nby the recommendation system to generate more accurate and personalized\nsuggestions, ultimately enhancing the user experience. We evaluate our\nLLM-based imputation method across various tasks within the recommendation\nsystem domain, including single classification, multi-classification, and\nregression compared to traditional data imputation methods. By demonstrating\nthe superiority of LLM imputation over traditional methods, we establish its\npotential for improving recommendation system performance.\n","authors":["Zhicheng Ding","Jiahao Tian","Zhenkai Wang","Jinman Zhao","Siyang Li"],"pdf_url":"https://arxiv.org/pdf/2407.10078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10042v1","updated":"2024-07-14T01:52:10Z","published":"2024-07-14T01:52:10Z","title":"Harnessing Feature Clustering For Enhanced Anomaly Detection With\n  Variational Autoencoder And Dynamic Threshold","summary":"  We introduce an anomaly detection method for multivariate time series data\nwith the aim of identifying critical periods and features influencing extreme\nclimate events like snowmelt in the Arctic. This method leverages the\nVariational Autoencoder (VAE) integrated with dynamic thresholding and\ncorrelation-based feature clustering. This framework enhances the VAE's ability\nto identify localized dependencies and learn the temporal relationships in\nclimate data, thereby improving the detection of anomalies as demonstrated by\nits higher F1-score on benchmark datasets. The study's main contributions\ninclude the development of a robust anomaly detection method, improving feature\nrepresentation within VAEs through clustering, and creating a dynamic threshold\nalgorithm for localized anomaly detection. This method offers explainability of\nclimate anomalies across different regions.\n","authors":["Tolulope Ale","Nicole-Jeanne Schlegel","Vandana P. Janeja"],"pdf_url":"https://arxiv.org/pdf/2407.10042v1.pdf","comment":"This work was presented at the 2024 IEEE International Geoscience and\n  Remote Sensing Symposium, IGARSS 2024, 07-12 July 2024, Athens, Greece"}]},"2024-07-13T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.10020v1","updated":"2024-07-13T22:33:29Z","published":"2024-07-13T22:33:29Z","title":"Causality extraction from medical text using Large Language Models\n  (LLMs)","summary":"  This study explores the potential of natural language models, including large\nlanguage models, to extract causal relations from medical texts, specifically\nfrom Clinical Practice Guidelines (CPGs). The outcomes causality extraction\nfrom Clinical Practice Guidelines for gestational diabetes are presented,\nmarking a first in the field. We report on a set of experiments using variants\nof BERT (BioBERT, DistilBERT, and BERT) and using Large Language Models (LLMs),\nnamely GPT-4 and LLAMA2. Our experiments show that BioBERT performed better\nthan other models, including the Large Language Models, with an average\nF1-score of 0.72. GPT-4 and LLAMA2 results show similar performance but less\nconsistency. We also release the code and an annotated a corpus of causal\nstatements within the Clinical Practice Guidelines for gestational diabetes.\n","authors":["Seethalakshmi Gopalakrishnan","Luciana Garbayo","Wlodek Zadrozny"],"pdf_url":"https://arxiv.org/pdf/2407.10020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09962v1","updated":"2024-07-13T18:05:07Z","published":"2024-07-13T18:05:07Z","title":"Correlating Power Outage Spread with Infrastructure Interdependencies\n  During Hurricanes","summary":"  Power outages caused by extreme weather events, such as hurricanes, can\nsignificantly disrupt essential services and delay recovery efforts,\nunderscoring the importance of enhancing our infrastructure's resilience. This\nstudy investigates the spread of power outages during hurricanes by analyzing\nthe correlation between the network of critical infrastructure and outage\npropagation. We leveraged datasets from Hurricanemapping.com, the North\nAmerican Energy Resilience Model Interdependency Analysis (NAERM-IA), and\nhistorical power outage data from the Oak Ridge National Laboratory (ORNL)'s\nEAGLE-I system. Our analysis reveals a consistent positive correlation between\nthe extent of critical infrastructure components accessible within a certain\nnumber of steps (k-hop distance) from initial impact areas and the occurrence\nof power outages in broader regions. This insight suggests that understanding\nthe interconnectedness among critical infrastructure elements is key to\nidentifying areas indirectly affected by extreme weather events.\n","authors":["Avishek Bose","Sangkeun Lee","Narayan Bhusal","Supriya Chinthavali"],"pdf_url":"https://arxiv.org/pdf/2407.09962v1.pdf","comment":"IEEE 25th International Conference on Information Reuse and\n  Integration for Data Science (IEEE IRI-2024)"},{"id":"http://arxiv.org/abs/2407.09939v1","updated":"2024-07-13T16:32:16Z","published":"2024-07-13T16:32:16Z","title":"Popular News Always Compete for the User's Attention! POPK: Mitigating\n  Popularity Bias via a Temporal-Counterfactual","summary":"  In news recommendation systems, reducing popularity bias is essential for\ndelivering accurate and diverse recommendations. This paper presents POPK, a\nnew method that uses temporal-counterfactual analysis to mitigate the influence\nof popular news articles. By asking, \"What if, at a given time $t$, a set of\npopular news articles were competing for the user's attention to be clicked?\",\nPOPK aims to improve recommendation accuracy and diversity. We tested POPK on\nthree different language datasets (Japanese, English, and Norwegian) and found\nthat it successfully enhances traditional methods. POPK offers flexibility for\ncustomization to enhance either accuracy or diversity, alongside providing\ndistinct ways of measuring popularity. We argue that popular news articles\nalways compete for attention, even if they are not explicitly present in the\nuser's impression list. POPK systematically eliminates the implicit influence\nof popular news articles during each training step. We combine counterfactual\nreasoning with a temporal approach to adjust the negative sample space,\nrefining understanding of user interests. Our findings underscore how POPK\neffectively enhances the accuracy and diversity of recommended articles while\nalso tailoring the approach to specific needs.\n","authors":["Igor L. R. Azevedo","Toyotaro Suzumura","Yuichiro Yasui"],"pdf_url":"https://arxiv.org/pdf/2407.09939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09747v1","updated":"2024-07-13T02:46:37Z","published":"2024-07-13T02:46:37Z","title":"SocialRec: User Activity Based Post Weighted Dynamic Personalized Post\n  Recommendation System in Social Media","summary":"  User activities can influence their subsequent interactions with a post,\ngenerating interest in the user. Typically, users interact with posts from\nfriends by commenting and using reaction emojis, reflecting their level of\ninterest on social media such as Facebook, Twitter, and Reddit. Our objective\nis to analyze user history over time, including their posts and engagement on\nvarious topics. Additionally, we take into account the user's profile, seeking\nconnections between their activities and social media platforms. By integrating\nuser history, engagement, and persona, we aim to assess recommendation scores\nbased on relevant item sharing by Hit Rate (HR) and the quality of the ranking\nsystem by Normalized Discounted Cumulative Gain (NDCG), where we achieve the\nhighest for NeuMF 0.80 and 0.6 respectively. Our hybrid approach solves the\ncold-start problem when there is a new user, for new items cold-start problem\nwill never occur, as we consider the post category values. To improve the\nperformance of the model during cold-start we introduce collaborative filtering\nby looking for similar users and ranking the users based on the highest\nsimilarity scores.\n","authors":["Ismail Hossain","Sai Puppala","Md Jahangir Alam","Sajedul Talukder"],"pdf_url":"https://arxiv.org/pdf/2407.09747v1.pdf","comment":"This research paper has been accepted in the Social Media Sway:\n  Unraveling the Impact of Social Media on Human Behavior - SMS workshop, to be\n  held in conjunction with the International Conference on Social Networks\n  Analysis and Mining (ASONAM 2024) and will be published in Springer"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.09992v1","updated":"2024-07-13T19:56:31Z","published":"2024-07-13T19:56:31Z","title":"TOP:A New Target-Audience Oriented Content Paraphrase Task","summary":"  Recommendation systems usually recommend the existing contents to different\nusers. However, in comparison to static recommendation methods, a\nrecommendation logic that dynamically adjusts based on user interest\npreferences may potentially attract a larger user base. Thus, we consider\nparaphrasing existing content based on the interests of the users to modify the\ncontent to better align with the preferences of users. In this paper, we\npropose a new task named Target-Audience Oriented Content Paraphrase aims to\ngenerate more customized contents for the target audience. We introduce the\ntask definition and the corresponding framework for the proposed task and the\ncreation of the corresponding datasets. We utilize the Large Language Models\n(LLMs) and Large Vision Models (LVMs) to accomplish the base implementation of\nthe TOP framework and provide the referential baseline results for the proposed\ntask.\n","authors":["Boda Lin","Jiaxin Shi","Haolong Yan","Binghao Tang","Xiaocheng Gong","Si Li"],"pdf_url":"https://arxiv.org/pdf/2407.09992v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2407.09935v1","updated":"2024-07-13T16:09:45Z","published":"2024-07-13T16:09:45Z","title":"LeRF: Learning Resampling Function for Adaptive and Efficient Image\n  Interpolation","summary":"  Image resampling is a basic technique that is widely employed in daily\napplications, such as camera photo editing. Recent deep neural networks (DNNs)\nhave made impressive progress in performance by introducing learned data\npriors. Still, these methods are not the perfect substitute for interpolation,\ndue to the drawbacks in efficiency and versatility. In this work, we propose a\nnovel method of Learning Resampling Function (termed LeRF), which takes\nadvantage of both the structural priors learned by DNNs and the locally\ncontinuous assumption of interpolation. Specifically, LeRF assigns spatially\nvarying resampling functions to input image pixels and learns to predict the\nhyper-parameters that determine the shapes of these resampling functions with a\nneural network. Based on the formulation of LeRF, we develop a family of\nmodels, including both efficiency-orientated and performance-orientated ones.\nTo achieve interpolation-level efficiency, we adopt look-up tables (LUTs) to\naccelerate the inference of the learned neural network. Furthermore, we design\na directional ensemble strategy and edge-sensitive indexing patterns to better\ncapture local structures. On the other hand, to obtain DNN-level performance,\nwe propose an extension of LeRF to enable it in cooperation with pre-trained\nupsampling models for cascaded resampling. Extensive experiments show that the\nefficiency-orientated version of LeRF runs as fast as interpolation,\ngeneralizes well to arbitrary transformations, and outperforms interpolation\nsignificantly, e.g., up to 3dB PSNR gain over Bicubic for x2 upsampling on\nManga109. Besides, the performance-orientated version of LeRF reaches\ncomparable performance with existing DNNs at much higher efficiency, e.g., less\nthan 25% running time on a desktop GPU.\n","authors":["Jiacheng Li","Chang Chen","Fenglong Song","Youliang Yan","Zhiwei Xiong"],"pdf_url":"https://arxiv.org/pdf/2407.09935v1.pdf","comment":"Code: https://github.com/ddlee-cn/LeRF-PyTorch"},{"id":"http://arxiv.org/abs/2407.09801v1","updated":"2024-07-13T08:20:37Z","published":"2024-07-13T08:20:37Z","title":"IoT-LM: Large Multisensory Language Models for the Internet of Things","summary":"  The Internet of Things (IoT) network integrating billions of smart physical\ndevices embedded with sensors, software, and communication technologies is a\ncritical and rapidly expanding component of our modern world. The IoT ecosystem\nprovides a rich source of real-world modalities such as motion, thermal,\ngeolocation, imaging, depth, sensors, and audio to recognize the states of\nhumans and physical objects. Machine learning presents a rich opportunity to\nautomatically process IoT data at scale, enabling efficient inference for\nunderstanding human wellbeing, controlling physical devices, and\ninterconnecting smart cities. To realize this potential, we introduce IoT-LM,\nan open-source large multisensory language model tailored for the IoT\necosystem. IoT-LM is enabled by two technical contributions: the first is\nMultiIoT, the most expansive unified IoT dataset to date, encompassing over\n1.15 million samples from 12 modalities and 8 tasks prepared for multisensory\npre-training and instruction-tuning. The second is a new multisensory multitask\nadapter layer to condition pre-trained large language models on multisensory\nIoT data. Not only does IoT-LM yield substantial improvements on 8 supervised\nIoT classification tasks, but it also demonstrates new interactive\nquestion-answering, reasoning, and dialog capabilities conditioned on IoT\nsensors. We release IoT-LM's data sources and new multisensory language\nmodeling framework.\n","authors":["Shentong Mo","Russ Salakhutdinov","Louis-Philippe Morency","Paul Pu Liang"],"pdf_url":"https://arxiv.org/pdf/2407.09801v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2311.06217"},{"id":"http://arxiv.org/abs/2407.09774v1","updated":"2024-07-13T05:02:42Z","published":"2024-07-13T05:02:42Z","title":"TemporalStory: Enhancing Consistency in Story Visualization using\n  Spatial-Temporal Attention","summary":"  Story visualization presents a challenging task in text-to-image generation,\nrequiring not only the rendering of visual details from text prompt but also\nensuring consistency across images. Recently, most approaches address\ninconsistency problem using an auto-regressive manner conditioned on previous\nimage-sentence pairs. However, they overlook the fact that story context is\ndispersed across all sentences. The auto-regressive approach fails to encode\ninformation from susequent image-sentence pairs, thus unable to capture the\nentirety of the story context. To address this, we introduce TemporalStory,\nleveraging Spatial-Temporal attention to model complex spatial and temporal\ndependencies in images, enabling the generation of coherent images based on a\ngiven storyline. In order to better understand the storyline context, we\nintroduce a text adapter capable of integrating information from other\nsentences into the embedding of the current sentence. Additionally, to utilize\nscene changes between story images as guidance for the model, we propose the\nStoryFlow Adapter to measure the degree of change between images. Through\nextensive experiments on two popular benchmarks, PororoSV and FlintstonesSV,\nour TemporalStory outperforms the previous state-of-the-art in both story\nvisualization and story continuation tasks.\n","authors":["Sixiao Zheng","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2407.09774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06524v3","updated":"2024-07-13T04:52:32Z","published":"2024-07-09T03:32:00Z","title":"Improving Speech Enhancement by Integrating Inter-Channel and Band\n  Features with Dual-branch Conformer","summary":"  Recent speech enhancement methods based on convolutional neural networks\n(CNNs) and transformer have been demonstrated to efficaciously capture\ntime-frequency (T-F) information on spectrogram. However, the correlation of\neach channels of speech features is failed to explore. Theoretically, each\nchannel map of speech features obtained by different convolution kernels\ncontains information with different scales demonstrating strong correlations.\nTo fill this gap, we propose a novel dual-branch architecture named\nchannel-aware dual-branch conformer (CADB-Conformer), which effectively\nexplores the long range time and frequency correlations among different\nchannels, respectively, to extract channel relation aware time-frequency\ninformation. Ablation studies conducted on DNS-Challenge 2020 dataset\ndemonstrate the importance of channel feature leveraging while showing the\nsignificance of channel relation aware T-F information for speech enhancement.\nExtensive experiments also show that the proposed model achieves superior\nperformance than recent methods with an attractive computational costs.\n","authors":["Jizhen Li","Xinmeng Xu","Weiping Tu","Yuhong Yang","Rong Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.06524v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09766v1","updated":"2024-07-13T04:15:02Z","published":"2024-07-13T04:15:02Z","title":"User Digital Twin-Driven Video Streaming for Customized Preferences and\n  Adaptive Transcoding","summary":"  In the rapidly evolving field of multimedia services, video streaming has\nbecome increasingly prevalent, demanding innovative solutions to enhance user\nexperience and system efficiency. This paper introduces a novel approach that\nintegrates user digital twins-a dynamic digital representation of a user's\npreferences and behaviors-with traditional video streaming systems. We explore\nthe potential of this integration to dynamically adjust video preferences and\noptimize transcoding processes according to real-time data. The methodology\nleverages advanced machine learning algorithms to continuously update the\nuser's digital twin, which in turn informs the transcoding service to adapt\nvideo parameters for optimal quality and minimal buffering. Experimental\nresults show that our approach not only improves the personalization of content\ndelivery but also significantly enhances the overall efficiency of video\nstreaming services by reducing bandwidth usage and improving video playback\nquality. The implications of such advancements suggest a shift towards more\nadaptive, user-centric multimedia services, potentially transforming how video\ncontent is consumed and delivered.\n","authors":["Stephen Jimmy","Kalkidan Berhane","Kevin Muhammad"],"pdf_url":"https://arxiv.org/pdf/2407.09766v1.pdf","comment":null}]}}